{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fMxD-dwDloHp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow import keras\n",
        "import pickle as pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "IRmTzMSOszqz",
        "outputId": "861a57ef-0859-4d40-cfef-a6517a52be65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temperature : 0.3625\n",
            "(10000, 8, 8)\n"
          ]
        }
      ],
      "source": [
        "file = open('8x8lattices.pkl','rb')\n",
        "lattice_set = pkl.load(file)\n",
        "\n",
        "t = 0.05 + 5*(2.0/32)\n",
        "print(f'Temperature : {t}')\n",
        "#10k samples each of shape (8,8)\n",
        "lattices = np.array(lattice_set[5])\n",
        "print(lattices.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 8, 8, 1)\n"
          ]
        }
      ],
      "source": [
        "data = tf.expand_dims(lattices, axis=3)\n",
        "print(data.get_shape())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f93NFA0f0vgV"
      },
      "outputs": [],
      "source": [
        "def checkerboard(height, width, reverse=False, dtype=tf.float32):\n",
        "  checkerboard = [[((i % 2) + j) % 2 for j in range(width)] for i in range(height)]\n",
        "  checkerboard = tf.convert_to_tensor(checkerboard, dtype = dtype)\n",
        "  if reverse:\n",
        "      checkerboard = 1 - checkerboard\n",
        "  checkerboard = tf.reshape(checkerboard, (1,height,width,1))\n",
        "  return tf.cast(checkerboard, dtype=dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xhWev_c6pEbK"
      },
      "outputs": [],
      "source": [
        "def periodic_padding(x, padding=1):\n",
        "  '''\n",
        "  x: shape (batch_size, d1, d2)\n",
        "  return x padded with periodic boundaries. i.e. torus or donut\n",
        "  '''\n",
        "  d1 = x.shape[1] # dimension 1: height\n",
        "  d2 = x.shape[2] # dimension 2: width\n",
        "  p = padding\n",
        "  # assemble padded x from slices\n",
        "  #            tl,tc,tr\n",
        "  # padded_x = ml,mc,mr\n",
        "  #            bl,bc,br\n",
        "  top_left = x[:, -p:, -p:] # top left\n",
        "  top_center = x[:, -p:, :] # top center\n",
        "  top_right = x[:, -p:, :p] # top right\n",
        "  middle_left = x[:, :, -p:] # middle left\n",
        "  middle_center = x # middle center\n",
        "  middle_right = x[:, :, :p] # middle right\n",
        "  bottom_left = x[:, :p, -p:] # bottom left\n",
        "  bottom_center = x[:, :p, :] # bottom center\n",
        "  bottom_right = x[:, :p, :p] # bottom right\n",
        "  top = tf.concat([top_left, top_center, top_right], axis=2)\n",
        "  middle = tf.concat([middle_left, middle_center, middle_right], axis=2)\n",
        "  bottom = tf.concat([bottom_left, bottom_center, bottom_right], axis=2)\n",
        "  padded_x = tf.concat([top, middle, bottom], axis=1)\n",
        "  return padded_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "5tPtTf5G1Byl"
      },
      "outputs": [],
      "source": [
        "filters = 64\n",
        "\n",
        "def Coupling(input_shape):\n",
        "  input = keras.layers.Input(shape=input_shape)\n",
        "\n",
        "  layer1 = keras.layers.Conv2D(filters,3, activation=\"relu\",padding = 'valid',name = 'layer1')(periodic_padding(input))\n",
        "  # print(f'layer1 : {layer1.get_shape()}')\n",
        "  layer2 = keras.layers.Conv2D(filters,3, activation=\"relu\",padding = 'valid',name = 'layer2')(periodic_padding(layer1))\n",
        "  # print(f'layer2 : {layer2.get_shape()}')\n",
        "  t_layer= keras.layers.Conv2D(1,3,padding = 'valid',name = 't_layer')(periodic_padding(layer2))\n",
        "  # print(f't : {t_layer.get_shape()}')\n",
        "  s_layer= keras.layers.Conv2D(1,3,activation = 'tanh',padding = 'valid', name = 's_layer')(periodic_padding(layer2))\n",
        "  # print(f's : {s_layer.get_shape()}')\n",
        "\n",
        "  return keras.Model(inputs=input, outputs=[s_layer, t_layer])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "L_eD5TLw1Cy1"
      },
      "outputs": [],
      "source": [
        "class SimpleNormal:\n",
        "  def __init__(self, loc, var):\n",
        "    self.dist = tfp.distributions.Normal(tf.reshape(loc,(-1,)), tf.reshape(var,(-1,)))\n",
        "    self.shape = loc.shape\n",
        "  def log_prob(self, x):\n",
        "    logp = self.dist.log_prob(tf.reshape(x,(x.shape[0], -1)))\n",
        "    return tf.reduce_sum(logp, axis=1)\n",
        "  def sample_n(self, batch_size , seed = None):\n",
        "    x = self.dist.sample((batch_size,),seed = seed)\n",
        "    return tf.reshape(x,(batch_size, *self.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "bRnTpfa71FuV"
      },
      "outputs": [],
      "source": [
        "class RealNVP(keras.Model):\n",
        "  def __init__(self, num_coupling_layers,input_shape,data_constraint):\n",
        "      super().__init__()\n",
        "      self.num_coupling_layers = num_coupling_layers\n",
        "      self.distribution = SimpleNormal(tf.zeros((8,8)), tf.ones((8,8)))\n",
        "      self.masks = [checkerboard(input_shape[0],input_shape[1], reverse=False),checkerboard(input_shape[0],input_shape[1], reverse=True)]*(num_coupling_layers // 2)\n",
        "      self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "      self.layers_list = [Coupling(input_shape) for i in range(num_coupling_layers)]\n",
        "      self.data_constraint = data_constraint\n",
        "\n",
        "  def call(self, x, forward=True):\n",
        "    if forward:\n",
        "      alpha = tf.constant(self.data_constraint)\n",
        "      logq = self.distribution.log_prob(tf.reshape(x,(-1,8,8)))\n",
        "      x,ldj1 = self.forward_flow(x)\n",
        "      # ldj2 = tf.math.softplus(x) + tf.math.softplus(-x) + tf.math.log(tf.constant(1.-2*self.data_constraint))\n",
        "      # ldj2 = tf.reduce_sum(ldj2,[1,2,3])\n",
        "      logq = logq - ldj1 #+ ldj2\n",
        "      # x   = (tf.math.sigmoid(x) - alpha)/(1-2*alpha)\n",
        "      return x, logq\n",
        "    else:\n",
        "      # x = self.data_constraint + (1-2*self.data_constraint)*x\n",
        "      # x = tf.math.log(x/(1.-x))\n",
        "      # Save log-determinant of Jacobian of initial transform\n",
        "      # ldj1 = tf.math.softplus(x) + tf.math.softplus(-x) + tf.math.log(tf.constant(1.-2*self.data_constraint))\n",
        "      # ldj1 = tf.reduce_sum(ldj1,[1,2,3])\n",
        "      x,ldj2 = self.reverse_flow(x)\n",
        "      logq = self.distribution.log_prob(tf.reshape(x,(-1,8,8)))\n",
        "      logq = logq + ldj2 #+ ldj1\n",
        "      return x , logq\n",
        "\n",
        "  def forward_flow(self, x):\n",
        "    ldj = 0\n",
        "    for i in range(self.num_coupling_layers):\n",
        "      x_frozen = x * self.masks[i]\n",
        "      reversed_mask = 1 - self.masks[i]\n",
        "      x_active = x * reversed_mask\n",
        "      s, t = self.layers_list[i](x_frozen)\n",
        "      s *= reversed_mask\n",
        "      t *= reversed_mask\n",
        "\n",
        "      fx = t + x_active *tf.exp(s) + x_frozen\n",
        "      ldj += tf.reduce_sum(s, [1,2,3])\n",
        "      x = fx\n",
        "    return x, ldj\n",
        "\n",
        "  def reverse_flow(self, fx):\n",
        "    ldj = 0\n",
        "    for i in reversed(range(self.num_coupling_layers)):\n",
        "      fx_frozen = fx*self.masks[i]\n",
        "      reversed_mask = 1 - self.masks[i]\n",
        "      fx_active = fx*reversed_mask\n",
        "      s, t = self.layers_list[i](fx_frozen)\n",
        "      s *= reversed_mask\n",
        "      t *= reversed_mask\n",
        "\n",
        "      x = (fx_active - t) *tf.exp(-s) + fx_frozen\n",
        "      ldj -= tf.reduce_sum(s, [1,2,3])\n",
        "      fx = x\n",
        "    return fx,ldj\n",
        "\n",
        "  def log_loss(self, x, forward=True):\n",
        "    fx, ldj = self(x, forward=forward)\n",
        "    log_likelihood = self.distribution.log_prob(fx) + ldj\n",
        "    return -tf.reduce_mean(log_likelihood)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "WavxC4yclg7N"
      },
      "outputs": [],
      "source": [
        "model = RealNVP(2,(8,8,1),0.2)\n",
        "optimizer=keras.optimizers.Adam(learning_rate=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "data_batches = tf.data.Dataset.from_tensor_slices(data).batch(batch_size)\n",
        "train_batches = data_batches\n",
        "# train_split = 0.8\n",
        "# shuffle = True\n",
        "# size = 10000\n",
        "# train_size = int(size*train_split)\n",
        "\n",
        "# if shuffle:\n",
        "#     data_batches = data_batches.shuffle(size, seed=10)\n",
        "\n",
        "# train_batches = data_batches.take(train_size)\n",
        "# val_batches = data_batches.skip(train_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 train_loss = 112.48396301269531\n",
            "Epoch 2 train_loss = 87.18617248535156\n",
            "Epoch 3 train_loss = 69.34950256347656\n",
            "Epoch 4 train_loss = 62.55675506591797\n",
            "Epoch 5 train_loss = 59.70025634765625\n",
            "Epoch 6 train_loss = 58.266883850097656\n",
            "Epoch 7 train_loss = 57.40083694458008\n",
            "Epoch 8 train_loss = 56.87458419799805\n",
            "Epoch 9 train_loss = 56.54747772216797\n",
            "Epoch 10 train_loss = 56.342506408691406\n",
            "Epoch 11 train_loss = 56.20722579956055\n",
            "Epoch 12 train_loss = 56.11528396606445\n",
            "Epoch 13 train_loss = 56.048179626464844\n",
            "Epoch 14 train_loss = 55.9981575012207\n",
            "Epoch 15 train_loss = 55.9578971862793\n",
            "Epoch 16 train_loss = 55.9250373840332\n",
            "Epoch 17 train_loss = 55.896278381347656\n",
            "Epoch 18 train_loss = 55.87171173095703\n",
            "Epoch 19 train_loss = 55.851905822753906\n",
            "Epoch 20 train_loss = 55.836219787597656\n",
            "Epoch 21 train_loss = 55.82378005981445\n",
            "Epoch 22 train_loss = 55.813636779785156\n",
            "Epoch 23 train_loss = 55.806861877441406\n",
            "Epoch 24 train_loss = 55.79999923706055\n",
            "Epoch 25 train_loss = 55.7947998046875\n",
            "Epoch 26 train_loss = 55.788246154785156\n",
            "Epoch 27 train_loss = 55.7880859375\n",
            "Epoch 28 train_loss = 55.78520584106445\n",
            "Epoch 29 train_loss = 55.782470703125\n",
            "Epoch 30 train_loss = 55.7800178527832\n",
            "Epoch 31 train_loss = 55.778480529785156\n",
            "Epoch 32 train_loss = 55.77592849731445\n",
            "Epoch 33 train_loss = 55.77479934692383\n",
            "Epoch 34 train_loss = 55.77283477783203\n",
            "Epoch 35 train_loss = 55.77241134643555\n",
            "Epoch 36 train_loss = 55.771026611328125\n",
            "Epoch 37 train_loss = 55.7702751159668\n",
            "Epoch 38 train_loss = 55.76898193359375\n",
            "Epoch 39 train_loss = 55.76805877685547\n",
            "Epoch 40 train_loss = 55.7667350769043\n",
            "Epoch 41 train_loss = 55.76515579223633\n",
            "Epoch 42 train_loss = 55.76287841796875\n",
            "Epoch 43 train_loss = 55.760536193847656\n",
            "Epoch 44 train_loss = 55.75800704956055\n",
            "Epoch 45 train_loss = 55.7559700012207\n",
            "Epoch 46 train_loss = 55.7534065246582\n",
            "Epoch 47 train_loss = 55.75053787231445\n",
            "Epoch 48 train_loss = 55.74809646606445\n",
            "Epoch 49 train_loss = 55.7459831237793\n",
            "Epoch 50 train_loss = 55.74365234375\n",
            "Epoch 51 train_loss = 55.74140548706055\n",
            "Epoch 52 train_loss = 55.739234924316406\n",
            "Epoch 53 train_loss = 55.73705291748047\n",
            "Epoch 54 train_loss = 55.7348747253418\n",
            "Epoch 55 train_loss = 55.732826232910156\n",
            "Epoch 56 train_loss = 55.73052978515625\n",
            "Epoch 57 train_loss = 55.728614807128906\n",
            "Epoch 58 train_loss = 55.72654342651367\n",
            "Epoch 59 train_loss = 55.72452926635742\n",
            "Epoch 60 train_loss = 55.72239303588867\n",
            "Epoch 61 train_loss = 55.720703125\n",
            "Epoch 62 train_loss = 55.71857452392578\n",
            "Epoch 63 train_loss = 55.71680450439453\n",
            "Epoch 64 train_loss = 55.7147216796875\n",
            "Epoch 65 train_loss = 55.7132682800293\n",
            "Epoch 66 train_loss = 55.711509704589844\n",
            "Epoch 67 train_loss = 55.7101936340332\n",
            "Epoch 68 train_loss = 55.70899200439453\n",
            "Epoch 69 train_loss = 55.7076530456543\n",
            "Epoch 70 train_loss = 55.70610427856445\n",
            "Epoch 71 train_loss = 55.70484161376953\n",
            "Epoch 72 train_loss = 55.70343017578125\n",
            "Epoch 73 train_loss = 55.70208740234375\n",
            "Epoch 74 train_loss = 55.70038986206055\n",
            "Epoch 75 train_loss = 55.69854736328125\n",
            "Epoch 76 train_loss = 55.69670486450195\n",
            "Epoch 77 train_loss = 55.69476318359375\n",
            "Epoch 78 train_loss = 55.6932487487793\n",
            "Epoch 79 train_loss = 55.69160079956055\n",
            "Epoch 80 train_loss = 55.6899299621582\n",
            "Epoch 81 train_loss = 55.68816375732422\n",
            "Epoch 82 train_loss = 55.6866455078125\n",
            "Epoch 83 train_loss = 55.6848030090332\n",
            "Epoch 84 train_loss = 55.683006286621094\n",
            "Epoch 85 train_loss = 55.681365966796875\n",
            "Epoch 86 train_loss = 55.679664611816406\n",
            "Epoch 87 train_loss = 55.6781005859375\n",
            "Epoch 88 train_loss = 55.67658233642578\n",
            "Epoch 89 train_loss = 55.67499923706055\n",
            "Epoch 90 train_loss = 55.67351150512695\n",
            "Epoch 91 train_loss = 55.67168045043945\n",
            "Epoch 92 train_loss = 55.670257568359375\n",
            "Epoch 93 train_loss = 55.668670654296875\n",
            "Epoch 94 train_loss = 55.6672248840332\n",
            "Epoch 95 train_loss = 55.66572189331055\n",
            "Epoch 96 train_loss = 55.66424560546875\n",
            "Epoch 97 train_loss = 55.6626091003418\n",
            "Epoch 98 train_loss = 55.661231994628906\n",
            "Epoch 99 train_loss = 55.65964889526367\n",
            "Epoch 100 train_loss = 55.658203125\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "\n",
        "for e in range(epochs):\n",
        "    losses = []\n",
        "    for step,t_batch in enumerate(train_batches):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = model.log_loss(t_batch, forward=False)\n",
        "\n",
        "        grad = tape.gradient(loss, model.trainable_variables)  \n",
        "        optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
        "        model.loss_tracker.update_state(loss)  \n",
        "        losses.append(loss)\n",
        "    \n",
        "    train_loss = np.mean(np.array(losses))\n",
        "    print(f'Epoch {e+1} train_loss = {train_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
