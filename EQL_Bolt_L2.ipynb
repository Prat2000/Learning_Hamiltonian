{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j29h2u-mS2Ia"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-10 15:12:47.068449: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.regularizers import L1\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import numpy as np\n",
        "import math\n",
        "import tensorflow.keras.backend as K\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ykGGFFjvTLv4"
      },
      "outputs": [],
      "source": [
        "import sympy\n",
        "from itertools import accumulate\n",
        "import pickle as pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Nh9hI4iUPLNx",
        "outputId": "e7b644f9-dba8-4872-b507-ac5a293f1e40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "device_name = tf.test.gpu_device_name()\n",
        "device_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 1000, 5)\n"
          ]
        }
      ],
      "source": [
        "file = open('conditional_data.pkl','rb')\n",
        "cond_data = pkl.load(file)\n",
        "print(cond_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q8cjMW0R4jU6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-10 15:12:50.242052: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
          ]
        }
      ],
      "source": [
        "global input_dim\n",
        "# global regularise_weights\n",
        "\n",
        "size = 1000\n",
        "batch_size = 200\n",
        "\n",
        "l0_reg = tf.Variable(initial_value=0, trainable=False, dtype=tf.float32)\n",
        "l1_reg = tf.Variable(initial_value=0, trainable=False, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EEqE5ct-4O8L"
      },
      "outputs": [],
      "source": [
        "def threshold_matrix(matrix, thresh):\n",
        "  with tf.device(device_name):\n",
        "    mask = tf.abs(matrix) > thresh\n",
        "    new_mat = tf.multiply(tf.cast(mask,dtype=tf.float32),matrix)\n",
        "    return new_mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hoDebAhbEPHn"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def l1_regularizer(weights):\n",
        "  with tf.device(device_name):\n",
        "    loss = l1_reg*tf.reduce_sum(tf.abs(weights))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VBmHedO0tPZE"
      },
      "outputs": [],
      "source": [
        "class dense_layer(layers.Layer):\n",
        "  def __init__(self, w0, d, v, begin=False, last=False):\n",
        "    super().__init__()\n",
        "    w_init = tf.random_uniform_initializer(minval = -w0, maxval = w0, seed = 3)\n",
        "    if last:\n",
        "      self.layer = layers.Dense(d, input_shape = (3*v,), kernel_initializer=w_init) # kernel_regularizer=l1_regularizer, bias_regularizer=l1_regularizer)\n",
        "    elif begin:\n",
        "      self.layer = layers.Dense(3*v, input_shape = (input_dim,), kernel_initializer=w_init) # kernel_regularizer=l1_regularizer, bias_regularizer=l1_regularizer)\n",
        "    else:\n",
        "      self.layer = layers.Dense(3*v, input_shape = (3*v,), kernel_initializer=w_init) # kernel_regularizer=l1_regularizer, bias_regularizer=l1_regularizer)\n",
        "\n",
        "  def call(self, input):\n",
        "    z = self.layer(input)\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "44OU6L9j4Srm"
      },
      "outputs": [],
      "source": [
        "ops = {'id' : tf.identity,\n",
        "       'sin' : tf.sin,\n",
        "       'cos' : tf.cos }\n",
        "\n",
        "sympy_ops = [sympy.Id, sympy.sin, sympy.cos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qmghgEIh3Egw"
      },
      "outputs": [],
      "source": [
        "def eql_func(var):\n",
        "  with tf.device(device_name):\n",
        "    z = var[0]\n",
        "    v = var[1]\n",
        "    units = tf.split(z,num_or_size_splits=[v,v,v],axis=1)\n",
        "    outputs = []\n",
        "    i = 0\n",
        "    for _,operator in ops.items():\n",
        "      inputs = tf.split(units[i],[v]*1,axis=1)\n",
        "      outputs.append(operator(*inputs))\n",
        "      i = i+1\n",
        "    y = tf.concat(outputs,axis=1)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R75ryomKMay1",
        "outputId": "081f621e-5141-40c9-96c7-3f15158eff9d"
      },
      "outputs": [],
      "source": [
        "# # Dataset\n",
        "# tf.random.set_seed(3)\n",
        "# x_data = tf.random.uniform(shape=[size,5],minval=-1,maxval=1,seed=10)\n",
        "\n",
        "# ## Hamiltonian eqn\n",
        "# y_data = tf.math.cos(x_data[:,0]-x_data[:,1]) + tf.math.cos(x_data[:,0]-x_data[:,2]) \\\n",
        "#           + tf.math.cos(x_data[:,0]-x_data[:,3]) + tf.math.cos(x_data[:,0]-x_data[:,4])\n",
        "\n",
        "# np.random.seed(3)\n",
        "# train_split = 0.8\n",
        "# idx = np.arange(size)\n",
        "# np.random.shuffle(idx)\n",
        "# train_idx = idx[:int(train_split*size)]\n",
        "# val_idx = idx[int(train_split*size):]\n",
        "\n",
        "# x_train = tf.gather(x_data, train_idx)\n",
        "# x_val = tf.gather(x_data, val_idx)\n",
        "# y_train = tf.gather(y_data, train_idx)\n",
        "# y_val = tf.gather(y_data, val_idx)\n",
        "\n",
        "# print(x_train.get_shape(), y_train.get_shape())\n",
        "# print(x_val.get_shape(), y_val.get_shape())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(800, 5) (800,) (200, 5) (200,)\n"
          ]
        }
      ],
      "source": [
        "# Conditional Dataset ##################################################\n",
        "pos = 22\n",
        "x_data = cond_data[pos]\n",
        "x_data = x_data*(2*np.pi)\n",
        "#true values\n",
        "y_data = tf.math.cos(x_data[:,0]-x_data[:,1]) + tf.math.cos(x_data[:,0]-x_data[:,2]) \\\n",
        "          + tf.math.cos(x_data[:,0]-x_data[:,3]) + tf.math.cos(x_data[:,0]-x_data[:,4])\n",
        "\n",
        "np.random.seed(3)\n",
        "train_split = 0.8\n",
        "idx = np.arange(size)\n",
        "np.random.shuffle(idx)\n",
        "train_idx = idx[:int(train_split*size)]\n",
        "val_idx = idx[int(train_split*size):]\n",
        "\n",
        "x_train = tf.gather(x_data, train_idx)\n",
        "y_train = tf.gather(y_data, train_idx)\n",
        "x_val = tf.gather(x_data, val_idx)\n",
        "y_val = tf.gather(y_data, val_idx)\n",
        "\n",
        "print(x_train.get_shape(), y_train.get_shape(), x_val.get_shape(), y_val.get_shape())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.61334608, 2.71950583, 1.95799552, 2.85443892, 2.47289138])>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(800,) (200,)\n"
          ]
        }
      ],
      "source": [
        "x_train = x_train[:,0]\n",
        "x_val = x_val[:,0]\n",
        "print(x_train.get_shape(), x_val.get_shape())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "88WuQ7ScMOd6"
      },
      "outputs": [],
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(batch_size)\n",
        "val_data = tf.data.Dataset.from_tensor_slices((x_val,y_val)).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvT-viXNT-7z",
        "outputId": "bd53eb08-a009-4126-fbc3-e44d729947de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w0 : 0.5773502691896258\n",
            "reg_start : 500\n",
            "reg_end : 900\n"
          ]
        }
      ],
      "source": [
        "#input params\n",
        "reg_ph = [0.25,0.45]\n",
        "epoch_factor = 2000\n",
        "beta1 = 0.4\n",
        "l0_thresh = 0.05\n",
        "reg_scale = 1e-3\n",
        "wt_init_param = 1.0\n",
        "L = 2\n",
        "out_dim = 1\n",
        "\n",
        "w0 = wt_init_param/math.sqrt(1 + L)\n",
        "reg_start = math.floor((L-1)*epoch_factor*reg_ph[0])\n",
        "reg_end = math.floor((L-1)*epoch_factor*reg_ph[1])\n",
        "print(f'w0 : {w0}')\n",
        "print(f'reg_start : {reg_start}')\n",
        "print(f'reg_end : {reg_end}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4vtLqU3O9H7e"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4, beta_1=beta1, epsilon=1e-4)\n",
        "loss_func = tf.keras.losses.MeanSquaredError()\n",
        "# mae = tf.keras.losses.MeanAbsoluteError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byS15Abv0Dyi"
      },
      "outputs": [],
      "source": [
        "#### Using Keras API ##############################\n",
        "\n",
        "class myCallback(Callback):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def on_epoch_begin(self, epoch, logs={}):\n",
        "    if epoch>=reg_start and epoch<=reg_end:\n",
        "      l1_reg.assign(tf.cast(reg_scale, dtype=tf.float32))\n",
        "    if epoch>reg_end:\n",
        "      # regularise_weights = True\n",
        "      l1_reg.assign(tf.cast(0, dtype=tf.float32))\n",
        "      l0_reg.assign(tf.cast(l0_thresh, dtype=tf.float32))\n",
        "\n",
        "call_back = myCallback()\n",
        "\n",
        "input_dim = 5\n",
        "d = 1\n",
        "v = 10\n",
        "T = (L-1)*20\n",
        "\n",
        "begin = True\n",
        "input = tf.keras.Input(shape=(input_dim,), name= 'model_input')\n",
        "y = input\n",
        "for i in range(L-1):\n",
        "  z = dense_layer(w0, d, v, begin = begin)(y)\n",
        "  y = layers.Lambda(eql_func, name = f'func_layer_{i}')([z,v])\n",
        "  begin = False\n",
        "output = dense_layer(w0, d, v, last=True)(y)\n",
        "\n",
        "model = tf.keras.Model(inputs = input, outputs = output, name = 'eql_div')\n",
        "model.compile(optimizer, loss = loss_func) #, run_eagerly=True)\n",
        "\n",
        "model.summary()\n",
        "model.fit(x_train, y_train, epochs=T, batch_size=20, validation_split=0.2, callbacks=[call_back])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPg8IWuIJBCI"
      },
      "outputs": [],
      "source": [
        "saved_weight = False\n",
        "\n",
        "if saved_weight:\n",
        "    model.built = True\n",
        "    model.load_weights('eql_bolt_L3.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8nRcoLAI4ff"
      },
      "outputs": [],
      "source": [
        "#save model\n",
        "model.save_weights('eql_bolt_L3_reg.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlrqwMccI4cv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OMLwjbGs0h9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "McoYwQwFs0fF"
      },
      "outputs": [],
      "source": [
        "#### Method 2 ############################################\n",
        "\n",
        "input_dim = 1\n",
        "d = 1\n",
        "v = 4\n",
        "T = (L-1)*10000\n",
        "temp = 0.5\n",
        "\n",
        "begin = True\n",
        "d_layers = []\n",
        "for i in range(L-1):\n",
        "  d_layers.append(dense_layer(w0, d, v, begin = begin))\n",
        "  begin = False\n",
        "d_layers.append(dense_layer(w0, d, v, last=True))\n",
        "\n",
        "input = tf.keras.Input(shape=(input_dim,), name= 'model_input')\n",
        "y = input\n",
        "for i in range(L-1):\n",
        "  z = d_layers[i](y)\n",
        "  y = layers.Lambda(eql_func, name = f'func_layer_{i}')([z,v])\n",
        "output = d_layers[-1](y)\n",
        "\n",
        "model = tf.keras.Model(inputs = input, outputs = output, name = 'eql_div')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_4xnYUOiBtsQ"
      },
      "outputs": [],
      "source": [
        "def reassign_values(e):\n",
        "  if e<reg_start:\n",
        "    l1_reg.assign(tf.cast(0, dtype=tf.float32))\n",
        "    l0_reg.assign(tf.cast(0, dtype=tf.float32))\n",
        "  elif e>=reg_start and e<=reg_end:\n",
        "    l1_reg.assign(tf.cast(reg_scale, dtype=tf.float32))\n",
        "  elif e>reg_end:\n",
        "    l1_reg.assign(tf.cast(0, dtype=tf.float32))\n",
        "    l0_reg.assign(tf.cast(l0_thresh, dtype=tf.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def l1_loss(model):\n",
        "    loss = 0.\n",
        "    for wgt in model.trainable_weights:\n",
        "        loss += l1_reg*tf.reduce_sum(tf.abs(wgt)) \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Hh4nySRNLRDk"
      },
      "outputs": [],
      "source": [
        "## L2 loss + L0 loss\n",
        "\n",
        "@tf.function\n",
        "def train_step(x,y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        output = model(x, training=True)\n",
        "        loss_value = loss_func(y,output) + l1_loss(model)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    return loss_value\n",
        "\n",
        "@tf.function\n",
        "def val_step(x,y):\n",
        "    output = model(x, training=False)\n",
        "    loss_value = loss_func(y,output)\n",
        "    return loss_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ofAy_qbjs0ce",
        "outputId": "8f9783ba-be4c-46a5-c623-220bd30624b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-10 15:13:29.774323: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [800]\n",
            "\t [[{{node Placeholder/_1}}]]\n",
            "2023-07-10 15:13:30.311156: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [200]\n",
            "\t [[{{node Placeholder/_1}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/10000      train_loss = 13.215621948242188     val_loss = 13.912654876708984\n",
            "Epoch 10/10000      train_loss = 12.822466850280762     val_loss = 13.490107536315918\n",
            "Epoch 20/10000      train_loss = 12.444992065429688     val_loss = 13.084449768066406\n",
            "Epoch 30/10000      train_loss = 12.081725120544434     val_loss = 12.694189071655273\n",
            "Epoch 40/10000      train_loss = 11.731765747070312     val_loss = 12.318290710449219\n",
            "Epoch 50/10000      train_loss = 11.394187927246094     val_loss = 11.955735206604004\n",
            "Epoch 60/10000      train_loss = 11.068164825439453     val_loss = 11.605668067932129\n",
            "Epoch 70/10000      train_loss = 10.753049850463867     val_loss = 11.267434120178223\n",
            "Epoch 80/10000      train_loss = 10.44831657409668     val_loss = 10.9404935836792\n",
            "Epoch 90/10000      train_loss = 10.153505325317383     val_loss = 10.624366760253906\n",
            "Epoch 100/10000      train_loss = 9.86816692352295     val_loss = 10.31856918334961\n",
            "Epoch 110/10000      train_loss = 9.591817855834961     val_loss = 10.022579193115234\n",
            "Epoch 120/10000      train_loss = 9.323908805847168     val_loss = 9.735785484313965\n",
            "Epoch 130/10000      train_loss = 9.063823699951172     val_loss = 9.457517623901367\n",
            "Epoch 140/10000      train_loss = 8.81091594696045     val_loss = 9.187061309814453\n",
            "Epoch 150/10000      train_loss = 8.564574241638184     val_loss = 8.923751831054688\n",
            "Epoch 160/10000      train_loss = 8.324289321899414     val_loss = 8.667030334472656\n",
            "Epoch 170/10000      train_loss = 8.08968734741211     val_loss = 8.416495323181152\n",
            "Epoch 180/10000      train_loss = 7.8605241775512695     val_loss = 8.171882629394531\n",
            "Epoch 190/10000      train_loss = 7.636648178100586     val_loss = 7.933026313781738\n",
            "Epoch 200/10000      train_loss = 7.417954444885254     val_loss = 7.699809551239014\n",
            "Epoch 210/10000      train_loss = 7.2043609619140625     val_loss = 7.472146511077881\n",
            "Epoch 220/10000      train_loss = 6.995787620544434     val_loss = 7.249942779541016\n",
            "Epoch 230/10000      train_loss = 6.7921552658081055     val_loss = 7.033115386962891\n",
            "Epoch 240/10000      train_loss = 6.593376159667969     val_loss = 6.8215651512146\n",
            "Epoch 250/10000      train_loss = 6.399362564086914     val_loss = 6.615197658538818\n",
            "Epoch 260/10000      train_loss = 6.210027694702148     val_loss = 6.413920879364014\n",
            "Epoch 270/10000      train_loss = 6.025288105010986     val_loss = 6.217642307281494\n",
            "Epoch 280/10000      train_loss = 5.845059394836426     val_loss = 6.026271820068359\n",
            "Epoch 290/10000      train_loss = 5.669261455535889     val_loss = 5.839725494384766\n",
            "Epoch 300/10000      train_loss = 5.497819423675537     val_loss = 5.65792179107666\n",
            "Epoch 310/10000      train_loss = 5.330654621124268     val_loss = 5.480776309967041\n",
            "Epoch 320/10000      train_loss = 5.167689800262451     val_loss = 5.308205604553223\n",
            "Epoch 330/10000      train_loss = 5.008850574493408     val_loss = 5.140130996704102\n",
            "Epoch 340/10000      train_loss = 4.85405969619751     val_loss = 4.976467609405518\n",
            "Epoch 350/10000      train_loss = 4.70324182510376     val_loss = 4.81713342666626\n",
            "Epoch 360/10000      train_loss = 4.556316375732422     val_loss = 4.662039279937744\n",
            "Epoch 370/10000      train_loss = 4.413207530975342     val_loss = 4.511103630065918\n",
            "Epoch 380/10000      train_loss = 4.273837089538574     val_loss = 4.364239692687988\n",
            "Epoch 390/10000      train_loss = 4.138125419616699     val_loss = 4.221358776092529\n",
            "Epoch 400/10000      train_loss = 4.005993843078613     val_loss = 4.0823750495910645\n",
            "Epoch 410/10000      train_loss = 3.877363681793213     val_loss = 3.947199821472168\n",
            "Epoch 420/10000      train_loss = 3.752157211303711     val_loss = 3.8157479763031006\n",
            "Epoch 430/10000      train_loss = 3.630295991897583     val_loss = 3.687931776046753\n",
            "Epoch 440/10000      train_loss = 3.5117075443267822     val_loss = 3.5636708736419678\n",
            "Epoch 450/10000      train_loss = 3.396313428878784     val_loss = 3.4428775310516357\n",
            "Epoch 460/10000      train_loss = 3.284043312072754     val_loss = 3.3254740238189697\n",
            "Epoch 470/10000      train_loss = 3.17482328414917     val_loss = 3.2113776206970215\n",
            "Epoch 480/10000      train_loss = 3.0685839653015137     val_loss = 3.1005096435546875\n",
            "Epoch 490/10000      train_loss = 2.965259313583374     val_loss = 2.9927985668182373\n",
            "Epoch 500/10000      train_loss = 2.8733999729156494     val_loss = 2.8881735801696777\n",
            "Epoch 510/10000      train_loss = 2.7758030891418457     val_loss = 2.7866029739379883\n",
            "Epoch 520/10000      train_loss = 2.68092679977417     val_loss = 2.6879723072052\n",
            "Epoch 530/10000      train_loss = 2.588714122772217     val_loss = 2.592217206954956\n",
            "Epoch 540/10000      train_loss = 2.4991068840026855     val_loss = 2.4992733001708984\n",
            "Epoch 550/10000      train_loss = 2.4120514392852783     val_loss = 2.409080743789673\n",
            "Epoch 560/10000      train_loss = 2.327495574951172     val_loss = 2.3215808868408203\n",
            "Epoch 570/10000      train_loss = 2.245387077331543     val_loss = 2.2367162704467773\n",
            "Epoch 580/10000      train_loss = 2.165678024291992     val_loss = 2.154432773590088\n",
            "Epoch 590/10000      train_loss = 2.08832049369812     val_loss = 2.074676275253296\n",
            "Epoch 600/10000      train_loss = 2.0132691860198975     val_loss = 1.9973965883255005\n",
            "Epoch 610/10000      train_loss = 1.9404802322387695     val_loss = 1.9225448369979858\n",
            "Epoch 620/10000      train_loss = 1.8699105978012085     val_loss = 1.850073218345642\n",
            "Epoch 630/10000      train_loss = 1.8015203475952148     val_loss = 1.7799360752105713\n",
            "Epoch 640/10000      train_loss = 1.7352670431137085     val_loss = 1.7120863199234009\n",
            "Epoch 650/10000      train_loss = 1.6711128950119019     val_loss = 1.6464817523956299\n",
            "Epoch 660/10000      train_loss = 1.6090192794799805     val_loss = 1.5830786228179932\n",
            "Epoch 670/10000      train_loss = 1.5489500761032104     val_loss = 1.5218368768692017\n",
            "Epoch 680/10000      train_loss = 1.490866780281067     val_loss = 1.4627134799957275\n",
            "Epoch 690/10000      train_loss = 1.4347341060638428     val_loss = 1.4056686162948608\n",
            "Epoch 700/10000      train_loss = 1.380516767501831     val_loss = 1.3506628274917603\n",
            "Epoch 710/10000      train_loss = 1.3281795978546143     val_loss = 1.2976559400558472\n",
            "Epoch 720/10000      train_loss = 1.277687668800354     val_loss = 1.2466094493865967\n",
            "Epoch 730/10000      train_loss = 1.2290065288543701     val_loss = 1.1974848508834839\n",
            "Epoch 740/10000      train_loss = 1.182102918624878     val_loss = 1.150244116783142\n",
            "Epoch 750/10000      train_loss = 1.1369414329528809     val_loss = 1.1048481464385986\n",
            "Epoch 760/10000      train_loss = 1.093488335609436     val_loss = 1.0612592697143555\n",
            "Epoch 770/10000      train_loss = 1.0517103672027588     val_loss = 1.019439458847046\n",
            "Epoch 780/10000      train_loss = 1.0115725994110107     val_loss = 0.9793498516082764\n",
            "Epoch 790/10000      train_loss = 0.9730402231216431     val_loss = 0.9409517049789429\n",
            "Epoch 800/10000      train_loss = 0.936079740524292     val_loss = 0.9042072892189026\n",
            "Epoch 810/10000      train_loss = 0.9006561636924744     val_loss = 0.8690778613090515\n",
            "Epoch 820/10000      train_loss = 0.8667341470718384     val_loss = 0.8355234265327454\n",
            "Epoch 830/10000      train_loss = 0.83427894115448     val_loss = 0.8035056591033936\n",
            "Epoch 840/10000      train_loss = 0.8032547235488892     val_loss = 0.7729842662811279\n",
            "Epoch 850/10000      train_loss = 0.7736252546310425     val_loss = 0.7439190745353699\n",
            "Epoch 860/10000      train_loss = 0.745354413986206     val_loss = 0.7162699699401855\n",
            "Epoch 870/10000      train_loss = 0.7184058427810669     val_loss = 0.6899962425231934\n",
            "Epoch 880/10000      train_loss = 0.6927414536476135     val_loss = 0.6650559902191162\n",
            "Epoch 890/10000      train_loss = 0.6683246493339539     val_loss = 0.6414084434509277\n",
            "Epoch 900/10000      train_loss = 0.645116925239563     val_loss = 0.6190112829208374\n",
            "Epoch 910/10000      train_loss = 0.6125973463058472     val_loss = 0.5977851748466492\n",
            "Epoch 920/10000      train_loss = 0.5916186571121216     val_loss = 0.5777279138565063\n",
            "Epoch 930/10000      train_loss = 0.5717369318008423     val_loss = 0.5587968230247498\n",
            "Epoch 940/10000      train_loss = 0.5529128313064575     val_loss = 0.5409479737281799\n",
            "Epoch 950/10000      train_loss = 0.5351068377494812     val_loss = 0.5241383910179138\n",
            "Epoch 960/10000      train_loss = 0.5182796716690063     val_loss = 0.5083248615264893\n",
            "Epoch 970/10000      train_loss = 0.5023916959762573     val_loss = 0.4934639632701874\n",
            "Epoch 980/10000      train_loss = 0.48740291595458984     val_loss = 0.4795120656490326\n",
            "Epoch 990/10000      train_loss = 0.4732736051082611     val_loss = 0.46642592549324036\n",
            "Epoch 1000/10000      train_loss = 0.45996421575546265     val_loss = 0.4541623592376709\n",
            "Epoch 1010/10000      train_loss = 0.44743525981903076     val_loss = 0.4426787197589874\n",
            "Epoch 1020/10000      train_loss = 0.43564754724502563     val_loss = 0.4319324493408203\n",
            "Epoch 1030/10000      train_loss = 0.40864840149879456     val_loss = 0.4107637107372284\n",
            "Epoch 1040/10000      train_loss = 0.39947235584259033     val_loss = 0.40216872096061707\n",
            "Epoch 1050/10000      train_loss = 0.39084309339523315     val_loss = 0.39413684606552124\n",
            "Epoch 1060/10000      train_loss = 0.3827258348464966     val_loss = 0.386629194021225\n",
            "Epoch 1070/10000      train_loss = 0.37508708238601685     val_loss = 0.37960824370384216\n",
            "Epoch 1080/10000      train_loss = 0.36789408326148987     val_loss = 0.37303733825683594\n",
            "Epoch 1090/10000      train_loss = 0.3611155152320862     val_loss = 0.3668816089630127\n",
            "Epoch 1100/10000      train_loss = 0.3547205328941345     val_loss = 0.3611071705818176\n",
            "Epoch 1110/10000      train_loss = 0.34868013858795166     val_loss = 0.3556815981864929\n",
            "Epoch 1120/10000      train_loss = 0.3429659307003021     val_loss = 0.35057392716407776\n",
            "Epoch 1130/10000      train_loss = 0.3375508487224579     val_loss = 0.34575438499450684\n",
            "Epoch 1140/10000      train_loss = 0.33240941166877747     val_loss = 0.341195285320282\n",
            "Epoch 1150/10000      train_loss = 0.32751667499542236     val_loss = 0.33686938881874084\n",
            "Epoch 1160/10000      train_loss = 0.32284945249557495     val_loss = 0.3327517807483673\n",
            "Epoch 1170/10000      train_loss = 0.31838592886924744     val_loss = 0.3288188874721527\n",
            "Epoch 1180/10000      train_loss = 0.31410539150238037     val_loss = 0.32504868507385254\n",
            "Epoch 1190/10000      train_loss = 0.309988409280777     val_loss = 0.32142043113708496\n",
            "Epoch 1200/10000      train_loss = 0.3060167133808136     val_loss = 0.31791502237319946\n",
            "Epoch 1210/10000      train_loss = 0.30217355489730835     val_loss = 0.31451475620269775\n",
            "Epoch 1220/10000      train_loss = 0.298443466424942     val_loss = 0.31120365858078003\n",
            "Epoch 1230/10000      train_loss = 0.2948119044303894     val_loss = 0.30796658992767334\n",
            "Epoch 1240/10000      train_loss = 0.2912655472755432     val_loss = 0.3047902286052704\n",
            "Epoch 1250/10000      train_loss = 0.287792444229126     val_loss = 0.3016623258590698\n",
            "Epoch 1260/10000      train_loss = 0.28438133001327515     val_loss = 0.2985718250274658\n",
            "Epoch 1270/10000      train_loss = 0.28102239966392517     val_loss = 0.2955089807510376\n",
            "Epoch 1280/10000      train_loss = 0.2777066230773926     val_loss = 0.29246512055397034\n",
            "Epoch 1290/10000      train_loss = 0.2744258642196655     val_loss = 0.2894325256347656\n",
            "Epoch 1300/10000      train_loss = 0.27117300033569336     val_loss = 0.28640443086624146\n",
            "Epoch 1310/10000      train_loss = 0.2679414749145508     val_loss = 0.2833750247955322\n",
            "Epoch 1320/10000      train_loss = 0.26472586393356323     val_loss = 0.28033947944641113\n",
            "Epoch 1330/10000      train_loss = 0.26152095198631287     val_loss = 0.27729329466819763\n",
            "Epoch 1340/10000      train_loss = 0.25832265615463257     val_loss = 0.27423304319381714\n",
            "Epoch 1350/10000      train_loss = 0.25512707233428955     val_loss = 0.2711557447910309\n",
            "Epoch 1360/10000      train_loss = 0.2519311308860779     val_loss = 0.26805901527404785\n",
            "Epoch 1370/10000      train_loss = 0.2487320452928543     val_loss = 0.2649409770965576\n",
            "Epoch 1380/10000      train_loss = 0.24552753567695618     val_loss = 0.26180025935173035\n",
            "Epoch 1390/10000      train_loss = 0.24231579899787903     val_loss = 0.2586357593536377\n",
            "Epoch 1400/10000      train_loss = 0.23909509181976318     val_loss = 0.2554466128349304\n",
            "Epoch 1410/10000      train_loss = 0.23586438596248627     val_loss = 0.25223273038864136\n",
            "Epoch 1420/10000      train_loss = 0.2326226681470871     val_loss = 0.24899375438690186\n",
            "Epoch 1430/10000      train_loss = 0.22936919331550598     val_loss = 0.24572984874248505\n",
            "Epoch 1440/10000      train_loss = 0.22610345482826233     val_loss = 0.24244122207164764\n",
            "Epoch 1450/10000      train_loss = 0.22282524406909943     val_loss = 0.2391284555196762\n",
            "Epoch 1460/10000      train_loss = 0.21953432261943817     val_loss = 0.23579193651676178\n",
            "Epoch 1470/10000      train_loss = 0.21623080968856812     val_loss = 0.23243257403373718\n",
            "Epoch 1480/10000      train_loss = 0.21291480958461761     val_loss = 0.22905117273330688\n",
            "Epoch 1490/10000      train_loss = 0.20958662033081055     val_loss = 0.2256484180688858\n",
            "Epoch 1500/10000      train_loss = 0.20624667406082153     val_loss = 0.22222557663917542\n",
            "Epoch 1510/10000      train_loss = 0.20289528369903564     val_loss = 0.2187834233045578\n",
            "Epoch 1520/10000      train_loss = 0.19953319430351257     val_loss = 0.21532322466373444\n",
            "Epoch 1530/10000      train_loss = 0.1961609572172165     val_loss = 0.21184608340263367\n",
            "Epoch 1540/10000      train_loss = 0.19277942180633545     val_loss = 0.20835329592227936\n",
            "Epoch 1550/10000      train_loss = 0.1893892139196396     val_loss = 0.20484590530395508\n",
            "Epoch 1560/10000      train_loss = 0.18599140644073486     val_loss = 0.2013254165649414\n",
            "Epoch 1570/10000      train_loss = 0.18258686363697052     val_loss = 0.19779294729232788\n",
            "Epoch 1580/10000      train_loss = 0.1791764497756958     val_loss = 0.1942499577999115\n",
            "Epoch 1590/10000      train_loss = 0.17576143145561218     val_loss = 0.19069786369800568\n",
            "Epoch 1600/10000      train_loss = 0.17234280705451965     val_loss = 0.18713806569576263\n",
            "Epoch 1610/10000      train_loss = 0.16892188787460327     val_loss = 0.18357208371162415\n",
            "Epoch 1620/10000      train_loss = 0.16549989581108093     val_loss = 0.18000154197216034\n",
            "Epoch 1630/10000      train_loss = 0.16207808256149292     val_loss = 0.17642776668071747\n",
            "Epoch 1640/10000      train_loss = 0.1586577594280243     val_loss = 0.1728525608778\n",
            "Epoch 1650/10000      train_loss = 0.15524037182331085     val_loss = 0.1692773699760437\n",
            "Epoch 1660/10000      train_loss = 0.15182749927043915     val_loss = 0.16570404171943665\n",
            "Epoch 1670/10000      train_loss = 0.14842061698436737     val_loss = 0.1621343195438385\n",
            "Epoch 1680/10000      train_loss = 0.14502127468585968     val_loss = 0.15856990218162537\n",
            "Epoch 1690/10000      train_loss = 0.1416311264038086     val_loss = 0.15501266717910767\n",
            "Epoch 1700/10000      train_loss = 0.1382519006729126     val_loss = 0.15146441757678986\n",
            "Epoch 1710/10000      train_loss = 0.13488519191741943     val_loss = 0.1479269564151764\n",
            "Epoch 1720/10000      train_loss = 0.1315329372882843     val_loss = 0.14440244436264038\n",
            "Epoch 1730/10000      train_loss = 0.12819692492485046     val_loss = 0.14089271426200867\n",
            "Epoch 1740/10000      train_loss = 0.12487892806529999     val_loss = 0.13739976286888123\n",
            "Epoch 1750/10000      train_loss = 0.12158095836639404     val_loss = 0.1339256763458252\n",
            "Epoch 1760/10000      train_loss = 0.11830491572618484     val_loss = 0.1304725557565689\n",
            "Epoch 1770/10000      train_loss = 0.11505279690027237     val_loss = 0.12704242765903473\n",
            "Epoch 1780/10000      train_loss = 0.11182643473148346     val_loss = 0.12363731116056442\n",
            "Epoch 1790/10000      train_loss = 0.1086280345916748     val_loss = 0.12025949358940125\n",
            "Epoch 1800/10000      train_loss = 0.1054592877626419     val_loss = 0.11691079288721085\n",
            "Epoch 1810/10000      train_loss = 0.10232248902320862     val_loss = 0.1135937049984932\n",
            "Epoch 1820/10000      train_loss = 0.09921959042549133     val_loss = 0.1103101372718811\n",
            "Epoch 1830/10000      train_loss = 0.09615274518728256     val_loss = 0.10706246644258499\n",
            "Epoch 1840/10000      train_loss = 0.09312386065721512     val_loss = 0.10385263711214066\n",
            "Epoch 1850/10000      train_loss = 0.09013503789901733     val_loss = 0.10068288445472717\n",
            "Epoch 1860/10000      train_loss = 0.08718819916248322     val_loss = 0.09755522012710571\n",
            "Epoch 1870/10000      train_loss = 0.084285207092762     val_loss = 0.09447166323661804\n",
            "Epoch 1880/10000      train_loss = 0.0814281478524208     val_loss = 0.09143433719873428\n",
            "Epoch 1890/10000      train_loss = 0.07861894369125366     val_loss = 0.0884452611207962\n",
            "Epoch 1900/10000      train_loss = 0.07585926353931427     val_loss = 0.08550624549388885\n",
            "Epoch 1910/10000      train_loss = 0.07315093278884888     val_loss = 0.08261916786432266\n",
            "Epoch 1920/10000      train_loss = 0.07049565017223358     val_loss = 0.07978584617376328\n",
            "Epoch 1930/10000      train_loss = 0.06789499521255493     val_loss = 0.07700798660516739\n",
            "Epoch 1940/10000      train_loss = 0.06535059213638306     val_loss = 0.07428725063800812\n",
            "Epoch 1950/10000      train_loss = 0.0628637820482254     val_loss = 0.07162503898143768\n",
            "Epoch 1960/10000      train_loss = 0.060435906052589417     val_loss = 0.06902286410331726\n",
            "Epoch 1970/10000      train_loss = 0.05806821212172508     val_loss = 0.06648200750350952\n",
            "Epoch 1980/10000      train_loss = 0.05576175078749657     val_loss = 0.06400366872549057\n",
            "Epoch 1990/10000      train_loss = 0.05351746454834938     val_loss = 0.06158871948719025\n",
            "Epoch 2000/10000      train_loss = 0.05133624002337456     val_loss = 0.05923828110098839\n",
            "Epoch 2010/10000      train_loss = 0.04921870306134224     val_loss = 0.05695296451449394\n",
            "Epoch 2020/10000      train_loss = 0.04716537892818451     val_loss = 0.05473337322473526\n",
            "Epoch 2030/10000      train_loss = 0.045176681131124496     val_loss = 0.052579980343580246\n",
            "Epoch 2040/10000      train_loss = 0.043252818286418915     val_loss = 0.050493065267801285\n",
            "Epoch 2050/10000      train_loss = 0.04139392077922821     val_loss = 0.0484728142619133\n",
            "Epoch 2060/10000      train_loss = 0.039599765092134476     val_loss = 0.04651905596256256\n",
            "Epoch 2070/10000      train_loss = 0.03787023574113846     val_loss = 0.04463168978691101\n",
            "Epoch 2080/10000      train_loss = 0.036204904317855835     val_loss = 0.04281037300825119\n",
            "Epoch 2090/10000      train_loss = 0.03460317850112915     val_loss = 0.04105452448129654\n",
            "Epoch 2100/10000      train_loss = 0.03306445851922035     val_loss = 0.03936358913779259\n",
            "Epoch 2110/10000      train_loss = 0.03158789128065109     val_loss = 0.03773672878742218\n",
            "Epoch 2120/10000      train_loss = 0.030172565951943398     val_loss = 0.03617306426167488\n",
            "Epoch 2130/10000      train_loss = 0.028817428275942802     val_loss = 0.0346715971827507\n",
            "Epoch 2140/10000      train_loss = 0.0275214072316885     val_loss = 0.033231280744075775\n",
            "Epoch 2150/10000      train_loss = 0.0262831412255764     val_loss = 0.031850751489400864\n",
            "Epoch 2160/10000      train_loss = 0.02510128542780876     val_loss = 0.030528707429766655\n",
            "Epoch 2170/10000      train_loss = 0.02397453971207142     val_loss = 0.029263881966471672\n",
            "Epoch 2180/10000      train_loss = 0.02290135622024536     val_loss = 0.02805468626320362\n",
            "Epoch 2190/10000      train_loss = 0.021880274638533592     val_loss = 0.026899755001068115\n",
            "Epoch 2200/10000      train_loss = 0.02090962417423725     val_loss = 0.025797361508011818\n",
            "Epoch 2210/10000      train_loss = 0.01998787373304367     val_loss = 0.024746060371398926\n",
            "Epoch 2220/10000      train_loss = 0.01911335438489914     val_loss = 0.02374418079853058\n",
            "Epoch 2230/10000      train_loss = 0.01828444004058838     val_loss = 0.022790079936385155\n",
            "Epoch 2240/10000      train_loss = 0.017499476671218872     val_loss = 0.021882152184844017\n",
            "Epoch 2250/10000      train_loss = 0.016756782308220863     val_loss = 0.021018709987401962\n",
            "Epoch 2260/10000      train_loss = 0.01605476811528206     val_loss = 0.02019817754626274\n",
            "Epoch 2270/10000      train_loss = 0.015391752123832703     val_loss = 0.019418878480792046\n",
            "Epoch 2280/10000      train_loss = 0.014766111969947815     val_loss = 0.018679220229387283\n",
            "Epoch 2290/10000      train_loss = 0.014176245778799057     val_loss = 0.01797758787870407\n",
            "Epoch 2300/10000      train_loss = 0.013620627112686634     val_loss = 0.01731247454881668\n",
            "Epoch 2310/10000      train_loss = 0.013097630813717842     val_loss = 0.016682248562574387\n",
            "Epoch 2320/10000      train_loss = 0.012605806812644005     val_loss = 0.016085468232631683\n",
            "Epoch 2330/10000      train_loss = 0.012143687345087528     val_loss = 0.015520650893449783\n",
            "Epoch 2340/10000      train_loss = 0.011709813959896564     val_loss = 0.014986379072070122\n",
            "Epoch 2350/10000      train_loss = 0.011302792467176914     val_loss = 0.014481221325695515\n",
            "Epoch 2360/10000      train_loss = 0.01092123705893755     val_loss = 0.014003799296915531\n",
            "Epoch 2370/10000      train_loss = 0.010563869960606098     val_loss = 0.013552835211157799\n",
            "Epoch 2380/10000      train_loss = 0.010229391977190971     val_loss = 0.013127007521688938\n",
            "Epoch 2390/10000      train_loss = 0.009916574694216251     val_loss = 0.012725090608000755\n",
            "Epoch 2400/10000      train_loss = 0.009624207392334938     val_loss = 0.012345871888101101\n",
            "Epoch 2410/10000      train_loss = 0.00935119204223156     val_loss = 0.011988233774900436\n",
            "Epoch 2420/10000      train_loss = 0.009096375666558743     val_loss = 0.011651020497083664\n",
            "Epoch 2430/10000      train_loss = 0.008858714252710342     val_loss = 0.011333164758980274\n",
            "Epoch 2440/10000      train_loss = 0.008637193590402603     val_loss = 0.011033648625016212\n",
            "Epoch 2450/10000      train_loss = 0.008430816233158112     val_loss = 0.010751444846391678\n",
            "Epoch 2460/10000      train_loss = 0.008238663896918297     val_loss = 0.010485639795660973\n",
            "Epoch 2470/10000      train_loss = 0.008059817366302013     val_loss = 0.010235275141894817\n",
            "Epoch 2480/10000      train_loss = 0.00789343286305666     val_loss = 0.00999949499964714\n",
            "Epoch 2490/10000      train_loss = 0.007738688960671425     val_loss = 0.009777459315955639\n",
            "Epoch 2500/10000      train_loss = 0.007594783790409565     val_loss = 0.009568344801664352\n",
            "Epoch 2510/10000      train_loss = 0.007461002096533775     val_loss = 0.009371411055326462\n",
            "Epoch 2520/10000      train_loss = 0.007336628157645464     val_loss = 0.009185899049043655\n",
            "Epoch 2530/10000      train_loss = 0.007220993284136057     val_loss = 0.009011157788336277\n",
            "Epoch 2540/10000      train_loss = 0.007113451138138771     val_loss = 0.008846469223499298\n",
            "Epoch 2550/10000      train_loss = 0.007013414055109024     val_loss = 0.008691241964697838\n",
            "Epoch 2560/10000      train_loss = 0.006920313462615013     val_loss = 0.008544879034161568\n",
            "Epoch 2570/10000      train_loss = 0.0068335989490151405     val_loss = 0.008406778797507286\n",
            "Epoch 2580/10000      train_loss = 0.006752785760909319     val_loss = 0.008276441134512424\n",
            "Epoch 2590/10000      train_loss = 0.006677396595478058     val_loss = 0.008153355680406094\n",
            "Epoch 2600/10000      train_loss = 0.006606986280530691     val_loss = 0.008037038147449493\n",
            "Epoch 2610/10000      train_loss = 0.006541133392602205     val_loss = 0.00792702566832304\n",
            "Epoch 2620/10000      train_loss = 0.0064794644713401794     val_loss = 0.0078229159116745\n",
            "Epoch 2630/10000      train_loss = 0.0064216069877147675     val_loss = 0.007724294438958168\n",
            "Epoch 2640/10000      train_loss = 0.00636723916977644     val_loss = 0.007630803622305393\n",
            "Epoch 2650/10000      train_loss = 0.006316050887107849     val_loss = 0.007542094215750694\n",
            "Epoch 2660/10000      train_loss = 0.006267748773097992     val_loss = 0.0074578262865543365\n",
            "Epoch 2670/10000      train_loss = 0.006222068332135677     val_loss = 0.0073777008801698685\n",
            "Epoch 2680/10000      train_loss = 0.006178772076964378     val_loss = 0.007301426026970148\n",
            "Epoch 2690/10000      train_loss = 0.006137635093182325     val_loss = 0.007228742353618145\n",
            "Epoch 2700/10000      train_loss = 0.006098457612097263     val_loss = 0.007159404922276735\n",
            "Epoch 2710/10000      train_loss = 0.006061053369194269     val_loss = 0.00709318183362484\n",
            "Epoch 2720/10000      train_loss = 0.006025254726409912     val_loss = 0.007029858883470297\n",
            "Epoch 2730/10000      train_loss = 0.005990906152874231     val_loss = 0.006969226524233818\n",
            "Epoch 2740/10000      train_loss = 0.005957877263426781     val_loss = 0.006911132484674454\n",
            "Epoch 2750/10000      train_loss = 0.005926044657826424     val_loss = 0.006855388171970844\n",
            "Epoch 2760/10000      train_loss = 0.005895289592444897     val_loss = 0.006801824551075697\n",
            "Epoch 2770/10000      train_loss = 0.005865517072379589     val_loss = 0.006750313099473715\n",
            "Epoch 2780/10000      train_loss = 0.005836644675582647     val_loss = 0.006700719706714153\n",
            "Epoch 2790/10000      train_loss = 0.005808594170957804     val_loss = 0.0066529326140880585\n",
            "Epoch 2800/10000      train_loss = 0.005781291984021664     val_loss = 0.0066068219020962715\n",
            "Epoch 2810/10000      train_loss = 0.005754680838435888     val_loss = 0.00656227907165885\n",
            "Epoch 2820/10000      train_loss = 0.005728696007281542     val_loss = 0.006519215181469917\n",
            "Epoch 2830/10000      train_loss = 0.005703307222574949     val_loss = 0.006477547809481621\n",
            "Epoch 2840/10000      train_loss = 0.005678462330251932     val_loss = 0.006437184289097786\n",
            "Epoch 2850/10000      train_loss = 0.0056541236117482185     val_loss = 0.0063980440609157085\n",
            "Epoch 2860/10000      train_loss = 0.005630255676805973     val_loss = 0.006360049359500408\n",
            "Epoch 2870/10000      train_loss = 0.005606840364634991     val_loss = 0.00632314570248127\n",
            "Epoch 2880/10000      train_loss = 0.0055838353000581264     val_loss = 0.006287249270826578\n",
            "Epoch 2890/10000      train_loss = 0.005561234429478645     val_loss = 0.006252331659197807\n",
            "Epoch 2900/10000      train_loss = 0.0055390046909451485     val_loss = 0.0062183234840631485\n",
            "Epoch 2910/10000      train_loss = 0.005517135374248028     val_loss = 0.006185164675116539\n",
            "Epoch 2920/10000      train_loss = 0.005495600402355194     val_loss = 0.006152810528874397\n",
            "Epoch 2930/10000      train_loss = 0.005474390462040901     val_loss = 0.006121222861111164\n",
            "Epoch 2940/10000      train_loss = 0.005453482270240784     val_loss = 0.006090350914746523\n",
            "Epoch 2950/10000      train_loss = 0.00543286744505167     val_loss = 0.006060150917619467\n",
            "Epoch 2960/10000      train_loss = 0.005412526428699493     val_loss = 0.00603058934211731\n",
            "Epoch 2970/10000      train_loss = 0.005392448976635933     val_loss = 0.006001629866659641\n",
            "Epoch 2980/10000      train_loss = 0.005372620653361082     val_loss = 0.00597322266548872\n",
            "Epoch 2990/10000      train_loss = 0.005353021435439587     val_loss = 0.00594534445554018\n",
            "Epoch 3000/10000      train_loss = 0.005333648528903723     val_loss = 0.0059179747477173805\n",
            "Epoch 3010/10000      train_loss = 0.005314478650689125     val_loss = 0.005891065578907728\n",
            "Epoch 3020/10000      train_loss = 0.005295503418892622     val_loss = 0.005864589940756559\n",
            "Epoch 3030/10000      train_loss = 0.0052767046727240086     val_loss = 0.005838521756231785\n",
            "Epoch 3040/10000      train_loss = 0.005258071701973677     val_loss = 0.0058128321543335915\n",
            "Epoch 3050/10000      train_loss = 0.005239586345851421     val_loss = 0.005787496455013752\n",
            "Epoch 3060/10000      train_loss = 0.005221238359808922     val_loss = 0.0057624876499176025\n",
            "Epoch 3070/10000      train_loss = 0.005203007720410824     val_loss = 0.00573778385296464\n",
            "Epoch 3080/10000      train_loss = 0.005184887908399105     val_loss = 0.005713369697332382\n",
            "Epoch 3090/10000      train_loss = 0.005166858900338411     val_loss = 0.005689199082553387\n",
            "Epoch 3100/10000      train_loss = 0.005148907192051411     val_loss = 0.005665263161063194\n",
            "Epoch 3110/10000      train_loss = 0.005131021607667208     val_loss = 0.005641551688313484\n",
            "Epoch 3120/10000      train_loss = 0.005113184452056885     val_loss = 0.005618039518594742\n",
            "Epoch 3130/10000      train_loss = 0.00509538222104311     val_loss = 0.005594712682068348\n",
            "Epoch 3140/10000      train_loss = 0.005077601410448551     val_loss = 0.0055715362541377544\n",
            "Epoch 3150/10000      train_loss = 0.005059828516095877     val_loss = 0.005548502318561077\n",
            "Epoch 3160/10000      train_loss = 0.005042047705501318     val_loss = 0.005525579676032066\n",
            "Epoch 3170/10000      train_loss = 0.0050242505967617035     val_loss = 0.005502769723534584\n",
            "Epoch 3180/10000      train_loss = 0.00500642042607069     val_loss = 0.005480056628584862\n",
            "Epoch 3190/10000      train_loss = 0.004988546017557383     val_loss = 0.005457418505102396\n",
            "Epoch 3200/10000      train_loss = 0.004970616661012173     val_loss = 0.005434843245893717\n",
            "Epoch 3210/10000      train_loss = 0.004952619317919016     val_loss = 0.005412306636571884\n",
            "Epoch 3220/10000      train_loss = 0.004934545140713453     val_loss = 0.00538981007412076\n",
            "Epoch 3230/10000      train_loss = 0.004916377365589142     val_loss = 0.0053673298098146915\n",
            "Epoch 3240/10000      train_loss = 0.004898111801594496     val_loss = 0.005344856530427933\n",
            "Epoch 3250/10000      train_loss = 0.004879737738519907     val_loss = 0.0053223734721541405\n",
            "Epoch 3260/10000      train_loss = 0.0048612430691719055     val_loss = 0.005299875512719154\n",
            "Epoch 3270/10000      train_loss = 0.004842619877308607     val_loss = 0.0052773477509617805\n",
            "Epoch 3280/10000      train_loss = 0.0048238616436719894     val_loss = 0.005254777614027262\n",
            "Epoch 3290/10000      train_loss = 0.0048049623146653175     val_loss = 0.005232161842286587\n",
            "Epoch 3300/10000      train_loss = 0.004785912111401558     val_loss = 0.005209484603255987\n",
            "Epoch 3310/10000      train_loss = 0.004766704514622688     val_loss = 0.005186736583709717\n",
            "Epoch 3320/10000      train_loss = 0.004747328814119101     val_loss = 0.005163911730051041\n",
            "Epoch 3330/10000      train_loss = 0.004727788269519806     val_loss = 0.005140999332070351\n",
            "Epoch 3340/10000      train_loss = 0.004708070773631334     val_loss = 0.005117989145219326\n",
            "Epoch 3350/10000      train_loss = 0.004688174929469824     val_loss = 0.005094877444207668\n",
            "Epoch 3360/10000      train_loss = 0.004668097477406263     val_loss = 0.005071654450148344\n",
            "Epoch 3370/10000      train_loss = 0.0046478318981826305     val_loss = 0.0050483159720897675\n",
            "Epoch 3380/10000      train_loss = 0.004627374932169914     val_loss = 0.0050248559564352036\n",
            "Epoch 3390/10000      train_loss = 0.004606723785400391     val_loss = 0.00500126788392663\n",
            "Epoch 3400/10000      train_loss = 0.004585880786180496     val_loss = 0.004977537784725428\n",
            "Epoch 3410/10000      train_loss = 0.004564834758639336     val_loss = 0.004953673575073481\n",
            "Epoch 3420/10000      train_loss = 0.004543589893728495     val_loss = 0.004929658025503159\n",
            "Epoch 3430/10000      train_loss = 0.004522144794464111     val_loss = 0.00490549486130476\n",
            "Epoch 3440/10000      train_loss = 0.004500498063862324     val_loss = 0.00488117802888155\n",
            "Epoch 3450/10000      train_loss = 0.004478643648326397     val_loss = 0.004856704268604517\n",
            "Epoch 3460/10000      train_loss = 0.004456590861082077     val_loss = 0.0048320661298930645\n",
            "Epoch 3470/10000      train_loss = 0.004434330854564905     val_loss = 0.004807259421795607\n",
            "Epoch 3480/10000      train_loss = 0.0044118668884038925     val_loss = 0.004782285075634718\n",
            "Epoch 3490/10000      train_loss = 0.00438920222222805     val_loss = 0.004757151938974857\n",
            "Epoch 3500/10000      train_loss = 0.0043663326650857925     val_loss = 0.004731836263090372\n",
            "Epoch 3510/10000      train_loss = 0.0043432628735899925     val_loss = 0.0047063506208360195\n",
            "Epoch 3520/10000      train_loss = 0.004319989588111639     val_loss = 0.004680687561631203\n",
            "Epoch 3530/10000      train_loss = 0.004296516999602318     val_loss = 0.004654846619814634\n",
            "Epoch 3540/10000      train_loss = 0.004272846970707178     val_loss = 0.004628831520676613\n",
            "Epoch 3550/10000      train_loss = 0.004248980898410082     val_loss = 0.0046026320196688175\n",
            "Epoch 3560/10000      train_loss = 0.004224915988743305     val_loss = 0.004576254636049271\n",
            "Epoch 3570/10000      train_loss = 0.004200661554932594     val_loss = 0.004549694247543812\n",
            "Epoch 3580/10000      train_loss = 0.004176216199994087     val_loss = 0.004522957839071751\n",
            "Epoch 3590/10000      train_loss = 0.004151576664298773     val_loss = 0.004496037028729916\n",
            "Epoch 3600/10000      train_loss = 0.004126755055040121     val_loss = 0.004468938801437616\n",
            "Epoch 3610/10000      train_loss = 0.004101746715605259     val_loss = 0.004441666416823864\n",
            "Epoch 3620/10000      train_loss = 0.004076557233929634     val_loss = 0.004414209630340338\n",
            "Epoch 3630/10000      train_loss = 0.004051188938319683     val_loss = 0.004386582411825657\n",
            "Epoch 3640/10000      train_loss = 0.004025643691420555     val_loss = 0.004358775448054075\n",
            "Epoch 3650/10000      train_loss = 0.003999922890216112     val_loss = 0.004330792929977179\n",
            "Epoch 3660/10000      train_loss = 0.003974032122641802     val_loss = 0.004302637651562691\n",
            "Epoch 3670/10000      train_loss = 0.0039479732513427734     val_loss = 0.004274318926036358\n",
            "Epoch 3680/10000      train_loss = 0.003921751398593187     val_loss = 0.004245830234140158\n",
            "Epoch 3690/10000      train_loss = 0.0038953651674091816     val_loss = 0.004217172972857952\n",
            "Epoch 3700/10000      train_loss = 0.0038688231725245714     val_loss = 0.004188354592770338\n",
            "Epoch 3710/10000      train_loss = 0.003842124482616782     val_loss = 0.0041593690402805805\n",
            "Epoch 3720/10000      train_loss = 0.0038152746856212616     val_loss = 0.00413023354485631\n",
            "Epoch 3730/10000      train_loss = 0.0037882744800299406     val_loss = 0.004100939258933067\n",
            "Epoch 3740/10000      train_loss = 0.0037611322477459908     val_loss = 0.004071490839123726\n",
            "Epoch 3750/10000      train_loss = 0.0037338477559387684     val_loss = 0.004041892476379871\n",
            "Epoch 3760/10000      train_loss = 0.0037064235657453537     val_loss = 0.0040121544152498245\n",
            "Epoch 3770/10000      train_loss = 0.00367886945605278     val_loss = 0.003982270136475563\n",
            "Epoch 3780/10000      train_loss = 0.003651183098554611     val_loss = 0.003952245227992535\n",
            "Epoch 3790/10000      train_loss = 0.003623370546847582     val_loss = 0.003922089468687773\n",
            "Epoch 3800/10000      train_loss = 0.0035954357590526342     val_loss = 0.003891799133270979\n",
            "Epoch 3810/10000      train_loss = 0.0035673831589519978     val_loss = 0.0038613928481936455\n",
            "Epoch 3820/10000      train_loss = 0.0035392139106988907     val_loss = 0.0038308543153107166\n",
            "Epoch 3830/10000      train_loss = 0.003510937327519059     val_loss = 0.0038001942448318005\n",
            "Epoch 3840/10000      train_loss = 0.0034825545735657215     val_loss = 0.003769424045458436\n",
            "Epoch 3850/10000      train_loss = 0.00345406960695982     val_loss = 0.0037385420873761177\n",
            "Epoch 3860/10000      train_loss = 0.003425485920161009     val_loss = 0.003707554889842868\n",
            "Epoch 3870/10000      train_loss = 0.003396805841475725     val_loss = 0.0036764636170119047\n",
            "Epoch 3880/10000      train_loss = 0.0033680403139442205     val_loss = 0.0036452708300203085\n",
            "Epoch 3890/10000      train_loss = 0.003339187242090702     val_loss = 0.0036139923613518476\n",
            "Epoch 3900/10000      train_loss = 0.0033102529123425484     val_loss = 0.0035826244857162237\n",
            "Epoch 3910/10000      train_loss = 0.0032812412828207016     val_loss = 0.0035511713940650225\n",
            "Epoch 3920/10000      train_loss = 0.0032521598041057587     val_loss = 0.00351964239962399\n",
            "Epoch 3930/10000      train_loss = 0.003223010106012225     val_loss = 0.0034880354069173336\n",
            "Epoch 3940/10000      train_loss = 0.003193795448169112     val_loss = 0.0034563643857836723\n",
            "Epoch 3950/10000      train_loss = 0.003164521884173155     val_loss = 0.003424624912440777\n",
            "Epoch 3960/10000      train_loss = 0.003135195467621088     val_loss = 0.0033928249031305313\n",
            "Epoch 3970/10000      train_loss = 0.003105815965682268     val_loss = 0.0033609711099416018\n",
            "Epoch 3980/10000      train_loss = 0.0030763933900743723     val_loss = 0.003329063067212701\n",
            "Epoch 3990/10000      train_loss = 0.003046926576644182     val_loss = 0.003297120099887252\n",
            "Epoch 4000/10000      train_loss = 0.003017426934093237     val_loss = 0.0032651328947395086\n",
            "Epoch 4010/10000      train_loss = 0.0029878919012844563     val_loss = 0.003233104944229126\n",
            "Epoch 4020/10000      train_loss = 0.0029583305586129427     val_loss = 0.003201055573299527\n",
            "Epoch 4030/10000      train_loss = 0.0029287454672157764     val_loss = 0.0031689738389104605\n",
            "Epoch 4040/10000      train_loss = 0.002899142913520336     val_loss = 0.003136879298835993\n",
            "Epoch 4050/10000      train_loss = 0.0028695270884782076     val_loss = 0.0031047710217535496\n",
            "Epoch 4060/10000      train_loss = 0.0028398986905813217     val_loss = 0.003072639461606741\n",
            "Epoch 4070/10000      train_loss = 0.0028102651704102755     val_loss = 0.003040505340322852\n",
            "Epoch 4080/10000      train_loss = 0.002780631883069873     val_loss = 0.0030083744786679745\n",
            "Epoch 4090/10000      train_loss = 0.002751003485172987     val_loss = 0.0029762524645775557\n",
            "Epoch 4100/10000      train_loss = 0.0027213823050260544     val_loss = 0.0029441348742693663\n",
            "Epoch 4110/10000      train_loss = 0.0026917734649032354     val_loss = 0.0029120314866304398\n",
            "Epoch 4120/10000      train_loss = 0.0026621795259416103     val_loss = 0.0028799439314752817\n",
            "Epoch 4130/10000      train_loss = 0.0026326116640120745     val_loss = 0.002847882453352213\n",
            "Epoch 4140/10000      train_loss = 0.002603063825517893     val_loss = 0.00281584938056767\n",
            "Epoch 4150/10000      train_loss = 0.0025735455565154552     val_loss = 0.002783850533887744\n",
            "Epoch 4160/10000      train_loss = 0.0025440650060772896     val_loss = 0.002751885913312435\n",
            "Epoch 4170/10000      train_loss = 0.0025146231055259705     val_loss = 0.002719963202252984\n",
            "Epoch 4180/10000      train_loss = 0.0024852226488292217     val_loss = 0.002688091481104493\n",
            "Epoch 4190/10000      train_loss = 0.0024558696895837784     val_loss = 0.0026562735438346863\n",
            "Epoch 4200/10000      train_loss = 0.0024265670217573643     val_loss = 0.0026245014742016792\n",
            "Epoch 4210/10000      train_loss = 0.002397318836301565     val_loss = 0.002592797391116619\n",
            "Epoch 4220/10000      train_loss = 0.0023681330494582653     val_loss = 0.0025611575692892075\n",
            "Epoch 4230/10000      train_loss = 0.0023390082642436028     val_loss = 0.0025295824743807316\n",
            "Epoch 4240/10000      train_loss = 0.0023099523968994617     val_loss = 0.002498089103028178\n",
            "Epoch 4250/10000      train_loss = 0.0022809654474258423     val_loss = 0.002466674195602536\n",
            "Epoch 4260/10000      train_loss = 0.0022520602215081453     val_loss = 0.002435337519273162\n",
            "Epoch 4270/10000      train_loss = 0.0022232301998883486     val_loss = 0.0024040909484028816\n",
            "Epoch 4280/10000      train_loss = 0.0021944832988083363     val_loss = 0.002372934715822339\n",
            "Epoch 4290/10000      train_loss = 0.0021658253390341997     val_loss = 0.002341873710975051\n",
            "Epoch 4300/10000      train_loss = 0.002137258183211088     val_loss = 0.002310912823304534\n",
            "Epoch 4310/10000      train_loss = 0.0021087864879518747     val_loss = 0.00228006299585104\n",
            "Epoch 4320/10000      train_loss = 0.0020804130472242832     val_loss = 0.0022493109572678804\n",
            "Epoch 4330/10000      train_loss = 0.0020521413534879684     val_loss = 0.002218676498159766\n",
            "Epoch 4340/10000      train_loss = 0.0020239767618477345     val_loss = 0.0021881600841879845\n",
            "Epoch 4350/10000      train_loss = 0.001995922066271305     val_loss = 0.0021577561274170876\n",
            "Epoch 4360/10000      train_loss = 0.0019679793622344732     val_loss = 0.0021274832542985678\n",
            "Epoch 4370/10000      train_loss = 0.0019401509780436754     val_loss = 0.0020973391365259886\n",
            "Epoch 4380/10000      train_loss = 0.0019124471582472324     val_loss = 0.0020673212129622698\n",
            "Epoch 4390/10000      train_loss = 0.0018848648760467768     val_loss = 0.002037437865510583\n",
            "Epoch 4400/10000      train_loss = 0.0018574101850390434     val_loss = 0.0020076974760740995\n",
            "Epoch 4410/10000      train_loss = 0.0018300879746675491     val_loss = 0.001978097017854452\n",
            "Epoch 4420/10000      train_loss = 0.0018028971971943974     val_loss = 0.0019486446399241686\n",
            "Epoch 4430/10000      train_loss = 0.0017758451867848635     val_loss = 0.00191934360191226\n",
            "Epoch 4440/10000      train_loss = 0.001748930662870407     val_loss = 0.001890195650048554\n",
            "Epoch 4450/10000      train_loss = 0.0017221628222614527     val_loss = 0.0018612041603773832\n",
            "Epoch 4460/10000      train_loss = 0.0016955409664660692     val_loss = 0.001832370413467288\n",
            "Epoch 4470/10000      train_loss = 0.0016690668417140841     val_loss = 0.0018037018598988652\n",
            "Epoch 4480/10000      train_loss = 0.0016427476657554507     val_loss = 0.0017751998966559768\n",
            "Epoch 4490/10000      train_loss = 0.0016165829729288816     val_loss = 0.001746866968460381\n",
            "Epoch 4500/10000      train_loss = 0.0015905789332464337     val_loss = 0.001718710409477353\n",
            "Epoch 4510/10000      train_loss = 0.0015647360123693943     val_loss = 0.0016907274257391691\n",
            "Epoch 4520/10000      train_loss = 0.0015390563057735562     val_loss = 0.0016629237215965986\n",
            "Epoch 4530/10000      train_loss = 0.0015135440044105053     val_loss = 0.0016352987149730325\n",
            "Epoch 4540/10000      train_loss = 0.0014882020186632872     val_loss = 0.001607865677215159\n",
            "Epoch 4550/10000      train_loss = 0.001463032327592373     val_loss = 0.00158061389811337\n",
            "Epoch 4560/10000      train_loss = 0.0014380402863025665     val_loss = 0.0015535594429820776\n",
            "Epoch 4570/10000      train_loss = 0.0014132234500721097     val_loss = 0.0015266933478415012\n",
            "Epoch 4580/10000      train_loss = 0.001388589502312243     val_loss = 0.0015000324929133058\n",
            "Epoch 4590/10000      train_loss = 0.0013641371624544263     val_loss = 0.0014735660515725613\n",
            "Epoch 4600/10000      train_loss = 0.0013398738810792565     val_loss = 0.0014473035698756576\n",
            "Epoch 4610/10000      train_loss = 0.001315796165727079     val_loss = 0.001421246211975813\n",
            "Epoch 4620/10000      train_loss = 0.0012919084401801229     val_loss = 0.0013953930465504527\n",
            "Epoch 4630/10000      train_loss = 0.001268215710297227     val_loss = 0.0013697531539946795\n",
            "Epoch 4640/10000      train_loss = 0.0012447190238162875     val_loss = 0.0013443254865705967\n",
            "Epoch 4650/10000      train_loss = 0.0012214186135679483     val_loss = 0.0013191142352297902\n",
            "Epoch 4660/10000      train_loss = 0.0011983189033344388     val_loss = 0.0012941214954480529\n",
            "Epoch 4670/10000      train_loss = 0.0011754217557609081     val_loss = 0.0012693427270278335\n",
            "Epoch 4680/10000      train_loss = 0.0011527265887707472     val_loss = 0.0012447885237634182\n",
            "Epoch 4690/10000      train_loss = 0.0011302398052066565     val_loss = 0.0012204577215015888\n",
            "Epoch 4700/10000      train_loss = 0.0011079601245000958     val_loss = 0.0011963588185608387\n",
            "Epoch 4710/10000      train_loss = 0.0010858906898647547     val_loss = 0.0011724828509613872\n",
            "Epoch 4720/10000      train_loss = 0.0010640344116836786     val_loss = 0.0011488432064652443\n",
            "Epoch 4730/10000      train_loss = 0.0010423916392028332     val_loss = 0.0011254295241087675\n",
            "Epoch 4740/10000      train_loss = 0.0010209647007286549     val_loss = 0.0011022505350410938\n",
            "Epoch 4750/10000      train_loss = 0.0009997532470151782     val_loss = 0.0010793108958750963\n",
            "Epoch 4760/10000      train_loss = 0.0009787650778889656     val_loss = 0.0010566111886873841\n",
            "Epoch 4770/10000      train_loss = 0.0009579977486282587     val_loss = 0.0010341489687561989\n",
            "Epoch 4780/10000      train_loss = 0.0009374517831020057     val_loss = 0.0010119298240169883\n",
            "Epoch 4790/10000      train_loss = 0.0009171303827315569     val_loss = 0.0009899543365463614\n",
            "Epoch 4800/10000      train_loss = 0.0008970349445007741     val_loss = 0.0009682237869128585\n",
            "Epoch 4810/10000      train_loss = 0.0008771670982241631     val_loss = 0.0009467402705922723\n",
            "Epoch 4820/10000      train_loss = 0.0008575283573009074     val_loss = 0.0009255013428628445\n",
            "Epoch 4830/10000      train_loss = 0.0008381199440918863     val_loss = 0.0009045166661962867\n",
            "Epoch 4840/10000      train_loss = 0.0008189424406737089     val_loss = 0.000883784145116806\n",
            "Epoch 4850/10000      train_loss = 0.0007999976514838636     val_loss = 0.0008633015095256269\n",
            "Epoch 4860/10000      train_loss = 0.0007812861003912985     val_loss = 0.000843072310090065\n",
            "Epoch 4870/10000      train_loss = 0.0007628112798556685     val_loss = 0.0008231005049310625\n",
            "Epoch 4880/10000      train_loss = 0.0007445719675160944     val_loss = 0.0008033841731958091\n",
            "Epoch 4890/10000      train_loss = 0.0007265707245096564     val_loss = 0.0007839237805455923\n",
            "Epoch 4900/10000      train_loss = 0.0007088060956448317     val_loss = 0.0007647221791557968\n",
            "Epoch 4910/10000      train_loss = 0.0006912822718732059     val_loss = 0.0007457821629941463\n",
            "Epoch 4920/10000      train_loss = 0.0006739976233802736     val_loss = 0.0007271007634699345\n",
            "Epoch 4930/10000      train_loss = 0.0006182933575473726     val_loss = 0.0006698127253912389\n",
            "Epoch 4940/10000      train_loss = 0.0005991499638184905     val_loss = 0.0006414081435650587\n",
            "Epoch 4950/10000      train_loss = 0.0005886424914933741     val_loss = 0.0006289883167482913\n",
            "Epoch 4960/10000      train_loss = 0.0005787987029179931     val_loss = 0.0006191664724610746\n",
            "Epoch 4970/10000      train_loss = 0.0005692641716450453     val_loss = 0.0006099157617427409\n",
            "Epoch 4980/10000      train_loss = 0.000559912296012044     val_loss = 0.0006007204065099359\n",
            "Epoch 4990/10000      train_loss = 0.0005506688612513244     val_loss = 0.0005914560751989484\n",
            "Epoch 5000/10000      train_loss = 0.0005414909101091325     val_loss = 0.0005820956430397928\n",
            "Epoch 5010/10000      train_loss = 0.0005323524819687009     val_loss = 0.0005726443487219512\n",
            "Epoch 5020/10000      train_loss = 0.000523240480106324     val_loss = 0.0005631199455820024\n",
            "Epoch 5030/10000      train_loss = 0.0005141486180946231     val_loss = 0.0005535365198738873\n",
            "Epoch 5040/10000      train_loss = 0.0005050748586654663     val_loss = 0.0005439136293716729\n",
            "Epoch 5050/10000      train_loss = 0.000496017630212009     val_loss = 0.0005342650692909956\n",
            "Epoch 5060/10000      train_loss = 0.00048697792226448655     val_loss = 0.0005246015498414636\n",
            "Epoch 5070/10000      train_loss = 0.00047795806312933564     val_loss = 0.000514934363309294\n",
            "Epoch 5080/10000      train_loss = 0.0004689627676270902     val_loss = 0.0005052732303738594\n",
            "Epoch 5090/10000      train_loss = 0.00045999151188880205     val_loss = 0.0004956249613314867\n",
            "Epoch 5100/10000      train_loss = 0.0004510488943196833     val_loss = 0.00048599689034745097\n",
            "Epoch 5110/10000      train_loss = 0.000442137592472136     val_loss = 0.00047639530384913087\n",
            "Epoch 5120/10000      train_loss = 0.0004332619719207287     val_loss = 0.00046682771062478423\n",
            "Epoch 5130/10000      train_loss = 0.0004244224401190877     val_loss = 0.00045729344128631055\n",
            "Epoch 5140/10000      train_loss = 0.0004156242939643562     val_loss = 0.00044780306052416563\n",
            "Epoch 5150/10000      train_loss = 0.0004068712005391717     val_loss = 0.0004383571504149586\n",
            "Epoch 5160/10000      train_loss = 0.00039816589560359716     val_loss = 0.0004289645585231483\n",
            "Epoch 5170/10000      train_loss = 0.0003895102418027818     val_loss = 0.00041962144314311445\n",
            "Epoch 5180/10000      train_loss = 0.0003809090703725815     val_loss = 0.00041033700108528137\n",
            "Epoch 5190/10000      train_loss = 0.00037236331263557076     val_loss = 0.00040111420094035566\n",
            "Epoch 5200/10000      train_loss = 0.000363878789357841     val_loss = 0.0003919569426216185\n",
            "Epoch 5210/10000      train_loss = 0.0003554565482772887     val_loss = 0.00038286615745164454\n",
            "Epoch 5220/10000      train_loss = 0.000347099790815264     val_loss = 0.00037384702591225505\n",
            "Epoch 5230/10000      train_loss = 0.0003388137265574187     val_loss = 0.00036490432103164494\n",
            "Epoch 5240/10000      train_loss = 0.0003305981808807701     val_loss = 0.0003560376644600183\n",
            "Epoch 5250/10000      train_loss = 0.00032245751935988665     val_loss = 0.0003472532262094319\n",
            "Epoch 5260/10000      train_loss = 0.0003143934882245958     val_loss = 0.0003385490854270756\n",
            "Epoch 5270/10000      train_loss = 0.00030640888144262135     val_loss = 0.00032993435161188245\n",
            "Epoch 5280/10000      train_loss = 0.0002985074243042618     val_loss = 0.0003214094031136483\n",
            "Epoch 5290/10000      train_loss = 0.00029069342417642474     val_loss = 0.0003129741526208818\n",
            "Epoch 5300/10000      train_loss = 0.0002829647564794868     val_loss = 0.00030463660368695855\n",
            "Epoch 5310/10000      train_loss = 0.00027532613603398204     val_loss = 0.00029639515560120344\n",
            "Epoch 5320/10000      train_loss = 0.00026778061874210835     val_loss = 0.00028825399931520224\n",
            "Epoch 5330/10000      train_loss = 0.0002603295142762363     val_loss = 0.00028021636535413563\n",
            "Epoch 5340/10000      train_loss = 0.0002529752964619547     val_loss = 0.0002722815261222422\n",
            "Epoch 5350/10000      train_loss = 0.00024572070105932653     val_loss = 0.00026445521507412195\n",
            "Epoch 5360/10000      train_loss = 0.00023856625193729997     val_loss = 0.00025673952768556774\n",
            "Epoch 5370/10000      train_loss = 0.00023151622735895216     val_loss = 0.0002491330960765481\n",
            "Epoch 5380/10000      train_loss = 0.00022456936130765826     val_loss = 0.0002416396455373615\n",
            "Epoch 5390/10000      train_loss = 0.00018832019122783095     val_loss = 0.00020040896197315305\n",
            "Epoch 5400/10000      train_loss = 0.00018127451767213643     val_loss = 0.0001909072743728757\n",
            "Epoch 5410/10000      train_loss = 0.00017673932597972453     val_loss = 0.0001867122045950964\n",
            "Epoch 5420/10000      train_loss = 0.00017299149476457387     val_loss = 0.00018354665371589363\n",
            "Epoch 5430/10000      train_loss = 0.00016967473493423313     val_loss = 0.00018067499331664294\n",
            "Epoch 5440/10000      train_loss = 0.00016659669927321374     val_loss = 0.0001778776932042092\n",
            "Epoch 5450/10000      train_loss = 0.00016364904877264053     val_loss = 0.0001750803494360298\n",
            "Epoch 5460/10000      train_loss = 0.00016077353211585432     val_loss = 0.00017225171905010939\n",
            "Epoch 5470/10000      train_loss = 0.0001579345262143761     val_loss = 0.0001693884696578607\n",
            "Epoch 5480/10000      train_loss = 0.00015511676610913128     val_loss = 0.00016649096505716443\n",
            "Epoch 5490/10000      train_loss = 0.00015231080760713667     val_loss = 0.0001635650114621967\n",
            "Epoch 5500/10000      train_loss = 0.00014951167395338416     val_loss = 0.0001606167497811839\n",
            "Epoch 5510/10000      train_loss = 0.0001467184629291296     val_loss = 0.00015765413991175592\n",
            "Epoch 5520/10000      train_loss = 0.00014392947196029127     val_loss = 0.00015468178025912493\n",
            "Epoch 5530/10000      train_loss = 0.00014114599616732448     val_loss = 0.000151703497976996\n",
            "Epoch 5540/10000      train_loss = 0.00013836791913490742     val_loss = 0.00014872288738843054\n",
            "Epoch 5550/10000      train_loss = 0.00013040189514867961     val_loss = 0.00013986295380163938\n",
            "Epoch 5560/10000      train_loss = 0.0001277366536669433     val_loss = 0.00013675782247446477\n",
            "Epoch 5570/10000      train_loss = 0.00011718682071659714     val_loss = 0.0001248013140866533\n",
            "Epoch 5580/10000      train_loss = 0.00011464738054201007     val_loss = 0.00012213090667501092\n",
            "Epoch 5590/10000      train_loss = 0.00011231643293285742     val_loss = 0.00011993677617283538\n",
            "Epoch 5600/10000      train_loss = 0.00011008211731677875     val_loss = 0.00011777271720347926\n",
            "Epoch 5610/10000      train_loss = 0.00010412794654257596     val_loss = 0.00011114643712062389\n",
            "Epoch 5620/10000      train_loss = 0.00010202822159044445     val_loss = 0.00010905161616392434\n",
            "Epoch 5630/10000      train_loss = 9.999179746955633e-05     val_loss = 0.00010703536099754274\n",
            "Epoch 5640/10000      train_loss = 9.79875767370686e-05     val_loss = 0.00010499895870452747\n",
            "Epoch 5650/10000      train_loss = 9.600001067155972e-05     val_loss = 0.00010293971718056127\n",
            "Epoch 5660/10000      train_loss = 9.402295836480334e-05     val_loss = 0.0001008629405987449\n",
            "Epoch 5670/10000      train_loss = 9.205421520164236e-05     val_loss = 9.877651609713212e-05\n",
            "Epoch 5680/10000      train_loss = 9.009304631035775e-05     val_loss = 9.668484563007951e-05\n",
            "Epoch 5690/10000      train_loss = 8.81394007592462e-05     val_loss = 9.45945066632703e-05\n",
            "Epoch 5700/10000      train_loss = 8.619402069598436e-05     val_loss = 9.250666334992275e-05\n",
            "Epoch 5710/10000      train_loss = 8.425655687460676e-05     val_loss = 9.042475721798837e-05\n",
            "Epoch 5720/10000      train_loss = 8.23291193228215e-05     val_loss = 8.835148037178442e-05\n",
            "Epoch 5730/10000      train_loss = 8.041160617722198e-05     val_loss = 8.628700015833601e-05\n",
            "Epoch 5740/10000      train_loss = 7.850589463487267e-05     val_loss = 8.423520193900913e-05\n",
            "Epoch 5750/10000      train_loss = 7.661202835151926e-05     val_loss = 8.219523442676291e-05\n",
            "Epoch 5760/10000      train_loss = 7.473054574802518e-05     val_loss = 8.016778156161308e-05\n",
            "Epoch 5770/10000      train_loss = 7.286305481102318e-05     val_loss = 7.815557182766497e-05\n",
            "Epoch 5780/10000      train_loss = 7.100914081092924e-05     val_loss = 7.615985668962821e-05\n",
            "Epoch 5790/10000      train_loss = 6.91707682562992e-05     val_loss = 7.41788899176754e-05\n",
            "Epoch 5800/10000      train_loss = 6.73486792948097e-05     val_loss = 7.22166514606215e-05\n",
            "Epoch 5810/10000      train_loss = 6.55424955766648e-05     val_loss = 7.02706747688353e-05\n",
            "Epoch 5820/10000      train_loss = 0.0002656715514604002     val_loss = 9.642837540013716e-05\n",
            "Epoch 5830/10000      train_loss = 4.437020106706768e-05     val_loss = 4.479499693843536e-05\n",
            "Epoch 5840/10000      train_loss = 4.035400343127549e-05     val_loss = 4.1147341107716784e-05\n",
            "Epoch 5850/10000      train_loss = 3.802141873165965e-05     val_loss = 3.9266436942853034e-05\n",
            "Epoch 5860/10000      train_loss = 3.659589128801599e-05     val_loss = 3.8188132748473436e-05\n",
            "Epoch 5870/10000      train_loss = 3.565926817827858e-05     val_loss = 3.7498735764529556e-05\n",
            "Epoch 5880/10000      train_loss = 3.498239675536752e-05     val_loss = 3.699219087138772e-05\n",
            "Epoch 5890/10000      train_loss = 3.444140747888014e-05     val_loss = 3.6565616028383374e-05\n",
            "Epoch 5900/10000      train_loss = 3.397043110453524e-05     val_loss = 3.616745743784122e-05\n",
            "Epoch 5910/10000      train_loss = 3.353453939780593e-05     val_loss = 3.577356255846098e-05\n",
            "Epoch 5920/10000      train_loss = 3.31150513375178e-05     val_loss = 3.537483644322492e-05\n",
            "Epoch 5930/10000      train_loss = 3.2702671887818724e-05     val_loss = 3.496703357086517e-05\n",
            "Epoch 5940/10000      train_loss = 3.229289359296672e-05     val_loss = 3.4550001146271825e-05\n",
            "Epoch 5950/10000      train_loss = 3.188302071066573e-05     val_loss = 3.412484147702344e-05\n",
            "Epoch 5960/10000      train_loss = 3.14721000904683e-05     val_loss = 3.3693322620820254e-05\n",
            "Epoch 5970/10000      train_loss = 3.1058847525855526e-05     val_loss = 3.3255382732022554e-05\n",
            "Epoch 5980/10000      train_loss = 3.064436896238476e-05     val_loss = 3.281287717982195e-05\n",
            "Epoch 5990/10000      train_loss = 3.022783130290918e-05     val_loss = 3.236678094253875e-05\n",
            "Epoch 6000/10000      train_loss = 2.9809223633492365e-05     val_loss = 3.1917141313897446e-05\n",
            "Epoch 6010/10000      train_loss = 2.938872785307467e-05     val_loss = 3.1464893254451454e-05\n",
            "Epoch 6020/10000      train_loss = 2.8966533136554062e-05     val_loss = 3.1009902158984914e-05\n",
            "Epoch 6030/10000      train_loss = 2.8542890504468232e-05     val_loss = 3.055288834730163e-05\n",
            "Epoch 6040/10000      train_loss = 2.8117290639784187e-05     val_loss = 3.009391548403073e-05\n",
            "Epoch 6050/10000      train_loss = 2.7690706701832823e-05     val_loss = 2.9633094527525827e-05\n",
            "Epoch 6060/10000      train_loss = 2.7262396542937495e-05     val_loss = 2.9171067581046373e-05\n",
            "Epoch 6070/10000      train_loss = 2.6833095034817234e-05     val_loss = 2.870809294108767e-05\n",
            "Epoch 6080/10000      train_loss = 2.640288948896341e-05     val_loss = 2.82436467387015e-05\n",
            "Epoch 6090/10000      train_loss = 2.5971698050852865e-05     val_loss = 2.777805093501229e-05\n",
            "Epoch 6100/10000      train_loss = 2.5539842681610025e-05     val_loss = 2.7312038582749665e-05\n",
            "Epoch 6110/10000      train_loss = 2.5107303372351453e-05     val_loss = 2.684556966414675e-05\n",
            "Epoch 6120/10000      train_loss = 2.0856417904724367e-05     val_loss = 2.028649760177359e-05\n",
            "Epoch 6130/10000      train_loss = 1.4506785191770177e-05     val_loss = 1.4143115549813956e-05\n",
            "Epoch 6140/10000      train_loss = 1.1029547749785706e-05     val_loss = 1.084598625311628e-05\n",
            "Epoch 6150/10000      train_loss = 9.072829925571568e-06     val_loss = 9.038779353431892e-06\n",
            "Epoch 6160/10000      train_loss = 7.975573680596426e-06     val_loss = 8.05633590061916e-06\n",
            "Epoch 6170/10000      train_loss = 7.36117272026604e-06     val_loss = 7.5279017437424045e-06\n",
            "Epoch 6180/10000      train_loss = 7.014733455434907e-06     val_loss = 7.245136657729745e-06\n",
            "Epoch 6190/10000      train_loss = 6.815466804255266e-06     val_loss = 7.092432952049421e-06\n",
            "Epoch 6200/10000      train_loss = 6.696112450299552e-06     val_loss = 7.006770829320885e-06\n",
            "Epoch 6210/10000      train_loss = 6.619878149649594e-06     val_loss = 6.9541520133498125e-06\n",
            "Epoch 6220/10000      train_loss = 6.5664448811730836e-06     val_loss = 6.91717968948069e-06\n",
            "Epoch 6230/10000      train_loss = 6.524642685690196e-06     val_loss = 6.886484243295854e-06\n",
            "Epoch 6240/10000      train_loss = 6.489018687716452e-06     val_loss = 6.858030246803537e-06\n",
            "Epoch 6250/10000      train_loss = 6.456182291003643e-06     val_loss = 6.829575795563869e-06\n",
            "Epoch 6260/10000      train_loss = 6.424658749892842e-06     val_loss = 6.800239134463482e-06\n",
            "Epoch 6270/10000      train_loss = 6.393553121597506e-06     val_loss = 6.77002844895469e-06\n",
            "Epoch 6280/10000      train_loss = 6.362490694300504e-06     val_loss = 6.738842785125598e-06\n",
            "Epoch 6290/10000      train_loss = 6.331277745630359e-06     val_loss = 6.706510248477571e-06\n",
            "Epoch 6300/10000      train_loss = 6.2998433350003324e-06     val_loss = 6.673377356491983e-06\n",
            "Epoch 6310/10000      train_loss = 6.26809332970879e-06     val_loss = 6.639633738814155e-06\n",
            "Epoch 6320/10000      train_loss = 6.236004082893487e-06     val_loss = 6.605449470953317e-06\n",
            "Epoch 6330/10000      train_loss = 6.203394150361419e-06     val_loss = 6.570415735041024e-06\n",
            "Epoch 6340/10000      train_loss = 6.1704754443780985e-06     val_loss = 6.534689873660682e-06\n",
            "Epoch 6350/10000      train_loss = 6.1371993069769815e-06     val_loss = 6.498699804069474e-06\n",
            "Epoch 6360/10000      train_loss = 6.103533905843506e-06     val_loss = 6.462116743932711e-06\n",
            "Epoch 6370/10000      train_loss = 6.069418304832652e-06     val_loss = 6.425192168535432e-06\n",
            "Epoch 6380/10000      train_loss = 6.034989382897038e-06     val_loss = 6.387745997926686e-06\n",
            "Epoch 6390/10000      train_loss = 6.000079793011537e-06     val_loss = 6.349784598569386e-06\n",
            "Epoch 6400/10000      train_loss = 5.9648541537171695e-06     val_loss = 6.311541710601887e-06\n",
            "Epoch 6410/10000      train_loss = 5.929204689891776e-06     val_loss = 6.272669452300761e-06\n",
            "Epoch 6420/10000      train_loss = 5.893183697480708e-06     val_loss = 6.23364076091093e-06\n",
            "Epoch 6430/10000      train_loss = 5.856803909409791e-06     val_loss = 6.193984518176876e-06\n",
            "Epoch 6440/10000      train_loss = 5.820055775984656e-06     val_loss = 6.153998128866078e-06\n",
            "Epoch 6450/10000      train_loss = 5.7827487580652814e-06     val_loss = 6.113456947787199e-06\n",
            "Epoch 6460/10000      train_loss = 5.745316229877062e-06     val_loss = 6.072726137063e-06\n",
            "Epoch 6470/10000      train_loss = 5.707314358005533e-06     val_loss = 6.031609245837899e-06\n",
            "Epoch 6480/10000      train_loss = 5.6690655583224725e-06     val_loss = 5.989921191940084e-06\n",
            "Epoch 6490/10000      train_loss = 5.630412033497123e-06     val_loss = 5.947806130279787e-06\n",
            "Epoch 6500/10000      train_loss = 5.591377885139082e-06     val_loss = 5.905279977014288e-06\n",
            "Epoch 6510/10000      train_loss = 5.5520258683827706e-06     val_loss = 5.862469151907135e-06\n",
            "Epoch 6520/10000      train_loss = 5.512342795555014e-06     val_loss = 5.819407306262292e-06\n",
            "Epoch 6530/10000      train_loss = 5.472179509524722e-06     val_loss = 5.775715635536471e-06\n",
            "Epoch 6540/10000      train_loss = 5.431745648820652e-06     val_loss = 5.731560122512747e-06\n",
            "Epoch 6550/10000      train_loss = 5.3909684538666625e-06     val_loss = 5.6873377616284415e-06\n",
            "Epoch 6560/10000      train_loss = 5.349822004063753e-06     val_loss = 5.642528321914142e-06\n",
            "Epoch 6570/10000      train_loss = 5.308423624228453e-06     val_loss = 5.597683866653824e-06\n",
            "Epoch 6580/10000      train_loss = 5.266738298814744e-06     val_loss = 5.5521300055261236e-06\n",
            "Epoch 6590/10000      train_loss = 5.22445589012932e-06     val_loss = 5.50631830265047e-06\n",
            "Epoch 6600/10000      train_loss = 5.18205206390121e-06     val_loss = 5.460411102831131e-06\n",
            "Epoch 6610/10000      train_loss = 5.13935583512648e-06     val_loss = 5.4137826737132855e-06\n",
            "Epoch 6620/10000      train_loss = 5.096324457554147e-06     val_loss = 5.367067842598772e-06\n",
            "Epoch 6630/10000      train_loss = 5.05293746755342e-06     val_loss = 5.319804586179089e-06\n",
            "Epoch 6640/10000      train_loss = 5.009289907320635e-06     val_loss = 5.272562702884898e-06\n",
            "Epoch 6650/10000      train_loss = 4.96547272632597e-06     val_loss = 5.224665528658079e-06\n",
            "Epoch 6660/10000      train_loss = 4.921223080600612e-06     val_loss = 5.1764632189588156e-06\n",
            "Epoch 6670/10000      train_loss = 4.876721050095512e-06     val_loss = 5.128112206875812e-06\n",
            "Epoch 6680/10000      train_loss = 4.832013473787811e-06     val_loss = 5.079255970485974e-06\n",
            "Epoch 6690/10000      train_loss = 4.7869821173662785e-06     val_loss = 5.03004275742569e-06\n",
            "Epoch 6700/10000      train_loss = 4.741595148516353e-06     val_loss = 4.9805676098912954e-06\n",
            "Epoch 6710/10000      train_loss = 4.6961777115939185e-06     val_loss = 4.930918294121511e-06\n",
            "Epoch 6720/10000      train_loss = 4.650355094781844e-06     val_loss = 4.880874257651158e-06\n",
            "Epoch 6730/10000      train_loss = 4.604375135386363e-06     val_loss = 4.8305159907613415e-06\n",
            "Epoch 6740/10000      train_loss = 4.558223281492246e-06     val_loss = 4.780023118655663e-06\n",
            "Epoch 6750/10000      train_loss = 4.511854058364406e-06     val_loss = 4.729336524178507e-06\n",
            "Epoch 6760/10000      train_loss = 4.4653434088104405e-06     val_loss = 4.67850577479112e-06\n",
            "Epoch 6770/10000      train_loss = 4.418505795911187e-06     val_loss = 4.627448561222991e-06\n",
            "Epoch 6780/10000      train_loss = 4.3716386244341265e-06     val_loss = 4.576231731334701e-06\n",
            "Epoch 6790/10000      train_loss = 4.324577730585588e-06     val_loss = 4.524933956417954e-06\n",
            "Epoch 6800/10000      train_loss = 4.277397238183767e-06     val_loss = 4.473615263123065e-06\n",
            "Epoch 6810/10000      train_loss = 4.230065769661451e-06     val_loss = 4.422306574269896e-06\n",
            "Epoch 6820/10000      train_loss = 4.182713382760994e-06     val_loss = 4.371085196908098e-06\n",
            "Epoch 6830/10000      train_loss = 4.135077233513584e-06     val_loss = 4.3197096601943485e-06\n",
            "Epoch 6840/10000      train_loss = 4.087494744453579e-06     val_loss = 4.268371412763372e-06\n",
            "Epoch 6850/10000      train_loss = 4.039909072162118e-06     val_loss = 4.217058176436694e-06\n",
            "Epoch 6860/10000      train_loss = 3.99219197788625e-06     val_loss = 4.16584407503251e-06\n",
            "Epoch 6870/10000      train_loss = 3.944374384445837e-06     val_loss = 4.114589955861447e-06\n",
            "Epoch 6880/10000      train_loss = 3.896485395671334e-06     val_loss = 4.063354481331771e-06\n",
            "Epoch 6890/10000      train_loss = 3.848711003229255e-06     val_loss = 4.012155841337517e-06\n",
            "Epoch 6900/10000      train_loss = 3.8009320633136667e-06     val_loss = 3.960979029216105e-06\n",
            "Epoch 6910/10000      train_loss = 3.75312811229378e-06     val_loss = 3.909879978891695e-06\n",
            "Epoch 6920/10000      train_loss = 3.7054132917546667e-06     val_loss = 3.858842319459654e-06\n",
            "Epoch 6930/10000      train_loss = 3.6577016544470098e-06     val_loss = 3.8076018427091185e-06\n",
            "Epoch 6940/10000      train_loss = 3.610129624576075e-06     val_loss = 3.7564936974376906e-06\n",
            "Epoch 6950/10000      train_loss = 3.5626376302388962e-06     val_loss = 3.7053423511679284e-06\n",
            "Epoch 6960/10000      train_loss = 3.515207481541438e-06     val_loss = 3.6543788155540824e-06\n",
            "Epoch 6970/10000      train_loss = 3.4679796954151243e-06     val_loss = 3.603363893489586e-06\n",
            "Epoch 6980/10000      train_loss = 3.4208221677545225e-06     val_loss = 3.552103862602962e-06\n",
            "Epoch 6990/10000      train_loss = 3.373921344973496e-06     val_loss = 3.5010993997275364e-06\n",
            "Epoch 7000/10000      train_loss = 3.3273431654379237e-06     val_loss = 3.4500687888794346e-06\n",
            "Epoch 7010/10000      train_loss = 3.2809559797897236e-06     val_loss = 3.3991400414379314e-06\n",
            "Epoch 7020/10000      train_loss = 3.2348432341677835e-06     val_loss = 3.3484270716144238e-06\n",
            "Epoch 7030/10000      train_loss = 3.189060180375236e-06     val_loss = 3.2976199690892827e-06\n",
            "Epoch 7040/10000      train_loss = 3.1435095024789916e-06     val_loss = 3.24691723108117e-06\n",
            "Epoch 7050/10000      train_loss = 3.0985854664322687e-06     val_loss = 3.196400939486921e-06\n",
            "Epoch 7060/10000      train_loss = 3.0540188618033426e-06     val_loss = 3.1461081562156323e-06\n",
            "Epoch 7070/10000      train_loss = 3.0097241960902466e-06     val_loss = 3.0961102766013937e-06\n",
            "Epoch 7080/10000      train_loss = 2.9660081963811535e-06     val_loss = 3.0462899758276762e-06\n",
            "Epoch 7090/10000      train_loss = 2.922740350186359e-06     val_loss = 2.9968659873702563e-06\n",
            "Epoch 7100/10000      train_loss = 2.879937937905197e-06     val_loss = 2.947709617728833e-06\n",
            "Epoch 7110/10000      train_loss = 2.83765120911994e-06     val_loss = 2.8990773444093065e-06\n",
            "Epoch 7120/10000      train_loss = 2.7957150905422168e-06     val_loss = 2.850887767635868e-06\n",
            "Epoch 7130/10000      train_loss = 2.7542530460777925e-06     val_loss = 2.803297775244573e-06\n",
            "Epoch 7140/10000      train_loss = 2.7131593469675863e-06     val_loss = 2.7562450668483507e-06\n",
            "Epoch 7150/10000      train_loss = 2.672471282494371e-06     val_loss = 2.7096427857031813e-06\n",
            "Epoch 7160/10000      train_loss = 2.631814368214691e-06     val_loss = 2.663613031472778e-06\n",
            "Epoch 7170/10000      train_loss = 2.5918075152731035e-06     val_loss = 2.6181992325291503e-06\n",
            "Epoch 7180/10000      train_loss = 2.551968918851344e-06     val_loss = 2.573427309471299e-06\n",
            "Epoch 7190/10000      train_loss = 2.5124782041530125e-06     val_loss = 2.52917470788816e-06\n",
            "Epoch 7200/10000      train_loss = 2.473003405611962e-06     val_loss = 2.485670165697229e-06\n",
            "Epoch 7210/10000      train_loss = 2.4342318738490576e-06     val_loss = 2.4426904019492213e-06\n",
            "Epoch 7220/10000      train_loss = 2.3956104087119456e-06     val_loss = 2.400256335022277e-06\n",
            "Epoch 7230/10000      train_loss = 2.3573529688292183e-06     val_loss = 2.358553274461883e-06\n",
            "Epoch 7240/10000      train_loss = 2.3194779714685865e-06     val_loss = 2.317404323548544e-06\n",
            "Epoch 7250/10000      train_loss = 2.2819003788754344e-06     val_loss = 2.2769295355828945e-06\n",
            "Epoch 7260/10000      train_loss = 2.245122914246167e-06     val_loss = 2.2370024908013875e-06\n",
            "Epoch 7270/10000      train_loss = 2.2084100237407256e-06     val_loss = 2.1975638446747325e-06\n",
            "Epoch 7280/10000      train_loss = 2.1722735255025327e-06     val_loss = 2.1587381979770726e-06\n",
            "Epoch 7290/10000      train_loss = 2.136436251021223e-06     val_loss = 2.120702674801578e-06\n",
            "Epoch 7300/10000      train_loss = 2.101527115883073e-06     val_loss = 2.083069830405293e-06\n",
            "Epoch 7310/10000      train_loss = 2.066641400233493e-06     val_loss = 2.046100235020276e-06\n",
            "Epoch 7320/10000      train_loss = 2.032400288953795e-06     val_loss = 2.0096213120268658e-06\n",
            "Epoch 7330/10000      train_loss = 1.998710558837047e-06     val_loss = 1.973884536710102e-06\n",
            "Epoch 7340/10000      train_loss = 1.965345290955156e-06     val_loss = 1.9385818177397596e-06\n",
            "Epoch 7350/10000      train_loss = 1.932589839270804e-06     val_loss = 1.9038373011426302e-06\n",
            "Epoch 7360/10000      train_loss = 1.9004030491487356e-06     val_loss = 1.8696944152907236e-06\n",
            "Epoch 7370/10000      train_loss = 1.868649178504711e-06     val_loss = 1.8362673017691122e-06\n",
            "Epoch 7380/10000      train_loss = 1.8372527392784832e-06     val_loss = 1.8031835224974202e-06\n",
            "Epoch 7390/10000      train_loss = 1.806454747566022e-06     val_loss = 1.7707577626424609e-06\n",
            "Epoch 7400/10000      train_loss = 1.776338649506215e-06     val_loss = 1.7388246078553493e-06\n",
            "Epoch 7410/10000      train_loss = 1.7464740267314482e-06     val_loss = 1.7075581126846373e-06\n",
            "Epoch 7420/10000      train_loss = 1.7171265653814771e-06     val_loss = 1.6766909993748413e-06\n",
            "Epoch 7430/10000      train_loss = 1.68837368619279e-06     val_loss = 1.6463490055684815e-06\n",
            "Epoch 7440/10000      train_loss = 1.659864210523665e-06     val_loss = 1.616655822544999e-06\n",
            "Epoch 7450/10000      train_loss = 1.63210120263102e-06     val_loss = 1.5873982874836656e-06\n",
            "Epoch 7460/10000      train_loss = 1.6045310076151509e-06     val_loss = 1.5585907249260345e-06\n",
            "Epoch 7470/10000      train_loss = 1.5774693338244106e-06     val_loss = 1.530443228148215e-06\n",
            "Epoch 7480/10000      train_loss = 1.551058744553302e-06     val_loss = 1.5026257642603014e-06\n",
            "Epoch 7490/10000      train_loss = 1.5249731859512394e-06     val_loss = 1.4754285757589969e-06\n",
            "Epoch 7500/10000      train_loss = 1.4992026535765035e-06     val_loss = 1.4486308828054462e-06\n",
            "Epoch 7510/10000      train_loss = 1.4741513041371945e-06     val_loss = 1.4223545576896868e-06\n",
            "Epoch 7520/10000      train_loss = 1.4493658682113164e-06     val_loss = 1.3965702692075865e-06\n",
            "Epoch 7530/10000      train_loss = 1.4251501170292613e-06     val_loss = 1.3712671034227242e-06\n",
            "Epoch 7540/10000      train_loss = 1.4011709481565049e-06     val_loss = 1.3464267567542265e-06\n",
            "Epoch 7550/10000      train_loss = 1.3778852689938503e-06     val_loss = 1.3219556649346487e-06\n",
            "Epoch 7560/10000      train_loss = 1.3546647323892103e-06     val_loss = 1.2979559187442646e-06\n",
            "Epoch 7570/10000      train_loss = 1.3318740457179956e-06     val_loss = 1.2743877277898719e-06\n",
            "Epoch 7580/10000      train_loss = 1.3097999271849403e-06     val_loss = 1.2513182809925638e-06\n",
            "Epoch 7590/10000      train_loss = 1.2878579127573175e-06     val_loss = 1.2286100172786973e-06\n",
            "Epoch 7600/10000      train_loss = 1.2664860378208687e-06     val_loss = 1.2063873100487399e-06\n",
            "Epoch 7610/10000      train_loss = 1.245222733814444e-06     val_loss = 1.1844946357086883e-06\n",
            "Epoch 7620/10000      train_loss = 1.2245041034475435e-06     val_loss = 1.1631103689069278e-06\n",
            "Epoch 7630/10000      train_loss = 1.204224076900573e-06     val_loss = 1.142151745625597e-06\n",
            "Epoch 7640/10000      train_loss = 1.1842033700304455e-06     val_loss = 1.1214923461011495e-06\n",
            "Epoch 7650/10000      train_loss = 1.1648996860458283e-06     val_loss = 1.1012357390427496e-06\n",
            "Epoch 7660/10000      train_loss = 1.1454194464022294e-06     val_loss = 1.0813271273946157e-06\n",
            "Epoch 7670/10000      train_loss = 1.1263938404226792e-06     val_loss = 1.0618717851684778e-06\n",
            "Epoch 7680/10000      train_loss = 1.1081150432801223e-06     val_loss = 1.042718622557004e-06\n",
            "Epoch 7690/10000      train_loss = 1.0898640994128073e-06     val_loss = 1.0238720733468654e-06\n",
            "Epoch 7700/10000      train_loss = 1.0716751148720505e-06     val_loss = 1.0055431403088733e-06\n",
            "Epoch 7710/10000      train_loss = 1.0542797781454283e-06     val_loss = 9.87454427558987e-07\n",
            "Epoch 7720/10000      train_loss = 1.0371603593739565e-06     val_loss = 9.6984376796172e-07\n",
            "Epoch 7730/10000      train_loss = 1.0200193401033175e-06     val_loss = 9.524425763629552e-07\n",
            "Epoch 7740/10000      train_loss = 1.00325996754691e-06     val_loss = 9.353653922516969e-07\n",
            "Epoch 7750/10000      train_loss = 9.87130533758318e-07     val_loss = 9.187730825033213e-07\n",
            "Epoch 7760/10000      train_loss = 9.709738151286729e-07     val_loss = 9.023372058436507e-07\n",
            "Epoch 7770/10000      train_loss = 9.55099949351279e-07     val_loss = 8.862198797032761e-07\n",
            "Epoch 7780/10000      train_loss = 9.397803069077781e-07     val_loss = 8.704597576070228e-07\n",
            "Epoch 7790/10000      train_loss = 9.247281695934362e-07     val_loss = 8.550925372219353e-07\n",
            "Epoch 7800/10000      train_loss = 9.098388886741304e-07     val_loss = 8.399566127081926e-07\n",
            "Epoch 7810/10000      train_loss = 8.952295615927142e-07     val_loss = 8.251668646153121e-07\n",
            "Epoch 7820/10000      train_loss = 8.810233111944399e-07     val_loss = 8.105286610771145e-07\n",
            "Epoch 7830/10000      train_loss = 8.669620683576795e-07     val_loss = 7.962377139847376e-07\n",
            "Epoch 7840/10000      train_loss = 8.529997899131558e-07     val_loss = 7.822332008800004e-07\n",
            "Epoch 7850/10000      train_loss = 8.394343922191183e-07     val_loss = 7.684731144763646e-07\n",
            "Epoch 7860/10000      train_loss = 8.261904440587386e-07     val_loss = 7.550411851298122e-07\n",
            "Epoch 7870/10000      train_loss = 8.129363209263829e-07     val_loss = 7.41838732665201e-07\n",
            "Epoch 7880/10000      train_loss = 8.003607376849686e-07     val_loss = 7.290054782060906e-07\n",
            "Epoch 7890/10000      train_loss = 7.875669325585477e-07     val_loss = 7.161690405155241e-07\n",
            "Epoch 7900/10000      train_loss = 7.751942234790477e-07     val_loss = 7.037410796328913e-07\n",
            "Epoch 7910/10000      train_loss = 7.630957270521321e-07     val_loss = 6.915245194250019e-07\n",
            "Epoch 7920/10000      train_loss = 7.511979447372141e-07     val_loss = 6.796311708967551e-07\n",
            "Epoch 7930/10000      train_loss = 7.39639574476314e-07     val_loss = 6.679121611341543e-07\n",
            "Epoch 7940/10000      train_loss = 7.28195118426811e-07     val_loss = 6.563664669556601e-07\n",
            "Epoch 7950/10000      train_loss = 7.169109039750765e-07     val_loss = 6.45193210857542e-07\n",
            "Epoch 7960/10000      train_loss = 7.058586106722942e-07     val_loss = 6.34062757853826e-07\n",
            "Epoch 7970/10000      train_loss = 6.948893656044675e-07     val_loss = 6.233304361558112e-07\n",
            "Epoch 7980/10000      train_loss = 6.844242079750984e-07     val_loss = 6.127050937720924e-07\n",
            "Epoch 7990/10000      train_loss = 6.740199296473293e-07     val_loss = 6.024360459377931e-07\n",
            "Epoch 8000/10000      train_loss = 6.637397405029333e-07     val_loss = 5.921490924265527e-07\n",
            "Epoch 8010/10000      train_loss = 6.53542144846142e-07     val_loss = 5.822030857416394e-07\n",
            "Epoch 8020/10000      train_loss = 6.437761612687609e-07     val_loss = 5.723269964619249e-07\n",
            "Epoch 8030/10000      train_loss = 6.340889626699209e-07     val_loss = 5.628241979138693e-07\n",
            "Epoch 8040/10000      train_loss = 6.246116868169338e-07     val_loss = 5.535659965971718e-07\n",
            "Epoch 8050/10000      train_loss = 6.153966296551516e-07     val_loss = 5.442514634523832e-07\n",
            "Epoch 8060/10000      train_loss = 6.06009848524991e-07     val_loss = 5.352803214009327e-07\n",
            "Epoch 8070/10000      train_loss = 5.970613301542471e-07     val_loss = 5.265186473479844e-07\n",
            "Epoch 8080/10000      train_loss = 5.884670599698438e-07     val_loss = 5.177340653972351e-07\n",
            "Epoch 8090/10000      train_loss = 5.797037374577485e-07     val_loss = 5.09320216224296e-07\n",
            "Epoch 8100/10000      train_loss = 5.714114763577527e-07     val_loss = 5.011766006646212e-07\n",
            "Epoch 8110/10000      train_loss = 5.631080171042413e-07     val_loss = 4.930333830088784e-07\n",
            "Epoch 8120/10000      train_loss = 5.545753651858831e-07     val_loss = 4.848807861890236e-07\n",
            "Epoch 8130/10000      train_loss = 5.46692831449036e-07     val_loss = 4.769912038682378e-07\n",
            "Epoch 8140/10000      train_loss = 5.389647412812337e-07     val_loss = 4.694592519172147e-07\n",
            "Epoch 8150/10000      train_loss = 5.311560471454868e-07     val_loss = 4.6203865622373996e-07\n",
            "Epoch 8160/10000      train_loss = 5.234230684436625e-07     val_loss = 4.5444414809026057e-07\n",
            "Epoch 8170/10000      train_loss = 5.159876650395745e-07     val_loss = 4.473026535833924e-07\n",
            "Epoch 8180/10000      train_loss = 5.089213459541497e-07     val_loss = 4.403682396514341e-07\n",
            "Epoch 8190/10000      train_loss = 5.016682393943483e-07     val_loss = 4.3324627085894463e-07\n",
            "Epoch 8200/10000      train_loss = 4.946864464727696e-07     val_loss = 4.2657225662878773e-07\n",
            "Epoch 8210/10000      train_loss = 4.87529405290843e-07     val_loss = 4.198992371584609e-07\n",
            "Epoch 8220/10000      train_loss = 4.807602067558037e-07     val_loss = 4.134719233661599e-07\n",
            "Epoch 8230/10000      train_loss = 4.743294539366616e-07     val_loss = 4.071354453571985e-07\n",
            "Epoch 8240/10000      train_loss = 4.678103664446098e-07     val_loss = 4.010316843050532e-07\n",
            "Epoch 8250/10000      train_loss = 4.615963291598746e-07     val_loss = 3.9480698887928156e-07\n",
            "Epoch 8260/10000      train_loss = 4.550471999209549e-07     val_loss = 3.887282105097256e-07\n",
            "Epoch 8270/10000      train_loss = 4.48996161139803e-07     val_loss = 3.8281402225948113e-07\n",
            "Epoch 8280/10000      train_loss = 4.427123485584161e-07     val_loss = 3.770726380025735e-07\n",
            "Epoch 8290/10000      train_loss = 4.3694831219909247e-07     val_loss = 3.7149612808207166e-07\n",
            "Epoch 8300/10000      train_loss = 4.3105774238938466e-07     val_loss = 3.6604035358323017e-07\n",
            "Epoch 8310/10000      train_loss = 4.249580172199785e-07     val_loss = 3.603161076171091e-07\n",
            "Epoch 8320/10000      train_loss = 4.196205054540769e-07     val_loss = 3.55236693394545e-07\n",
            "Epoch 8330/10000      train_loss = 4.1424254959565587e-07     val_loss = 3.500272498513368e-07\n",
            "Epoch 8340/10000      train_loss = 4.0866240169634693e-07     val_loss = 3.450149961281568e-07\n",
            "Epoch 8350/10000      train_loss = 4.033985874229984e-07     val_loss = 3.3994356840594264e-07\n",
            "Epoch 8360/10000      train_loss = 3.983543876984186e-07     val_loss = 3.353144677475939e-07\n",
            "Epoch 8370/10000      train_loss = 3.932370020720555e-07     val_loss = 3.3042758218471135e-07\n",
            "Epoch 8380/10000      train_loss = 3.8789937661931617e-07     val_loss = 3.2551125173085893e-07\n",
            "Epoch 8390/10000      train_loss = 3.83000099191122e-07     val_loss = 3.209628687272925e-07\n",
            "Epoch 8400/10000      train_loss = 3.7828877452739107e-07     val_loss = 3.1649091170038446e-07\n",
            "Epoch 8410/10000      train_loss = 3.7377682815531443e-07     val_loss = 3.1244744036484917e-07\n",
            "Epoch 8420/10000      train_loss = 3.687819685183058e-07     val_loss = 3.0797167482887744e-07\n",
            "Epoch 8430/10000      train_loss = 3.6420988180907443e-07     val_loss = 3.0377179882634664e-07\n",
            "Epoch 8440/10000      train_loss = 3.594661279748834e-07     val_loss = 2.993190264533041e-07\n",
            "Epoch 8450/10000      train_loss = 3.551747340679867e-07     val_loss = 2.9541794788201514e-07\n",
            "Epoch 8460/10000      train_loss = 3.5094853956252337e-07     val_loss = 2.916540324804373e-07\n",
            "Epoch 8470/10000      train_loss = 3.46580236509908e-07     val_loss = 2.873817095405684e-07\n",
            "Epoch 8480/10000      train_loss = 3.4267293358425377e-07     val_loss = 2.841310049461754e-07\n",
            "Epoch 8490/10000      train_loss = 3.386443268027506e-07     val_loss = 2.8036080834681343e-07\n",
            "Epoch 8500/10000      train_loss = 3.3443393476773053e-07     val_loss = 2.765645774616132e-07\n",
            "Epoch 8510/10000      train_loss = 3.3068005222958163e-07     val_loss = 2.7326990448273136e-07\n",
            "Epoch 8520/10000      train_loss = 3.2652289405632473e-07     val_loss = 2.6949189191327605e-07\n",
            "Epoch 8530/10000      train_loss = 3.2297242569256923e-07     val_loss = 2.663450118234323e-07\n",
            "Epoch 8540/10000      train_loss = 3.1907526931718166e-07     val_loss = 2.6277646725247905e-07\n",
            "Epoch 8550/10000      train_loss = 3.1563922675559297e-07     val_loss = 2.5965684358197905e-07\n",
            "Epoch 8560/10000      train_loss = 3.1166553071670933e-07     val_loss = 2.5592203201085795e-07\n",
            "Epoch 8570/10000      train_loss = 3.0841954412608175e-07     val_loss = 2.533383849367965e-07\n",
            "Epoch 8580/10000      train_loss = 3.049719339287549e-07     val_loss = 2.501361109352729e-07\n",
            "Epoch 8590/10000      train_loss = 3.015113065885089e-07     val_loss = 2.471801110459637e-07\n",
            "Epoch 8600/10000      train_loss = 2.9803055667798617e-07     val_loss = 2.44062817955637e-07\n",
            "Epoch 8610/10000      train_loss = 2.947415964626998e-07     val_loss = 2.410328647783899e-07\n",
            "Epoch 8620/10000      train_loss = 2.917437882388185e-07     val_loss = 2.385652635439328e-07\n",
            "Epoch 8630/10000      train_loss = 2.8844462462984666e-07     val_loss = 2.3581998220834066e-07\n",
            "Epoch 8640/10000      train_loss = 2.8536211971186276e-07     val_loss = 2.3284967198833328e-07\n",
            "Epoch 8650/10000      train_loss = 2.824107809829002e-07     val_loss = 2.301811150573485e-07\n",
            "Epoch 8660/10000      train_loss = 2.7933842261518294e-07     val_loss = 2.2778112906962633e-07\n",
            "Epoch 8670/10000      train_loss = 2.7615311637418927e-07     val_loss = 2.2502358376641496e-07\n",
            "Epoch 8680/10000      train_loss = 2.7343844521965366e-07     val_loss = 2.2255261455939035e-07\n",
            "Epoch 8690/10000      train_loss = 2.7075844855062314e-07     val_loss = 2.203482125651135e-07\n",
            "Epoch 8700/10000      train_loss = 2.679637987057504e-07     val_loss = 2.1783351655813021e-07\n",
            "Epoch 8710/10000      train_loss = 2.6512813633416954e-07     val_loss = 2.1560487084570923e-07\n",
            "Epoch 8720/10000      train_loss = 2.627804178700899e-07     val_loss = 2.1359696233957948e-07\n",
            "Epoch 8730/10000      train_loss = 2.5996664021477045e-07     val_loss = 2.1110879799834947e-07\n",
            "Epoch 8740/10000      train_loss = 2.5746686560523813e-07     val_loss = 2.0918626830734866e-07\n",
            "Epoch 8750/10000      train_loss = 2.54959445555869e-07     val_loss = 2.068021984769075e-07\n",
            "Epoch 8760/10000      train_loss = 2.5249943291782984e-07     val_loss = 2.0485356344579486e-07\n",
            "Epoch 8770/10000      train_loss = 2.4976441181934206e-07     val_loss = 2.0246373821919406e-07\n",
            "Epoch 8780/10000      train_loss = 2.475611324825877e-07     val_loss = 2.008775084050285e-07\n",
            "Epoch 8790/10000      train_loss = 2.4519061980754486e-07     val_loss = 1.988629207971826e-07\n",
            "Epoch 8800/10000      train_loss = 2.4249004582088673e-07     val_loss = 1.9629865732895269e-07\n",
            "Epoch 8810/10000      train_loss = 2.4055108838183514e-07     val_loss = 1.9505291959376336e-07\n",
            "Epoch 8820/10000      train_loss = 2.3817545979909482e-07     val_loss = 1.9291240960228606e-07\n",
            "Epoch 8830/10000      train_loss = 2.3620738431873178e-07     val_loss = 1.9140239260195813e-07\n",
            "Epoch 8840/10000      train_loss = 2.3411804761508392e-07     val_loss = 1.8976894011757395e-07\n",
            "Epoch 8850/10000      train_loss = 2.3168907148374274e-07     val_loss = 1.8761369346975698e-07\n",
            "Epoch 8860/10000      train_loss = 2.2983715552982176e-07     val_loss = 1.861099860889226e-07\n",
            "Epoch 8870/10000      train_loss = 2.2752549000415456e-07     val_loss = 1.8431295245591173e-07\n",
            "Epoch 8880/10000      train_loss = 2.2572446312096872e-07     val_loss = 1.8265006929141236e-07\n",
            "Epoch 8890/10000      train_loss = 2.2402640809104923e-07     val_loss = 1.8113884436843364e-07\n",
            "Epoch 8900/10000      train_loss = 2.218051946556443e-07     val_loss = 1.795768582724122e-07\n",
            "Epoch 8910/10000      train_loss = 2.2022206280780665e-07     val_loss = 1.783908771813003e-07\n",
            "Epoch 8920/10000      train_loss = 2.1841889008555881e-07     val_loss = 1.7688404341242858e-07\n",
            "Epoch 8930/10000      train_loss = 2.1609100997466157e-07     val_loss = 1.7502425464499538e-07\n",
            "Epoch 8940/10000      train_loss = 2.144701909401192e-07     val_loss = 1.7375110417106043e-07\n",
            "Epoch 8950/10000      train_loss = 2.1248675352580904e-07     val_loss = 1.7212249758813414e-07\n",
            "Epoch 8960/10000      train_loss = 2.109119634496892e-07     val_loss = 1.7097329418902518e-07\n",
            "Epoch 8970/10000      train_loss = 2.0966700731150922e-07     val_loss = 1.7004647645535442e-07\n",
            "Epoch 8980/10000      train_loss = 2.0782762533144705e-07     val_loss = 1.6844698791373958e-07\n",
            "Epoch 8990/10000      train_loss = 2.060219230770599e-07     val_loss = 1.6726433216263104e-07\n",
            "Epoch 9000/10000      train_loss = 2.045909184289485e-07     val_loss = 1.6607762631792866e-07\n",
            "Epoch 9010/10000      train_loss = 2.0317605731179356e-07     val_loss = 1.6487445009261137e-07\n",
            "Epoch 9020/10000      train_loss = 2.0129348854425189e-07     val_loss = 1.6366844590720575e-07\n",
            "Epoch 9030/10000      train_loss = 1.9970306652794534e-07     val_loss = 1.625816850037154e-07\n",
            "Epoch 9040/10000      train_loss = 1.981144208684782e-07     val_loss = 1.6129841640122322e-07\n",
            "Epoch 9050/10000      train_loss = 1.968945184671611e-07     val_loss = 1.6023602711356943e-07\n",
            "Epoch 9060/10000      train_loss = 1.9554272512323223e-07     val_loss = 1.5927311380892206e-07\n",
            "Epoch 9070/10000      train_loss = 1.9386355631922925e-07     val_loss = 1.577680137643256e-07\n",
            "Epoch 9080/10000      train_loss = 1.928057855593579e-07     val_loss = 1.56987738364478e-07\n",
            "Epoch 9090/10000      train_loss = 1.9125255334984104e-07     val_loss = 1.561293174745515e-07\n",
            "Epoch 9100/10000      train_loss = 1.8977294757860363e-07     val_loss = 1.5491177407511714e-07\n",
            "Epoch 9110/10000      train_loss = 1.882337699044001e-07     val_loss = 1.5382751428205665e-07\n",
            "Epoch 9120/10000      train_loss = 1.8760228215342067e-07     val_loss = 1.536010643121699e-07\n",
            "Epoch 9130/10000      train_loss = 1.8592091066693683e-07     val_loss = 1.5203275438580022e-07\n",
            "Epoch 9140/10000      train_loss = 1.8444376337356516e-07     val_loss = 1.5097275252173858e-07\n",
            "Epoch 9150/10000      train_loss = 1.8337249230171437e-07     val_loss = 1.5032115641133714e-07\n",
            "Epoch 9160/10000      train_loss = 1.8233194509775785e-07     val_loss = 1.494935446544332e-07\n",
            "Epoch 9170/10000      train_loss = 1.8082184283230163e-07     val_loss = 1.4845078055714112e-07\n",
            "Epoch 9180/10000      train_loss = 1.798252355911245e-07     val_loss = 1.4780923152102332e-07\n",
            "Epoch 9190/10000      train_loss = 1.7862060985862627e-07     val_loss = 1.4709630136167107e-07\n",
            "Epoch 9200/10000      train_loss = 1.7754717873685877e-07     val_loss = 1.4589618047011754e-07\n",
            "Epoch 9210/10000      train_loss = 1.7654140549439035e-07     val_loss = 1.4550647620126256e-07\n",
            "Epoch 9220/10000      train_loss = 1.7562416587679763e-07     val_loss = 1.4473064879894082e-07\n",
            "Epoch 9230/10000      train_loss = 1.7443396416183532e-07     val_loss = 1.4392144009889307e-07\n",
            "Epoch 9240/10000      train_loss = 1.7311603528469277e-07     val_loss = 1.4280941229571908e-07\n",
            "Epoch 9250/10000      train_loss = 1.7241144689705834e-07     val_loss = 1.4251605762183317e-07\n",
            "Epoch 9260/10000      train_loss = 1.7143750596915197e-07     val_loss = 1.4195265407579427e-07\n",
            "Epoch 9270/10000      train_loss = 1.7007305075367185e-07     val_loss = 1.4086202781982138e-07\n",
            "Epoch 9280/10000      train_loss = 1.6922733436786075e-07     val_loss = 1.4018887384281697e-07\n",
            "Epoch 9290/10000      train_loss = 1.685519777083755e-07     val_loss = 1.3963187939225463e-07\n",
            "Epoch 9300/10000      train_loss = 1.6782277612037433e-07     val_loss = 1.392355954976665e-07\n",
            "Epoch 9310/10000      train_loss = 1.6649086376219202e-07     val_loss = 1.3885438931993122e-07\n",
            "Epoch 9320/10000      train_loss = 1.655286610002804e-07     val_loss = 1.3817054878018098e-07\n",
            "Epoch 9330/10000      train_loss = 1.647592284825805e-07     val_loss = 1.37189203996968e-07\n",
            "Epoch 9340/10000      train_loss = 1.6388146661938663e-07     val_loss = 1.369092359482238e-07\n",
            "Epoch 9350/10000      train_loss = 1.6242201184013538e-07     val_loss = 1.3575800039689057e-07\n",
            "Epoch 9360/10000      train_loss = 1.624285630441591e-07     val_loss = 1.3628533679366228e-07\n",
            "Epoch 9370/10000      train_loss = 1.6107190958791762e-07     val_loss = 1.3506925711226359e-07\n",
            "Epoch 9380/10000      train_loss = 1.607049000540428e-07     val_loss = 1.3482821259458433e-07\n",
            "Epoch 9390/10000      train_loss = 1.5957155596879602e-07     val_loss = 1.3397432496731199e-07\n",
            "Epoch 9400/10000      train_loss = 1.5867149727455399e-07     val_loss = 1.3351510119719023e-07\n",
            "Epoch 9410/10000      train_loss = 1.5823256660496554e-07     val_loss = 1.333388581770123e-07\n",
            "Epoch 9420/10000      train_loss = 1.5744024040031945e-07     val_loss = 1.3269662701986817e-07\n",
            "Epoch 9430/10000      train_loss = 1.563047931085748e-07     val_loss = 1.3231168338734278e-07\n",
            "Epoch 9440/10000      train_loss = 1.557030486765143e-07     val_loss = 1.316209647939104e-07\n",
            "Epoch 9450/10000      train_loss = 1.5541397146989766e-07     val_loss = 1.315268320922769e-07\n",
            "Epoch 9460/10000      train_loss = 1.5416900112086296e-07     val_loss = 1.3059693060313293e-07\n",
            "Epoch 9470/10000      train_loss = 1.5334988745507871e-07     val_loss = 1.3013119826155162e-07\n",
            "Epoch 9480/10000      train_loss = 1.5242689244132634e-07     val_loss = 1.2959425532699242e-07\n",
            "Epoch 9490/10000      train_loss = 1.5193606373031798e-07     val_loss = 1.289479172328356e-07\n",
            "Epoch 9500/10000      train_loss = 1.5154081722812407e-07     val_loss = 1.2909129054605728e-07\n",
            "Epoch 9510/10000      train_loss = 1.5118452267870452e-07     val_loss = 1.2904770585464576e-07\n",
            "Epoch 9520/10000      train_loss = 1.503521644963257e-07     val_loss = 1.284586232941365e-07\n",
            "Epoch 9530/10000      train_loss = 1.4946114390568255e-07     val_loss = 1.2773502078289312e-07\n",
            "Epoch 9540/10000      train_loss = 1.4886147425841045e-07     val_loss = 1.278119299286118e-07\n",
            "Epoch 9550/10000      train_loss = 1.48586508430526e-07     val_loss = 1.2720489905859722e-07\n",
            "Epoch 9560/10000      train_loss = 1.4768697553790844e-07     val_loss = 1.2679194583142817e-07\n",
            "Epoch 9570/10000      train_loss = 1.4721695151820313e-07     val_loss = 1.2656218473239278e-07\n",
            "Epoch 9580/10000      train_loss = 1.4667816117253096e-07     val_loss = 1.264095601527515e-07\n",
            "Epoch 9590/10000      train_loss = 1.4556556493516837e-07     val_loss = 1.25305092524286e-07\n",
            "Epoch 9600/10000      train_loss = 1.4535069681187451e-07     val_loss = 1.2553515205127042e-07\n",
            "Epoch 9610/10000      train_loss = 1.447591415626448e-07     val_loss = 1.2512131775110902e-07\n",
            "Epoch 9620/10000      train_loss = 1.442547272745287e-07     val_loss = 1.246581007308123e-07\n",
            "Epoch 9630/10000      train_loss = 1.4388956515176687e-07     val_loss = 1.2467216947698034e-07\n",
            "Epoch 9640/10000      train_loss = 1.4347355659083405e-07     val_loss = 1.246420282541294e-07\n",
            "Epoch 9650/10000      train_loss = 1.4244032797705586e-07     val_loss = 1.238169602402195e-07\n",
            "Epoch 9660/10000      train_loss = 1.4231758882488066e-07     val_loss = 1.237993103586632e-07\n",
            "Epoch 9670/10000      train_loss = 1.4176616502936668e-07     val_loss = 1.234603530519962e-07\n",
            "Epoch 9680/10000      train_loss = 1.4127368785921135e-07     val_loss = 1.2327330978223472e-07\n",
            "Epoch 9690/10000      train_loss = 1.4103250123298494e-07     val_loss = 1.2344852962087316e-07\n",
            "Epoch 9700/10000      train_loss = 1.4011918381129362e-07     val_loss = 1.2269403271147894e-07\n",
            "Epoch 9710/10000      train_loss = 1.394185318304153e-07     val_loss = 1.2205053678826516e-07\n",
            "Epoch 9720/10000      train_loss = 1.3907445861605083e-07     val_loss = 1.219323308987441e-07\n",
            "Epoch 9730/10000      train_loss = 1.38611596867122e-07     val_loss = 1.2183748765437485e-07\n",
            "Epoch 9740/10000      train_loss = 1.3828757516876067e-07     val_loss = 1.2182361786017282e-07\n",
            "Epoch 9750/10000      train_loss = 1.3815612476264505e-07     val_loss = 1.2210520594635454e-07\n",
            "Epoch 9760/10000      train_loss = 1.3708188362215878e-07     val_loss = 1.2107693692087196e-07\n",
            "Epoch 9770/10000      train_loss = 1.3664148923453467e-07     val_loss = 1.208568392030429e-07\n",
            "Epoch 9780/10000      train_loss = 1.3657357555985072e-07     val_loss = 1.2099157231659774e-07\n",
            "Epoch 9790/10000      train_loss = 1.360053687449181e-07     val_loss = 1.2059440734901727e-07\n",
            "Epoch 9800/10000      train_loss = 1.357076513386346e-07     val_loss = 1.202730572913424e-07\n",
            "Epoch 9810/10000      train_loss = 1.3556291378336027e-07     val_loss = 1.2049707720507286e-07\n",
            "Epoch 9820/10000      train_loss = 1.3511724716863682e-07     val_loss = 1.2015776462703798e-07\n",
            "Epoch 9830/10000      train_loss = 1.3437119150694343e-07     val_loss = 1.1968752744451194e-07\n",
            "Epoch 9840/10000      train_loss = 1.3386065234044509e-07     val_loss = 1.19218881877714e-07\n",
            "Epoch 9850/10000      train_loss = 1.3343353089112497e-07     val_loss = 1.1917051523369082e-07\n",
            "Epoch 9860/10000      train_loss = 1.3334675941223395e-07     val_loss = 1.1906371355507872e-07\n",
            "Epoch 9870/10000      train_loss = 1.3301726653480728e-07     val_loss = 1.191719931625812e-07\n",
            "Epoch 9880/10000      train_loss = 1.3272874355152453e-07     val_loss = 1.1910255892644273e-07\n",
            "Epoch 9890/10000      train_loss = 1.3206935989273916e-07     val_loss = 1.1831401991457824e-07\n",
            "Epoch 9900/10000      train_loss = 1.31386912016751e-07     val_loss = 1.1842485747592946e-07\n",
            "Epoch 9910/10000      train_loss = 1.3169372437005222e-07     val_loss = 1.1898691099077041e-07\n",
            "Epoch 9920/10000      train_loss = 1.3097131557060493e-07     val_loss = 1.1806400124214633e-07\n",
            "Epoch 9930/10000      train_loss = 1.3109348628859152e-07     val_loss = 1.1848369751987775e-07\n",
            "Epoch 9940/10000      train_loss = 1.3007588961500005e-07     val_loss = 1.1786858777895759e-07\n",
            "Epoch 9950/10000      train_loss = 1.3008578036988183e-07     val_loss = 1.1806115907120329e-07\n",
            "Epoch 9960/10000      train_loss = 1.3040613566772663e-07     val_loss = 1.1859270898639807e-07\n",
            "Epoch 9970/10000      train_loss = 1.2992119025057036e-07     val_loss = 1.1804221600186793e-07\n",
            "Epoch 9980/10000      train_loss = 1.2878206234745448e-07     val_loss = 1.1694739754375405e-07\n",
            "Epoch 9990/10000      train_loss = 1.2911544899907312e-07     val_loss = 1.1783729547687471e-07\n"
          ]
        }
      ],
      "source": [
        "epoch_loss = []\n",
        "\n",
        "for e in range(T):\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  # mae_train = []\n",
        "  # mae_val = []\n",
        "\n",
        "  reassign_values(e)\n",
        "  for step, (x_tr_batch,y_tr_batch) in enumerate(train_data):\n",
        "    loss = train_step(x_tr_batch,y_tr_batch)\n",
        "    # y_pred_batch = model(x_tr_batch)\n",
        "    # error = mae(y_tr_batch, y_pred_batch)\n",
        "    # mae_train.append(error)\n",
        "    train_losses.append(loss)\n",
        "  \n",
        "  for step, (x_v_batch,y_v_batch) in enumerate(val_data):\n",
        "    loss = val_step(x_v_batch,y_v_batch)\n",
        "    # y_pred_batch = model(x_v_batch)\n",
        "    # error = mae(y_v_batch, y_pred_batch)\n",
        "    # mae_val.append(error)\n",
        "    val_losses.append(loss)\n",
        "\n",
        "  #l0 regularisation\n",
        "  for i in range(len(d_layers)):\n",
        "    kernel, bias = d_layers[i].layer.get_weights()\n",
        "    kernel1 = threshold_matrix(kernel, l0_reg)\n",
        "    bias1 = threshold_matrix(bias, l0_reg)\n",
        "    d_layers[i].layer.set_weights([kernel1, bias1])\n",
        "\n",
        "  train_loss = np.mean(np.array(train_losses))\n",
        "  val_loss = np.mean(np.array(val_losses))\n",
        "  # mae_t = np.mean(np.array(mae_train))\n",
        "  # mae_v = np.mean(np.array(mae_val))\n",
        "  epoch_loss.append([train_loss, val_loss])\n",
        "  \n",
        "  if e%10 == 0:\n",
        "    print(f'Epoch {e}/{T}      train_loss = {train_loss}     val_loss = {val_loss}') #    mae:  train = {mae_t}    val = {mae_v}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "epoch_loss = np.array(epoch_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SyyadLmrMQtj"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_results(losses):\n",
        "    train_loss = losses[:,0]\n",
        "    val_loss = losses[:,1]\n",
        "    epochs = np.arange(T)\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, train_loss, label='train loss')\n",
        "    plt.plot(epochs, val_loss, label='val loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_results(epoch_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KJgy9adcS_WK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-1.1920705]\n",
            " [-2.1194992]\n",
            " [-2.6077929]\n",
            " [-0.4929229]\n",
            " [-1.7848647]], shape=(5, 1), dtype=float32)\n",
            "tf.Tensor([-1.19190103 -2.11939958 -2.60754654 -0.49264155 -1.78476747], shape=(5,), dtype=float64)\n"
          ]
        }
      ],
      "source": [
        "x_test, y_test = x_train[:5], y_train[:5]\n",
        "y_pred = model(x_test)\n",
        "print(y_pred)\n",
        "print(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYwCUKxVYNOs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cog2Dw4CYNL0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "T9cqTAk-aGDH"
      },
      "outputs": [],
      "source": [
        "## Extract formula: Method 1 #################################\n",
        "kernels = []\n",
        "biases = []\n",
        "i = 0\n",
        "\n",
        "for layer in model.layers:\n",
        "  if i%2 == 1: #take only dense layers and not func layers\n",
        "    k,b = layer.get_weights()\n",
        "    kernels.append(k)\n",
        "    biases.append(b)\n",
        "  i = i+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Extract formula: Method 2 ###################################\n",
        "\n",
        "kernels = []\n",
        "biases = []\n",
        "for i in range(len(d_layers)):\n",
        "  k, b = d_layers[i].layer.get_weights()\n",
        "  kernels.append(k)\n",
        "  biases.append(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Ff0QWgLdS_Tu"
      },
      "outputs": [],
      "source": [
        "in_nodes = sympy.symbols([f'x_{j+1}' for j in range(input_dim)], real=True)\n",
        "\n",
        "def symbolic_dense(in_expr, kernel, bias):\n",
        "  out_expr = []\n",
        "  for i in range(kernel.shape[1]):\n",
        "    expr = bias[i] + sum([w*x for w,x in zip(kernel[:,i], in_expr)])\n",
        "    out_expr.append(expr)\n",
        "  return out_expr\n",
        "\n",
        "#referred https://github.com/martius-lab/EQL_Tensorflow/blob/master/utils.py\n",
        "def yield_with_repeats(iterable, repeats):\n",
        "    \"\"\" Yield the ith item in iterable repeats[i] times. \"\"\"\n",
        "    it = iter(iterable)\n",
        "    for num in repeats:\n",
        "        new_val = next(it)\n",
        "        for i in range(num):\n",
        "            yield new_val\n",
        "\n",
        "def yield_equal_chunks(l, n):\n",
        "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
        "    for i in range(0, len(l), n):\n",
        "        yield l[i:i + n]\n",
        "\n",
        "def iter_by_chunks(lst, chunk_lens):\n",
        "    splits = [0] + list(accumulate(chunk_lens))\n",
        "    for beg, end in zip(splits[:-1], splits[1:]):\n",
        "        yield lst[beg:end]\n",
        "\n",
        "def input_generator(in_expr):\n",
        "  num_args = [1,1,1]\n",
        "  repeats = [v,v,v]\n",
        "  lengths = (a * b for a, b in zip(repeats, num_args))\n",
        "  all_inputs = iter_by_chunks(in_expr, lengths)\n",
        "  for big_input, arg in zip(all_inputs, repeats):\n",
        "        yield from zip(*yield_equal_chunks(big_input, arg))\n",
        "\n",
        "def symbolic_eql(in_expr):\n",
        "  repeats = [v,v,v]\n",
        "  fn_iter = yield_with_repeats(sympy_ops, repeats)\n",
        "  inp_iter = input_generator(in_expr)\n",
        "  expr = [fn(*item) for item,fn in zip(inp_iter, fn_iter)]\n",
        "  return expr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "W94kbIsYS_RT"
      },
      "outputs": [],
      "source": [
        "res = in_nodes\n",
        "for i in range(L-1): ##hidden layers\n",
        "  res = symbolic_dense(res, kernels[i], biases[i])\n",
        "  res = symbolic_eql(res)\n",
        "final_expr = symbolic_dense(res, kernels[L-1], biases[L-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSZhdujAs0Zp",
        "outputId": "b5f91554-7333-46cf-8a72-8d9292622874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.00440522553743672*x_1 + 0.646656155586243*sin(0.942707657814026*x_1 - 0.763050496578217) + 0.788830280303955*sin(0.946794509887695*x_1 - 0.774977922439575) + 1.03096091747284*sin(1.0020968914032*x_1 - 0.939432442188263) + 1.32426679134369*sin(1.0506991147995*x_1 - 1.0812736749649) - 0.0343829196878342]\n"
          ]
        }
      ],
      "source": [
        "print(final_expr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGdCAYAAAAvwBgXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABEYElEQVR4nO3deXhU1f3H8fedyUxICAQh7CQkQArIJhLFICJoi6C1UiwVWxV+LogFFdGq1AVcsVXrLhW1olVRq4i4ixVZBQQNm+xJCKtJWBJCMJPMvb8/BkYuCUiAmTuZfF7PM4/MOZPMN4NtPp57zvcalmVZiIiIiDjA5XQBIiIiUnspiIiIiIhjFERERETEMQoiIiIi4hgFEREREXGMgoiIiIg4RkFEREREHKMgIiIiIo6JcbqAozFNk23btlGvXj0Mw3C6HBERETkGlmWxd+9eWrRogct19DWPiA4i27ZtIzk52ekyRERE5Dhs3ryZVq1aHfU1ER1E6tWrBwR+kPr16ztcjYiIiByL4uJikpOTg7/Hjyaig8jByzH169dXEBEREalhjmVbhTarioiIiGMURERERMQxCiIiIiLimIjeIyIiItHHsiwqKirw+/1OlyInwOPx4Ha7T/j7KIiIiEjY+Hw+tm/fTmlpqdOlyAkyDINWrVqRkJBwQt9HQURERMLCNE1ycnJwu920aNECr9erZpU1lGVZFBQUsGXLFtLT009oZURBREREwsLn82GaJsnJycTHxztdjpygxo0bk5ubS3l5+QkFEW1WFRGRsPqllt9SM5ys1Sz92yAiIiKOURARERGJIKmpqTz55JPB54ZhMH369LDXMWHCBE477bSQv4+CiNQo2QUlzFqbT07hPqdLEREJi+3btzNw4MBjem24wsPJpM2qUiPsKfVx09Qs5q/fQYqRT57VhLPTm/HM5d1JjPc4XZ6IiI3P58Pr9Z6U79WsWbOT8n0ilVZEpEa4aWoWCzf8yDTveGbF3so073gWbviRG6d+73RpIlIL9O3bl9GjRzN69GgaNGhAo0aNuPvuu7EsCwhcTnnwwQcZPnw4iYmJXHfddQAsWLCAPn36EBcXR3JyMjfddBP79v28opufn8/FF19MXFwcaWlpvPHGG5Xe+/BLM1u2bGHo0KE0bNiQunXrkpGRwaJFi5gyZQr33Xcfy5YtwzAMDMNgypQpABQVFTFixAiaNGlC/fr1Oe+881i2bJntfR555BGaNm1KvXr1uOaaa/jpp59O8qdYNQURiXjZBSXMWV9AS36kmysbgG6ubFryI3PWF+gyjUgtFe5Lta+++ioxMTEsWrSIp59+mieeeIKXXnopOP/oo4/SuXNnli5dyj333MOKFSu44IILGDx4MMuXL+ftt99m3rx5jB49Ovg1w4cPJzc3l6+++op3332X559/nvz8/CPWUFJSwrnnnsu2bduYMWMGy5Yt4/bbb8c0TS677DJuvfVWOnXqxPbt29m+fTuXXXYZlmVx0UUXsWPHDj755BOWLl3K6aefzvnnn8+uXbsAeOeddxg/fjwPPfQQS5YsoXnz5jz//POh+zAPoUszEvF+2FYMQJ7VhGVmG7q5sllmtiHPagJA7s59pCXVdbJEEQmjg5dq56wvCI71SW8c8ku1ycnJPPHEExiGQfv27VmxYgVPPPFEcPXjvPPO47bbbgu+/qqrruJPf/oTY8aMASA9PZ2nn36ac889l0mTJpGXl8enn37KwoUL6dmzJwAvv/wyHTt2PGINb775JgUFBXz77bc0bNgQgHbt2gXnExISiImJsV3O+eqrr1ixYgX5+fnExsYC8NhjjzF9+nTeffddRowYwZNPPsnVV1/NtddeC8CDDz7Il19+GZZVEa2ISMSbsiA3+Oex5SM5v+wfDPbdh59AA53URgohIrXJTVOzmL+h0DY2f0NhyC/VnnXWWbbeGZmZmaxfvz54z5yMjAzb65cuXcqUKVNISEgIPi644IJgh9nVq1cTExNj+7oOHTrQoEGDI9aQlZVF9+7dgyHkWCxdupSSkhIaNWpkqyUnJ4eNGzcCsHr1ajIzM21fd/jzUNGKiES0ZZt3s2TTbtz4meYdH1wNGey7D4AzUk/RaohILXLwUu3h/JYVvFTr1P8n1K1rf1/TNLn++uu56aabKr02JSWFtWvXAtVrDBYXF1ftukzTpHnz5nz99deV5o4WesIlpCsiEydO5IwzzqBevXo0adKEQYMGBT94kWNx1/srAUg1ttv2h6QYgWuow3qlOlWaiDhg066j3ywvd2fo9ossXLiw0vOj3Wfl9NNPZ9WqVbRr167Sw+v10rFjRyoqKliyZEnwa9auXcuePXuOWEPXrl3JysoK7u04nNfrrXRX49NPP50dO3YQExNTqY6kpCQAOnbsWOXPFw4hDSKzZ89m1KhRLFy4kJkzZ1JRUUH//v1tO4ZFjiS7oISV24px4+efnknB8WVmWnB/SKcWiU6VJyIOaN3w6PeoCeWl2s2bNzN27FjWrl3L1KlTeeaZZ7j55puP+Po77riDb775hlGjRpGVlcX69euZMWMGN954IwDt27dnwIABXHfddSxatIilS5dy7bXXHnXV4/LLL6dZs2YMGjSI+fPnk52dzXvvvcc333wDBE7v5OTkkJWVRWFhIWVlZfz6178mMzOTQYMG8fnnn5Obm8uCBQu4++67gyHo5ptv5t///jf//ve/WbduHePHj2fVqlUn8dM7spAGkc8++4zhw4fTqVMnunXrxiuvvEJeXh5Lly4N5dtKlDj4Xz4pRj7dXDnB8bHlN+DHTeeW9XVZRqSWadM4gT7pjXEfdjnDbRj0SW8c0v9PuOqqq9i/fz9nnnkmo0aN4sYbb2TEiBFHfH3Xrl2ZPXs269ev55xzzqF79+7cc889NG/ePPiaV155heTkZM4991wGDx4cPGJ7JF6vly+++IImTZpw4YUX0qVLFx555JHgqsyll17KgAED6NevH40bN2bq1KkYhsEnn3xCnz59uPrqq/nVr37F0KFDyc3NpWnTpgBcdtll3Hvvvdxxxx306NGDTZs2ccMNN5ykT+7oDOvgIegw2LBhA+np6axYsYLOnTtXmi8rK6OsrCz4vLi4mOTkZIqKiqhfv364ypQIkV1QwnmPz65yf4gfNzNGn03XVg2cLlNEjtFPP/1ETk4OaWlp1KlT57i/T1FpOTdO/T6sp2b69u3LaaedZmu9Xtsd7e+zuLiYxMTEY/r9HbbNqpZlMXbsWHr37l1lCIHAnpL77rsvXCVJBNtT6mPCjB8A8ONmsO++YEdVCzd90hsrhIjUUonxHl675kxyCveRu3MfqY3qanW0Bgvb8d3Ro0ezfPlypk6desTXjBs3jqKiouBj8+bN4SpPIszB43lu/KQZ2wHIsZrjx03vA//lIyK1W1pSXfq1b6IQUsOFZUXkxhtvZMaMGcyZM4dWrVod8XWxsbHBZitSex08nnekSzL3XdJJ95cRkbCq6uirnBwhXRGxLIvRo0czbdo0vvrqK9LS0kL5dhIl7JtUKx/ZDeXxPBERCa+QBpFRo0bx+uuv8+abb1KvXj127NjBjh072L9/fyjfVmq4g8fzDrZ0B2wt3dVJVUQkeoT00sykSYHeD3379rWNv/LKKwwfPjyUby012MHjefM3FNo2qWLE0Kddkq4Hi4hEkZAGkTCeDJYo88zl3YPH83KswJn7Pu2StElVRCTK6F4zEpF0PE9EpHZQEJGIlpakACIiEs3C1kdERERE5HAKIiIiIr+gb9++jBkzxukyopKCiIiIyAmyLIuKigqny6iRFERERESOYvjw4cyePZunnnoKwzAwDIMpU6ZgGAaff/45GRkZxMbGMnfuXIYPH86gQYNsXz9mzBhbGwvLsvjHP/5BmzZtiIuLo1u3brz77rvh/aEiiDarioiIHMVTTz3FunXr6Ny5M/fffz8Aq1atAuD222/nscceo02bNjRo0OCYvt/dd9/NtGnTmDRpEunp6cyZM4crrriCxo0bc+6554bqx4hYCiIiIlIz+Stgdy6ckgru0P06S0xMxOv1Eh8fT7NmzQBYs2YNAPfffz+/+c1vjvl77du3j3/+85989dVXZGZmAtCmTRvmzZvHCy+8oCAiIiJSI/gr4OVfw7bvoUV3uObLkIaRI8nIyKjW63/44Qd++umnSuHF5/PRvXvtbNioICIiIjXP7txACIHAP3fnQlK7sJdRt669z5HL5arUVby8vDz4Z9M0Afj4449p2bKl7XW19e7zCiIiIlLznJIaWAk5uCJySmpI387r9eL3+3/xdY0bN2blypW2saysLDweDwCnnnoqsbGx5OXl1crLMFVREBERkZrHHRO4HBOGPSIAqampLFq0iNzcXBISEoIrG4c777zzePTRR3nttdfIzMzk9ddfZ+XKlcHLLvXq1eO2227jlltuwTRNevfuTXFxMQsWLCAhIYFhw4aF9OeIRDq+KyIiNZM7JnA5Jgx7Q2677TbcbjennnoqjRs3Ji8vr8rXXXDBBdxzzz3cfvvtnHHGGezdu5errrrK9poHHniAe++9l4kTJ9KxY0cuuOACPvzwQ9LS0kL+c0Qiw4rgW+QWFxeTmJhIUVER9evXd7ocERE5AT/99BM5OTmkpaVRp04dp8uRE3S0v8/q/P7WioiIiIg4RkFEREREHKPNquKY7IISNu0qJbVRXdKS6v7yF4iISNRREJGw21Pq46apWcxZXxAc65PemGcu705ivMfBykREJNx0aUbC7qapWczfUIgbP2nGdtz4mb+hkBunfu90aSIiEmYKIhJW2QUlgZUQq4Jp3vHMir2Vad7xYFUwZ30BOYX7nC5RREIsgg9rSjWcrL9HBREJq027SgFIMfLp5soGoJsrmxQjH4DcnQoiItHqYHfR0tJShyuRk8Hn8wHgdrtP6Ptoj4iEzZ5SH8/P2gBAntWEZWYburmyWWa2Ic9qAkBqI21aFYlWbrebBg0akJ8f+A+P+Ph4DMNwuCo5HqZpUlBQQHx8PDExJxYlFEQkbG6amsV3m/bgxk+Kkc8Q3720NHYGQogRQ592STo9IxLlmjVrBhAMI1JzuVwuUlJSTjhMKohIWBzcG+LGzzTv+OBKyGDfffhxc0ZKA565vHbeAlukNjEMg+bNm9OkSRPbXWml5vF6vbhcJ77DQ0FEwuJoe0NyrOb85bx2OrorUou43e4T3lsg0UGbVSUsDv6LttVqRKnlBaDU8rLVagRAjEvXiUVEaiMFEQmLgzfMbmnsJN4I7LSON3y0NHYCUGHqOJ+ISG2kICJh0bphPPDzaRlAp2VERER7RCT09pT6mDDjBwD8uBnsu48UI1+nZURERCsiEno3Tc1i3oETM2nGdgByrOb4cXN2uySdlhERqcW0IiIhlZW3mznrC/Di4wPv3XR0bbEd273vkk46LSMiUotpRURC6u4PVuLGzwzvPXR0bQHU0l1ERH6mICIhk11QwsqtxaQY+XRwbQ6OrzZbaZOqiIgACiISQgebmAVOyqQBsMZsxSW+B/HjpnOL+tqkKiJSyymISMgc+i/XwXZlPjz4CXRTHN2vXdhrEhGRyKIgIiFzsIlZipFPV1cOAF1dOcH9IbFetXcWEantFEQkJPaU+njuqw2AmpiJiMiR6fiuhMQNr3/Hkk27ceMnxchniO9eWho71cRMRERsFETkpMvK28032Ttx42ea9166uXJYZqYx2Hc/ftyc0bqBmpiJiAigSzMSAmPeygKgjbGFbgf2hnRz5ZB6oKvq4NNbqYmZiIgACiJykmUXlJC7qxQ3fp7xPFvla4wqR0VEpDZSEJGT6r9LA43L2hhb6ODaGhxfa7Yk12oOQM82jRypTUREIo+CiJxUH2RtA6CVUWgbf6RiKH7cNKsfq02qIiISpCAiJ012QQnb9vyEGz+3xEwLjpdaHuabXQCY8LtOTpUnIiIRSEFETpofthUD9gZmAIN8D+DDS7zXzYDOzZ0qT0REIpCCiJw0UxbkArDVasQaMxkINDDbaLUE4KWrMpwqTUREIlRIg8icOXO4+OKLadGiBYZhMH369FC+nTgou6Ak2MDsv9776eDazGqzFUN89wZ6h6SeQq92SU6XKSIiESakQWTfvn1069aNZ5+t+hinRI+Dd9pNMfLp5soGoKNrCy2NnQAM65XqVGkiIhLBQtpZdeDAgQwcODCUbyER4pS4QIOyg/eV6ebKtt1XplOLRCfLExGRCBVRLd7LysooKysLPi8uLnawGqmOv3+2NvjnseUjAci1muPHTZ/0xjqyKyIiVYqozaoTJ04kMTEx+EhOTna6JDkG89YX2O4t87/Y2/mnZ1Jw/rb+v3KwOhERiWQRFUTGjRtHUVFR8LF582anS5JjMOK1JQCkG3lV3ltm1XatbImISNUi6tJMbGwssbGxTpch1TB7bT6l5SZefLzvHV/la3RvGREROZKIWhGRmidryx4AerrWEGdUBMezzWa6t4yIiPyikK6IlJSUsGHDhuDznJwcsrKyaNiwISkpKaF8awmT01o1AGCR2YFSy0u84WO/5eEi30P4cXNq8/raqCoiIkcU0iCyZMkS+vXrF3w+duxYAIYNG8aUKVNC+dYSBntKfbw8LxcAH15OK5tMT9caFpkd8OElxmUw9bqznC1SREQiWkiDSN++fbEsK5RvIQ664fXvgqdlUox88qwmzDW7AlC/Tgwf33gOifEeh6sUEZFIFlGbVaXmyC4osR3Z7ebKYZmZxmDf/fhx88Ho3iQ3ine6TBERiXDarCrHZdaafABSje1VHtldmL3TsdpERKTmUBCR4/LGojwA3JhVzuvIroiIHAsFEam27IISsgv34cbPY55/BceXm2k6sisiItWiICLV9snybUDgskxXV25w/NbyEfhx07ZxXR3ZFRGRY6IgItX2zKxAb5gYfLZxEzcAf+7ZOuw1iYhIzaQgItUye20+ZRXWgZbu9wXHl5mtg5dl+nVo4lR5IiJSwyiISLX8e37ghExvVxbxRnlw/NmKQfhxc0qcR5dlRETkmCmISLV8t2k3AC2wH89tROAOu+lNE8Jek4iI1FwKInLMsgtK2Fvmx4uPv3neDo77LfjA7A3AkIxkp8oTEZEaSEFEjtm7SzYDkOlaRbzx80bVkeU3s584DBRERESkehRE5JhNz9oKwA4r0TaeazUD4OKuzcJek4iI1GwKInJMsvJ2s62oDDd+HvW8HBxfZrYm22oFwJAzUpwqT0REaigFETkmd3+wEjjYxCwnOD62fBR+3NT1ujknvbFT5YmISA2lICK/KLughJVbi3Hj5wnPpOD4skNaur94ZYZT5YmISA2mICK/aNOuUgBSjPzDVkNuwI+bzi3q0ys9yanyRESkBlMQkV9UWlYBwFarEaWWNzBmedlsBTqoju7XzrHaRESkZlMQkV908N4yyUZ+8NhuvOGjpRFoahbrdTtWm4iI1GwKInJU2QUlrNm+98D+kOeD48vMNPIOrIikNlJLdxEROT4KInJUs9bkAwdPy+QGx28rH4EfN+lN6ureMiIictwUROSo3vo20E3VjWkbNwlcjvnTma3DXpOIiEQPBRE5oqy83azPLznQxOyF4PjyQ47t9u3QxKnyREQkCiiIyBGNm7YCCFyW6XbIsd1bDhzb7di8ni7LiIjICVEQkSplF5Swekdgk+qTh2xSLbU8wWO7N5+f7lR5IiISJRREpEqLcgJHc1ON7XQ5ZJNqvFH+87Fdj47tiojIiVEQkSpt2bW/yvE1Zisd2xURkZNGQUSq9K85GwH40WpAntkIgNVmS37nexA/bs5IPUX7Q0RE5ITFOF2ARJ5/z8vGb0Ic+1kWez1uw8K0YIhvPD68xMUYvHTVGU6XKSIiUUArIlLJswdaul/sWoDbsABwGTDQtQSAfh2bkhjvcaw+ERGJHgoiYpNdUMKufeW48XNlzMzguN8y+MjsCUC/9uodIiIiJ4eCiNh8tHwbAG2NrXRx5QXHB/nuZT9xuIAhGckOVSciItFGQURs1u7YC0BLo8A2nmSUAHBu+6Sw1yQiItFLQURsVm4rBmCbdYptfIsVCCCnt24Y9ppERCR6KYhIUHZBCZt2lh64t8zLwfFlZmuyrVYA/LZrC6fKExGRKKQgIkGrtgdWQ1KMfLoecm+ZseWj8OMmvUmCeoeIiMhJpSAiQZNmBZqYbbUasdoMrIAsM9sE77T7+JBujtUmIiLRSUFEAFi2eTc/bC/GjZ/3vBPo6NrCOrMlQ3z3Bjqptj6FrskNnC5TRESijIKIAHD9f5YC0MHIDd7k7leurbQ1tgLQvlmCU6WJiEgUUxARsgtK2FFchhs/kzxP2Oa6GoG9IjuKy5woTUREopyCiLAoZycAqcZ2Uly7guOmBTPMTAAu6NTMkdpERCS6KYgI89cHmpfVocQ2/nvf39hPHKBuqiIiEhoKIsKstYUHLss8axvvaGwHoFvLek6UJSIitYCCSC2XXVDCPp+/0mUZgGVWOwAyUhs5UZqIiNQCCiK13H++2QSAh3Lb+DqzOeusFACuyEwNd1kiIlJLKIjUctmFgX0hTY0i2/jEij/jx039OjHqpioiIiGjIFKL7Sn1sTgncDlmsdme/VYMAKWWh/lmZwBu7f8rx+oTEZHoF5Yg8vzzz5OWlkadOnXo0aMHc+fODcfbyi+48uVF7C838eLjfe944owKss1m9Cj7Fz68uIBhvdKcLlNERKJYyIPI22+/zZgxY7jrrrv4/vvvOeeccxg4cCB5eXmhfms5iuyCElZsDbR0n+H9Gx1cWwBo49pBCyPQV2T0ee2cLFFERGqBkAeRf/7zn1xzzTVce+21dOzYkSeffJLk5GQmTZoU6reWo/ho+TYA2hhb6ODaVuVrmjeIC2dJIiJSC4U0iPh8PpYuXUr//v1t4/3792fBggWVXl9WVkZxcbHtIaGxKDuw6pFi5NvGs83GwbvtntVGx3ZFRCS0QhpECgsL8fv9NG3a1DbetGlTduzYUen1EydOJDExMfhITlY3z1BZnLMbABeWbfyhiivx4yYpwaPTMiIiEnJh2axqGIbtuWVZlcYAxo0bR1FRUfCxefPmcJRX63yyYhvlpoUXH095ng+Ol1ou5ppdAXhwUBenyhMRkVokJpTfPCkpCbfbXWn1Iz8/v9IqCUBsbCyxsbGhLEmAu6YtB6CPaxnxhi84fnP5jfjwEuOCAZ2bO1WeiIjUIiFdEfF6vfTo0YOZM2faxmfOnEmvXr1C+dZyBNkFJeze78eNn7/FvGabMw/863BWmvaGiIhIeIR0RQRg7NixXHnllWRkZJCZmcnkyZPJy8tj5MiRoX5rqcJLczcCkG7k0ca10zaXZzUBoGdbBREREQmPkAeRyy67jJ07d3L//fezfft2OnfuzCeffELr1q1D/dZShelZgaO6ZxirbeN5ZiLZVisAftu1RdjrEhGR2inkQQTgL3/5C3/5y1/C8VZyFNkFJZT6TNz4GRHzsW3upYrf4sdNHY+h0zIiIhI2utdMLbJqe6AvS4qRT7Jrt21ukdURgEu7twp7XSIiUnspiNQiD334AwAFVj3MQ9qHLDNbscEKXCq7tk9bJ0oTEZFaSkGklsguKGHH3rID95YZh+uQNi5PVlwWaGJWV03MREQkvBREaon7PlwFwKnGhkqnZbZajQEYekZK2OsSEZHaTUGklpi3vhCAwcYc23ihWY+NVksALs1QS30REQkvBZFaILugBL8FbvycH/Odbe7Zigvx4ybWrdMyIiISfgoitcBHywO9Q9oYW0hxFdnmFlqBe8sMydBpGRERCT8FkVpg3oYCAJKNH23jOWYS663AvpBrztFpGRERCT8FkVpgcc4eADxU2MYfqLgCP24axsXosoyIiDhCQSTKvb04D4A49vO859ngeKnlZZ55GgAPX9rVidJEREQURKLdlG9yAfi9ay7uQ3qHPFL+R3x4cRkwoHNzZ4oTEZFaT0Ekyq3bsReAGMpt4xUHbjPUoVm9sNckIiJykIJIFPtkxTb8Fnjxcafnv8FxvwXvm+cA8H9npzlVnoiIiIJINPv7p2sAyHStIt7wBcevKx/DfuIwgCFqYiYiIg5SEIlS2QUlbNq1Hzd+xsb8vBpy6CbV+3/XyaHqREREAhREotSq7cUApBj5dHPlBscv8d2HDy8JsW6u7JXqTHEiIiIHKIhEqSe/WAfAVqsRq81A19RlZhuyrcCfJ1+Z4VhtIiIiBymIRKHsghI2Fu7DjZ93vXfT0bWFNWZThvjuxY+bM1qfQq92SU6XKSIioiASje7/cCUApxpr6eraAkAH14+0NzYAUK+O27HaREREDqUgEoW+XrcTgOuMT2zj1xqfArBg486w1yQiIlIVBZEok11QAoAbP+fELLfNxRE4wlthWmGvS0REpCoKIlHm3SWbAWhjbOEU1883ubMsuMu8FoDMNo0cqU1ERORwCiJR5rUD95Zpa2y1jY8tv5oCGgPwn2vPCndZIiIiVVIQiSLZBSWU+Ey8+HjS85xtbg8NAejSLMGJ0kRERKqkIBJFbvtvFgB9XN9Rx/h5H8hPlsF8szMAt190qhOliYiIVElBJIp8l1cEwGmst41PLr8QH14AzklvHPa6REREjkRBJEo8+79A+EighFGeT21zK2gHQKfmuiwjIiKRRUEkSry+KAeA/3N9imH8PO6zYLbZHYA7L9RlGRERiSwKIlFiR3E5AGXYu6Y+Uv5HXZYREZGIpSASBQ69LDPO865tbhMtAejdVr1DREQk8iiIRIEZywI9Q4a5Zla6LDPX7AbA9X3bOlGaiIjIUSmIRIF1+fsA+M5KtY1f4btDl2VERCSiKYjUcG8vzgMgjv284X3UNpdqBG5ud7Yuy4iISIRSEKnhHp+5BoAhrlm4DrksY1rwoZkJwKDuLZ0oTURE5BcpiNRw+XsDp2U6s9o2/nb5mewnDoAhGclhr0tERORYKIjUYPfPWAVAQ3YxxLPUNhdLoMW7V3/DIiISwfRrqgb7z8JcAMa7pthOywC8Yl0AwJlp2h8iIiKRS0GkBis3A/9sQLFtfI8JP1jtAXjg913CXZaIiMgxUxCpoQ5elkmkiD6edba5NyoG4D/QYTUtqW7YaxMRETlWCiI11PvfbwHgZte7tssylgWTzMEA3HSempiJiEhkUxCpoXbvrwCgFI9tfFL5byghcJfdsf07hL0uERGR6lAQqYE+Xr4NAC8+rvF8aZv70OoNwLnp2qQqIiKRT0GkBrrr/ZUAnO1aSZxRERzfaCaxzmoDwIRLtElVREQin4JIDZNdUMKe/eW48XN3zGu2udvLr8ePm7pelzapiohIjaAgUsPcc2A1pIuxjraufNtcT2MtAGmNE8Jel4iIyPFQEKlhFucGbmR3lfG5bdyy4DWzPwAXdm4e9rpERESOR0iDyEMPPUSvXr2Ij4+nQYMGoXyrWuNgE7PSA31CDppa3it4WuYv/dqFuywREZHjEtIg4vP5GDJkCDfccEMo36bWeOyzwI3tEijhz55vguOWBX83rwTgok5NHalNRETkeMSE8pvfd999AEyZMiWUb1Nr/Gt2NgDXuT6yNTF7vnwARSQC8NyVGU6UJiIiclxCGkSqq6ysjLKysuDz4uLio7y6dskuKKHCCvQOGemZYZvLJ9AzpFmC14nSREREjltEbVadOHEiiYmJwUdycrLTJUWMy/61AIABrjnEHnan3S00AaBNk3rhLktEROSEVDuITJgwAcMwjvpYsmTJcRUzbtw4ioqKgo/Nmzcf1/eJRgX7ygG4ls9s434L5prdAPj96S3DXpeIiMiJqPalmdGjRzN06NCjviY1NfW4iomNjSU2Nva4vjaazV4b6BfixUd7zzbb3D/LB+IjcElmSIZWkEREpGapdhBJSkoiKSkpFLXIETw2cw0A57iWVbos4yIQ3JrW0/4QERGpeUK6WTUvL49du3aRl5eH3+8nKysLgHbt2pGQoO6fx2rFlr0AtGWTbdyy4BXzQgAu7qbLMiIiUvOENIjce++9vPrqq8Hn3bt3B2DWrFn07ds3lG8dNe6fsQqARIoY55lmm3uufGCwidndvz017LWJiIicqJCempkyZQqWZVV6KIQcu3e+zQPgZtd7tt4hAB9bZwHQq03DcJclIiJyUkTU8V2prORAT/c91LGN55l1WWe1AeChwV3DXpeIiMjJoCASwUa8+i0AjSngFs9HtrnJFYPx4ybWDWlJdZ0oT0RE5IQpiESwOesCx3YfcE2xXZYxLXjP7AvA3y7S3hAREam5FEQi2E/+wD/X09g2/nT5APYTB8CwXmnhLktEROSkURCJUI8ecqfd0Z4vbHMd+BGAOjFGpa8TERGpSRREItTkOTkAXO362HZZxrLgHnM4AH3bN3GgMhERkZNHQSQCZReUUG5auPEzNGaWbe7R8gspOHCp5l9XZjhRnoiIyEmjIBKB7nxvGQBdjHW0cBXb5twH7iuTFB/SXnQiIiJhoSASgRbn7gFguGG/0+6hLd2vPqdtuMsSERE56RREIkx2QQkAbvz0ivnONvd2eWawpftf+rULe20iIiInm4JIhLnh9SUAdDVW0cTlt83NIrAnJL2xGpiJiEh0UBCJMGt/3AfAX423bePlFswyewBw7+86hb0uERGRUFAQiSBvLw7c4C6O/ZzlybHNzS/vgO/ARtVz0htX+loREZGaSEEkgvz9sx8AGOL6EtdhvcreoS8AzerHhrkqERGR0FEQiSC7SgN7QjJYYxsvt+BL8ywAHh3SLex1iYiIhIqCSIS4f8YqABIp4mLP97a5MeUjdFlGRESikoJIhHhnaWB/yFjXf20t3X0WzDR7AfD4H7QaIiIi0UVBJEKUlJkA1MXeSfX98p7B1ZBLM1qFvS4REZFQUhCJAM/+bz0QuCxzqWeJbS4GC4A43WlXRESikIJIBHhhzgYAxrjernSn3YfMqwD4bbeWTpQmIiISUgoiEWDvgcsysfxkG3+nvCe7aAjotIyIiEQnBRGHPfjRSgASKOFyz0Lb3ErSAWgY7w57XSIiIuGgIOKwl+ZtAmCka7rtsoxpwbtmPwDGXaiW7iIiEp0URBw0e20+AF58jPB8Ypt7s7w3+4kDYEhGcthrExERCQcFEQeNemMpABe6vsZ72KGYOZwGQFqjuDBXJSIiEj4KIg4q8QU2qf6Fj23jFRZ8bWYE5vqlh70uERGRcFEQccihd9pt5ymwzb1S3ifYxEyXZUREJJopiDjkuVmB3iG/d82pdKfdadb5AJzXXveVERGR6KYg4pC83fsBaEq+fdyMY53VBoB7LtZpGRERiW4KIg649e0sABqyi5s8n9rmcipS8BPoG5KWVDfcpYmIiISVgogDZv6wA4BxrjcqtXS/w7wegF5tTnGiNBERkbBSEHFAcZkfgPU0t40/UT6AHTQD4M0RvcJel4iISLgpiITZx8u3AYEmZrd6ptnmCmkKQPPE2LDXJSIi4gQFkTD76ztZAJzr+p7Yw1q6v2/2AWDsb9o7UJmIiEj4KYiE0bLNuymtsHDjZ0LMq7a5B8r/pJbuIiJS6yiIhNHg5xYA0N34gZauPba5vAOXZc5K0yZVERGpPRREwiS7oAQ/4MbPy55/2ObKLZhrdgNg1Hlq6S4iIrWHgkiYXP7iNwCcZWSR6PLb5l4o/02wpfs56eqmKiIitYeCSJj8WOwD4F7jddu4ZcG/zCEAjDgnLex1iYiIOElBJMzm0s72/JXyXpSQAMDfLjrViZJEREQcoyASBje89i0QuNPu1Z75trnsA8Gk9Sl1wl6XiIiI0xREwuDTHwI3trve9Y7tTrumBe+Z5wIw+vxfOVGaiIiIoxREQuzS5+cB0Iwd3Oz53Db3XXmqeoeIiEitpiASYkvzigB43PWC7QZ3AI9YlwGQ3ECXZUREpHZSEAmh2WsDl2S8+DjDs9Y2t8OM4XurMwCvXXtW2GsTERGJBAoiIXTHe8sA+LVrId7DVkOerxiKHzduIC2pbviLExERiQAhCyK5ublcc801pKWlERcXR9u2bRk/fjw+ny9UbxlxdhzoHfIH5tjGTQv+a/YD4N7f6ciuiIjUXjGh+sZr1qzBNE1eeOEF2rVrx8qVK7nuuuvYt28fjz32WKjeNmLc9f5yABIooZ/nB9vc5PLzgptUh/VSEzMREam9QhZEBgwYwIABA4LP27Rpw9q1a5k0aVKtCCJvLNoMwPWu6ZU2qX5o9QXg/A5q5y4iIrVbyIJIVYqKimjYsOER58vKyigrKws+Ly4uDkdZJ93bi/OAQAOzUZ5PbHM/mnGssQKrIHf/tlPYaxMREYkkYdusunHjRp555hlGjhx5xNdMnDiRxMTE4CM5uWb21nj+640ADHF9ZWtgBvBJRU/8uHEZ2qQqIiJS7SAyYcIEDMM46mPJkiW2r9m2bRsDBgxgyJAhXHvttUf83uPGjaOoqCj42Lx5c/V/ogiwaVcpAJ1YYxu3LHjSDPQOuaJnStjrEhERiTTVvjQzevRohg4detTXpKamBv+8bds2+vXrR2ZmJpMnTz7q18XGxhIbG1vdkiLKZyu2A+DGz+9iltnm1pQ3oYhEAO4f1CXstYmIiESaageRpKQkkpKSjum1W7dupV+/fvTo0YNXXnkFlyv625aMfOM7AM4wVhDnqrDNTeMcAOI9RqWvExERqY1Ctll127Zt9O3bl5SUFB577DEKCgqCc82aNQvV2zrq0E2qb3r/YZszLXjdvBCA+y7RaoiIiAiEMIh88cUXbNiwgQ0bNtCqVSvbnGVZoXpbR43/YAUAf3Z9XGmT6sPlf9AN7kRERA4Tsmslw4cPx7KsKh/R6id/4J+Hd1K1LHjDHAjAmPPbhbssERGRiBX9mzbC5MqXvgGgIbto7ym0zS0uTwmuhoz5Tfuw1yYiIhKpFEROkrkbdgFwv+vFSp1UH7X+DEDbRvHhLktERCSiKYicBM/+bz0QuK/MRR77kd29povvrcCN7V76vzPDXpuIiEgkUxA5CV6elw3Ada6PKq2GPFXxe/y4MVAnVRERkcMpiJwEu/cH+oUksds2blrwxoEjuxN+d2rY6xIREYl0CiIn6OapS4HAZZk/eeba5ob4xgU3qQ7rlRb22kRERCKdgsgJ+mDZDgBudv3HdlnmRzOWrAN7QyZf0cOJ0kRERCKegsgJuH/GKiBwZPfaw1ZDtlQ0xo8bgP6do7OTrIiIyIlSEDkB/16QC8ADrpcrbVL9hsBqSF2vPmIREZEj0W/Jk6Aja23PLQv+Zf4BgFH90p0oSUREpEZQEDlOf3h+HgCJFJHmKbXNrShvTgkJAPyln1q6i4iIHImCyHFaklcEwN9cr9ouy1gW3GCOBeBXTdU3RERE5GgURI7DHyfNB6AZO/ijZ6Ft7qXy3mylJQBf3NI3zJWJiIjULAoix+HbTXsA+LtrcqVNqtOtCwA4tVlCmKsSERGpeRREqmnZ5t1YgBs/bWLybXObzVNYY6UC8NwVGeEvTkREpIZREKmma15dAkAHI5dk1y7b3KSK3+HHTd1Yt+4rIyIicgwURKohu6CEwhIfbvy85PmHbc604H2zDwC3X9DeifJERERqHAWRanjgo0An1V8ZeTR37bXN5ZQ31H1lREREqklBpBpmrS0EoI/xjW3csmCEOQaAZvU84S5LRESkxlIQOUYj//MtEGhgdqfnI9vcnPI2bCTQuGzhXf3DXpuIiEhNpSByjD5bFTghc4fr9UpHdp+2LgcgqW5MuMsSERGp0RREjsEDH64EIIESLvfMt83tMeuQZXUA4L839A57bSIiIjWZgsgxeHn+JgBucr1ZaTXkror/w48br9vQkV0REZFqUhD5Ba/OzwEgjv1c5/naNldhwUyzJwBPX9493KWJiIjUeAoiv2D8hz8AMNQ1s9JqyGPlg/DhBWBA5+bhLk1ERKTGUxA5ircX5wGB1ZB7PG/Z5kwLXjUvBuDOCzqEvTYREZFooCByFI98uhqAIa6vcB22GvJBeUawgdnIfm3DXZqIiEhUUBA5it37KwDowlrbuGXBA+ZwAH7TsXG4yxIREYkaCiJHcMkzcwFoxg7+4Flim/tX+fnsoiEALw47M+y1iYiIRAsFkSNYtrUYgMddz9s2qVoWPGdeBsBvOzd1ojQREZGooSBShYcO3Nwujv1kejbY5qaWZ1JCAgDPXpER9tpERESiiYJIFV6clwvAMNd7tk2qlgV/N68C4KJOTRyoTEREJLooiBxmzNSlQKCd+x2eT2xzP5Q3o4hEAJ678oyw1yYiIhJtFEQOM33ZDgBucr1TaW/IDeYYAJIbxDpQmYiISPRREDnETW8GVkMSKeI6z5e2uc/KO5NHCgBz7/x12GsTERGJRgoih5ixPLAacmsVN7ebzjkANIyPCXdZIiIiUUtB5IDHvwh0UY1jP1d45trmfBbMOnBzu6cuPz3stYmIiEQrBZEDnvkqG4Ahrv9Vauc+pvz64M3tzklXJ1UREZGTRUEEeOzzNUBgNWSC503bXJkFX5qZADz+h25hr01ERCSaKYgAz87aCMBlVdzc7lLfeHx4cQOXZrQKf3EiIiJRrNYHkXumLwcCqyH3et6wzW0xG7DaagfAf67pGfbaREREol2tDyL/WbgZgKtd71daDflXxcX4cRPrhl7pSQ5UJyIiEt1qdRC56qVvgEDfkNs8H9nmTAveM/sCcP257cJdmoiISK1Qq4PInA27ALjD9VqlviHDfGPYTxwAY/u3D3dpIiIitUKtDSLXv7oYgIbs4nLPN7a5HWYdFlg9AHjyjzopIyIiEiohDSK/+93vSElJoU6dOjRv3pwrr7ySbdu2hfItj9nnqwsAuMc1pdJqyHXld+LHTXwMDDpdJ2VERERCJaRBpF+/frzzzjusXbuW9957j40bN/KHP/whlG95TCZ+sgoALz4u8iy1zRWadfnBagvAW9efHfbaREREapOQ3jjllltuCf65devW3HnnnQwaNIjy8nI8Hk8o3/qI9pT6eGFOLgADXPPwGlZwzrKgv+/v+HGTVNdD1+QGjtQoIiJSW4TtDm67du3ijTfeoFevXo6FEIDzH/8agMYU8JTnJdvcA+VD2UVDAP53a79wlyYiIhI2z/5vPV+u/pF2TRIYdV46aUl1Hakj5EHkjjvu4Nlnn6W0tJSzzjqLjz766IivLSsro6ysLPi8uLj4pNaSXVDCzn3luPHzmfd2294Q04Kp5m8A+L+zkkmMdy4siYiIhMqCDQX86aXFwedZW4p497utdGuVyGtX9wz7779q7xGZMGEChmEc9bFkyZLg6//617/y/fff88UXX+B2u7nqqquwLKvK7z1x4kQSExODj+Tk5OP/yaqwKCdwXLeNsYVGrjLb3KTyC4LHdccP6npS31dERCRSHAwhCZTwF9c0OhkbceNn2ZYibpz6fdjrMawjpYIjKCwspLCw8KivSU1NpU6dOpXGt2zZQnJyMgsWLCAzM7PSfFUrIsnJyRQVFVG/fv3qlFmlp/+3jn/OXM+5ru951ftocNyyoEvZZEpIYNhZKdw3qMsJv5eIiEikuf2/WbyzdCsJlLAidkTwysBKszWX+B7Ej5tZt/U94cs0xcXFJCYmHtPv72pfmklKSiIp6fjanR/MPIeGjUPFxsYSGxt7XN/7WDSuF/je35idKLW8xBs+fJaLs8qepoQEAIUQERGJWu8s3QrAKNc7tu0JnV2bSDHyybGak7tzX1j3i4Rsj8jixYtZvHgxvXv35pRTTiE7O5t7772Xtm3bVrkaEg490xoB4MPLaWWT6elawyKzAz68gJqXiYhI9Hrwo0DrigRKGOn50ja3y6xLntUEgNRG4d20GrI+InFxcUybNo3zzz+f9u3bc/XVV9O5c2dmz54d0lWPo2nTOIFebX8OI3PNrsEQ0qttIzUvExGRqLRiyx5empcLwAjX+5UaeV5TPgY/bhrEecJ+eqbae0TCqTrXmI5VUWk5N079njnrC4JjfdIb88zl3XVSRkREolKbcR9jWoHWFfNjb8Z7SBApNOPp6XsBP25mjD6brq0anPD7hXSPSE2XGO/htWvOJKdwH7k795HaqK5jZ6dFRERC7aGPV2FagTvNL4692bYaEmjk+Y/Aakh8zEkJIdVV64LIQWlJCiAiIhL9XpybC8BY19u2EOKz4Oyyp4KNPGffdp4D1dXiu++KiIhEu7/+N9AXJJEirvJ8bZu7zHcXBTQG4OWrMhzbnqAgIiIiEoVWbNnDf5cG7nh/i+st22rIj2YdllsdALjzgg6cf2pTJ0oEFERERESi0sXPzgcCG1SHeWbb5u6ruBo/bgBG9msb9toOpSAiIiISZR77fA0Acexn0WEbVMstN1+aZwIw5vx2TpRnoyAiIiISRfaU+nh21kYALnd9iuuwniE3l48M9tAa85v24S6vEgURERGRKLGn1MdZD88EAh1U7/a8a5v3WfCleQYAD1xyatjrq4qCiIiISJS48qVF/FQBXnx84b3VthpiWdCn7J/48OIGrsxMc6zOQymIiIiIRIF56wtYsa0YN35meMfRwrXXNv9g+aXsoBkAX/+1nxMlVklBREREpIbbU+rjipcXA9DByKGDa7tt3m/Bm+aFAPzpjFYkN4oPe41HoiAiIiJSw106aQEQOCUzzXuvbc5nuTi9bBL7iQPg4Usj607ztbbFu4iISDT4bMV2Nhbsw42fz7y3E2tr4+7i9LJ/UUICAJ/c2NuhKo9MQURERKSG2lPqY+Qb3wHQxVhHa9dO2/xX5V2CIWTuX/tF1CWZg3RpRkREpAbaU+qj58P/AwJHdad5H7DNWxbcY14NQJ/0RhEZQkBBREREpEb6w/PzKasw8eLjS+9fbUd1TQvOLXskeFO71645y6Eqf5mCiIiISA3z3tLNbCgsxY2fj7y308xVZJsf5LuHPFIAeOT3nZ0o8Zhpj4iIiEgNsqfUx63/XQ7A6cZKfuXKt81vN+uzyvoVAJOv6EH/zs3CXmN1aEVERESkhlixZQ+n3R9o4Z5IEe94/26btywY4Ps7fty0qF8n4kMIKIiIiIjUGJc8Ox8IbE792jvWdlddy4LeZY9SRCIAn47p40SJ1aZLMyIiIhFuT6mP3/xzNiaBpmXLYkfgriKEbKUlAG9fdxaJ8R5niq0mBREREZEItmnnPs577Gv8Frjxc63rQ1sIAbjENyEYQl6+KoOebRs5UOnxURARERGJYAOfmIPfCqyEfOK9kzRXgW1+hdmaVVZbAJ78YzfOP7WpE2UeNwURERGRCLSn1MeQSd9QeqBXyHexI4gz/LbXPO4bxPPmpfhx06ttIwad3sqhao+fNquKiIhEoCteWsj6ghLc+Bni+l+lEGJa8JJ5MX7ctG4Yx6Q/93Co0hOjFREREZEIsqfUx+UvLmT19r248TPNexfdXHm215RbLjLKnmM/cRjA7NvPc6bYk0BBREREJELsKfXR5x+zKP6pAjd++rq+rxRCDr+j7scReEfd6lAQERERiQBZebsZ+uJCfioP7An5wHsXHV1bba/xWW5OL5sU8XfUrQ4FEREREQftKfUxeNICsgv2AeDFx6feO2nr2mF7XZnlpkeUhRBQEBEREXHMlPnZTPhwNRDoEdLG2MILnqdoc1gI2Wg25be+h9lPHBA9IQQURERERMJu0859/O7ZeRTtrwACLds/9f6NZFeh7XUbzabcUH4LG62W+HHjMmD2bdETQkBBREREJKxmr83n2le/pdwMXIY51/U9L3iewnVYt9RssxkDfY/gwwtAx2b1eGtEZo1p3X6sFERERER+QXZBCZt2lZLaqC5pSXWr/fWz1+bz9boCpi7exE/lFhAIIVmx1xFvlFd6/WqzFZf4HsSHlzoeF++MyKRrcoMT/TEikoKIiIjIEewp9XHT1CzmrP+5rXqf9MY8c3l3EuM9ZBeUsChnJ2BwVptGtpCSXVDCtzm7eOiT1RT/FLgE48ZPurGFZONHTmVzpRBiWvBb3/2stdLw46Z+nRjm3n5e1K2CHEpBRERE5AhumprF/A2FuPGTYuSTZzVh/oZCRr6+FMOABRt32l6f2aYRf7+0K3dPX2kLL158ZLpWMTbmbVtfEMsC48Almc1mIwb6JgZPxZyW3IBX/+/MqA4hoCAiIiJSpeyCEuasLzjQ3XQ83VzZLDPbMNh3H99kBwKIGz+pxnYAcq3mfJO9k0uem0fx/opgeNlhNWBp7A3EG75K72EY8A/fEL60egQ3pAJMvrIH/Ts1C98P6yAFERERkSps2lUKQIqRTzdXNgDdXNmkGPnkWM1x4+d97710deUAsMxMY7DvfnaXYgsvG81mVYYQgFLLy0vmRcENqWmN6jJ91NlRvwpyKAURERGRKrRuGDgim2c1YZnZJrgikmc1AQIB5WAIAejmygmGlEPDS1vXDvZbHuKMckotL5f67qK5sRsTFwvMrsENqZ/e3Oe4NsLWdAoiIiIiVWjTOIE+6Y2Zv6GQwb77gntErAOXT/KsJiw302wrIgdDyuHhZajvLjJc61lkdsCHl9XWz+9Tv04MH994TlT1BqkOw7Is65df5ozi4mISExMpKiqifv36TpcjIiK1TFFpOTdO/b7SqZkK02TBxp2V9oj4cXNKvIfi/RVgVQTDy8G9H4dqkhDL/Zd0YkCX5mH7ecKlOr+/FURERER+QU7hPnJ37gv2ESkqLeeGN5ZWeWrmH5d25a7DTs10aVmf3ulJtElKIKle7HH3I6kpFERERETCIKdwH4uyd2JBpT4ih4eX2qQ6v7+1R0REROQ4pSUdOWQcbU5+5nK6ABEREam9FERERETEMQoiIiIi4piwBJGysjJOO+00DMMgKysrHG8pIiIiNUBYgsjtt99OixYtwvFWIiIiUoOEPIh8+umnfPHFFzz22GOhfisRERGpYUJ6fPfHH3/kuuuuY/r06cTH/3Lr2rKyMsrKyoLPi4uLQ1meiIiIOCxkKyKWZTF8+HBGjhxJRkbGMX3NxIkTSUxMDD6Sk5NDVZ6IiIhEgGoHkQkTJmAYxlEfS5Ys4ZlnnqG4uJhx48Yd8/ceN24cRUVFwcfmzZurW56IiIjUINVu8V5YWEhhYeFRX5OamsrQoUP58MMPMQwjOO73+3G73fz5z3/m1Vdf/cX3Uot3ERGRmici7jWTl5dn2+Oxbds2LrjgAt5991169uxJq1atfvF7FBUV0aBBAzZv3qwgIiIiUkMUFxeTnJzMnj17SExMPOprQ7ZZNSUlxfY8ISEBgLZt2x5TCAHYu3cvgPaKiIiI1EB79+51LoicDC1atGDz5s3Uq1fPdonnZDiY1rTaUjV9Pkemz+bo9PkcnT6fo9Pnc2Q16bOxLIu9e/ceUw+xsAWR1NRUqnsVyOVyHfPqyfGqX79+xP+FOkmfz5Hpszk6fT5Hp8/n6PT5HFlN+Wx+aSXkIN1rRkRERByjICIiIiKOqbVBJDY2lvHjxxMbG+t0KRFJn8+R6bM5On0+R6fP5+j0+RxZtH42ITu+KyIiIvJLau2KiIiIiDhPQUREREQcoyAiIiIijlEQEREREcfUyiDy/PPPk5aWRp06dejRowdz5851uqSIMWfOHC6++GJatGiBYRhMnz7d6ZIixsSJEznjjDOoV68eTZo0YdCgQaxdu9bpsiLGpEmT6Nq1a7DZUmZmJp9++qnTZUWkiRMnYhgGY8aMcbqUiFDVXd2bNWvmdFkRZevWrVxxxRU0atSI+Ph4TjvtNJYuXep0WSdFrQsib7/9NmPGjOGuu+7i+++/55xzzmHgwIHk5eU5XVpE2LdvH926dePZZ591upSIM3v2bEaNGsXChQuZOXMmFRUV9O/fn3379jldWkRo1aoVjzzyCEuWLGHJkiWcd955XHLJJaxatcrp0iLKt99+y+TJk+natavTpUSUTp06sX379uBjxYoVTpcUMXbv3s3ZZ5+Nx+Ph008/5YcffuDxxx+nQYMGTpd2cli1zJlnnmmNHDnSNtahQwfrzjvvdKiiyAVY77//vtNlRKz8/HwLsGbPnu10KRHrlFNOsV566SWny4gYe/futdLT062ZM2da5557rnXzzTc7XVJEGD9+vNWtWzeny4hYd9xxh9W7d2+nywiZWrUi4vP5WLp0Kf3797eN9+/fnwULFjhUldRURUVFADRs2NDhSiKP3+/nrbfeYt++fWRmZjpdTsQYNWoUF110Eb/+9a+dLiXirF+/nhYtWpCWlsbQoUPJzs52uqSIMWPGDDIyMhgyZAhNmjShe/fuvPjii06XddLUqiBSWFiI3++nadOmtvGmTZuyY8cOh6qSmsiyLMaOHUvv3r3p3Lmz0+VEjBUrVpCQkEBsbCwjR47k/fff59RTT3W6rIjw1ltv8d133zFx4kSnS4k4PXv25LXXXuPzzz/nxRdfZMeOHfTq1YudO3c6XVpEyM7OZtKkSaSnp/P5558zcuRIbrrpJl577TWnSzspwnb33UhiGIbtuWVZlcZEjmb06NEsX76cefPmOV1KRGnfvj1ZWVns2bOH9957j2HDhjF79uxaH0Y2b97MzTffzBdffEGdOnWcLifiDBw4MPjnLl26kJmZSdu2bXn11VcZO3asg5VFBtM0ycjI4OGHHwage/furFq1ikmTJnHVVVc5XN2Jq1UrIklJSbjd7kqrH/n5+ZVWSUSO5MYbb2TGjBnMmjWLVq1aOV1ORPF6vbRr146MjAwmTpxIt27deOqpp5wuy3FLly4lPz+fHj16EBMTQ0xMDLNnz+bpp58mJiYGv9/vdIkRpW7dunTp0oX169c7XUpEaN68eaUw37Fjx6g5ZFGrgojX66VHjx7MnDnTNj5z5kx69erlUFVSU1iWxejRo5k2bRpfffUVaWlpTpcU8SzLoqyszOkyHHf++eezYsUKsrKygo+MjAz+/Oc/k5WVhdvtdrrEiFJWVsbq1atp3ry506VEhLPPPrtSq4B169bRunVrhyo6uWrdpZmxY8dy5ZVXkpGRQWZmJpMnTyYvL4+RI0c6XVpEKCkpYcOGDcHnOTk5ZGVl0bBhQ1JSUhyszHmjRo3izTff5IMPPqBevXrBlbXExETi4uIcrs55f/vb3xg4cCDJycns3buXt956i6+//prPPvvM6dIcV69evUp7ierWrUujRo20xwi47bbbuPjii0lJSSE/P58HH3yQ4uJihg0b5nRpEeGWW26hV69ePPzww/zxj39k8eLFTJ48mcmTJztd2snh7KEdZzz33HNW69atLa/Xa51++uk6fnmIWbNmWUClx7Bhw5wuzXFVfS6A9corrzhdWkS4+uqrg/+7aty4sXX++edbX3zxhdNlRSwd3/3ZZZddZjVv3tzyeDxWixYtrMGDB1urVq1yuqyI8uGHH1qdO3e2YmNjrQ4dOliTJ092uqSTxrAsy3IoA4mIiEgtV6v2iIiIiEhkURARERERxyiIiIiIiGMURERERMQxCiIiIiLiGAURERERcYyCiIiIiDhGQUREREQcoyAiIiIijlEQEREREccoiIiIiIhjFERERETEMf8Pnz+TzyXfeXYAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "y_pred = model(x_data[:,0])\n",
        "y_true = tf.math.cos(x_data[:,0]-x_data[:,1]) + tf.math.cos(x_data[:,0]-x_data[:,2])\\\n",
        "          + tf.math.cos(x_data[:,0]-x_data[:,3]) + tf.math.cos(x_data[:,0]-x_data[:,4])\n",
        "plt.figure()\n",
        "plt.scatter(x_data[:,0],y_pred,label=\"predicted\", s=20)\n",
        "plt.scatter(x_data[:,0],y_true,label=\"true\", s=2)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Contrastive loss term : |log(p_\\theta1 - log(p_\\theta2.) + logq_\\theta2 -logq_theta1)|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.29794394, 1.71909646, 1.68910831, 1.96929167, 2.02049597])>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWDMRmNiENOG",
        "outputId": "79037056-b841-49ff-8f5f-dbae9aeda333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[x_1, x_2, x_3, x_4, x_5]\n"
          ]
        }
      ],
      "source": [
        "print(in_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTNqocEJdjpJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THPHyT795Iaf",
        "outputId": "ae1ef9c5-216e-4255-80a4-38155deb7f0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 5)\n",
            "(1000,)\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "[2.6729279 3.6438878 1.9597814 3.423102  2.3374755 3.783485  3.7766137\n",
            " 1.4907472 1.4525564 2.7729478]\n",
            "[[2.6761672]\n",
            " [3.6461947]\n",
            " [1.956576 ]\n",
            " [3.4249556]\n",
            " [2.338623 ]\n",
            " [3.7857301]\n",
            " [3.7799225]\n",
            " [1.5004735]\n",
            " [1.4647846]\n",
            " [2.782048 ]]\n"
          ]
        }
      ],
      "source": [
        "# Evaluate\n",
        "x_int_test = tf.random.uniform(shape=[1000,5],minval=-1,maxval=1,seed=10)\n",
        "y_int_test = tf.math.cos(x_int_test[:,0]-x_int_test[:,1]) + tf.math.cos(x_int_test[:,0]-x_int_test[:,2]) \\\n",
        "          + tf.math.cos(x_int_test[:,0]-x_int_test[:,3]) + tf.math.cos(x_int_test[:,0]-x_int_test[:,4])\n",
        "\n",
        "print(x_int_test.get_shape())\n",
        "print(y_int_test.get_shape())\n",
        "\n",
        "y_int_pred = model.predict(x_int_test)\n",
        "print(y_int_test[:10].numpy())\n",
        "print(y_int_pred[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "id": "odLi1JME4IlA",
        "outputId": "f11d6d1b-3984-4ce0-f437-ba40edc70234"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-fcdc7d8da914>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_int_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_int_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'y_true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_int_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_int_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'y_pred'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2813\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \"\"\"\n\u001b[1;32m   1687\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         result = (make_artist(x[:, j % ncx], y[:, j % ncy], kw,\n\u001b[0m\u001b[1;32m    538\u001b[0m                               {**kwargs, 'label': label})\n\u001b[1;32m    539\u001b[0m                   for j, label in enumerate(labels))\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_makeline\u001b[0;34m(self, x, y, kw, kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mdefault_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getdefaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setdefaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0;34m\"parameter will become keyword-only %(removal)s.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 name=name, obj_type=f\"parameter of {func.__name__}()\")\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Don't modify *func*'s signature, as boilerplate.py needs it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, xdata, ydata, linewidth, linestyle, color, gapcolor, marker, markersize, markeredgewidth, markeredgecolor, markerfacecolor, markerfacecoloralt, fillstyle, antialiased, dash_capstyle, solid_capstyle, dash_joinstyle, solid_joinstyle, pickradius, drawstyle, markevery, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;31m# update kwargs before updating data to give the caller a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;31m# chance to init axes (and hence unit support)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pickradius\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickradius\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mind_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36m_internal_update\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mlack\u001b[0m \u001b[0mof\u001b[0m \u001b[0mprenormalization\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mto\u001b[0m \u001b[0mmaintain\u001b[0m \u001b[0mbackcompatibility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \"\"\"\n\u001b[0;32m-> 1223\u001b[0;31m         return self._update_props(\n\u001b[0m\u001b[1;32m   1224\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{cls.__name__}.set() got an unexpected keyword argument \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m             \"{prop_name!r}\")\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36m_update_props\u001b[0;34m(self, props, errfmt)\u001b[0m\n\u001b[1;32m   1195\u001b[0m                     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"set_{k}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m                         raise AttributeError(\n\u001b[0m\u001b[1;32m   1198\u001b[0m                             errfmt.format(cls=type(self), prop_name=k))\n\u001b[1;32m   1199\u001b[0m                     \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Line2D.set() got an unexpected keyword argument 's'"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.scatter(x_int_test[:,0],y_int_test,c='y',s=2, label = 'y_true')\n",
        "plt.scatter(x_int_test[:,0],y_int_pred,c='k',s=1, label = 'y_pred')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qotpBKA4IiJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6XYgWli-BGy"
      },
      "outputs": [],
      "source": [
        "# Extrapolation dataset\n",
        "x_ext = tf.random.uniform(shape=[1000,1],minval=-2,maxval=2,seed=10)\n",
        "mask = tf.cast(tf.abs(x_ext) > 1.0, dtype='tf.float32')\n",
        "x_ext = tf.multiply(x_ext, mask)\n",
        "y_ext_test = (1./3)*(tf.multiply(1.+x_ext,tf.sin(np.pi*x_ext)) +\n",
        "                  tf.multiply(tf.multiply(x_ext,x_ext),x_ext))\n",
        "\n",
        "print(x_ext.get_shape())\n",
        "\n",
        "x_ext_test = tf.concat([x_ext]*4,axis=1)\n",
        "print(x_ext_test.get_shape())\n",
        "\n",
        "y_ext_pred = model(x_ext_test, 1e-4, 0, 0)\n",
        "print(y_ext_pred.get_shape())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDUbZrRnAzdN"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x_int_test[:,0],y_int_test,c='y',s=10, name = 'y_true')\n",
        "plt.scatter(x_int_test[:,0],y_int_pred,c='k',s=1, name = 'y_pred')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJQ-VZJSBJ1w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bcp2K7VRBJzE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "husIBwpfBJv_"
      },
      "outputs": [],
      "source": [
        "# Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqN1_22J4B0d"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0HexnIe4Ifa"
      },
      "outputs": [],
      "source": [
        "def model_score(alpha = 0.5, beta = 0.5, gamma = 0.5, use_extrapolation = 0):\n",
        "  errors = []\n",
        "  val_int_error = mean_squared_error(y_int_test, y_int_pred)\n",
        "  sparsity = None\n",
        "  errors.append(val_int_error)\n",
        "  errors.append(sparsity)\n",
        "\n",
        "  if use_extrapolation == 1:\n",
        "    beta = 0.0\n",
        "    val_ext_error = mean_squared_error(y_ext_test, y_ext_pred)\n",
        "    errors.append(val_ext_error)\n",
        "\n",
        "  return errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VZ7acla4IcH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fKlxCnc4IZZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tGTRpW63gvW"
      },
      "outputs": [],
      "source": [
        "function ConnectButton(){\n",
        "  console.log(\"Connect pushed\");\n",
        "  document.querySelector(\"#top-toolbar > colab-connectbutton\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmZGGpsK2xDv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W089c0ik2xA_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkrvlDfq2w-K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKXrJrbn2w7e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
